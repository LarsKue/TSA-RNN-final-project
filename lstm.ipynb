{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Analysis & Recurrent Neural Networks\n",
    "## Final Project\n",
    "### Lars KÃ¼hmichel, Nicolas Wolf\n",
    "\n",
    "The final project is about analyzing the dynamics of a time series of weather data from across Europe. The goal is to\n",
    "- reconstruct the dynamics behind the data, especially the annual and daily rhythms\n",
    "- predict the future of the time series\n",
    "- and compare two model architectures with respect to the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as lightning\n",
    "import pytorch_lightning.loggers as loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import ray.tune as tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=200)\n",
    "plt.rc(\"legend\", fontsize=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first run a modified version of the provided code to download the dataset and save it locally."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    w = np.ones(N) / N\n",
    "    if x.ndim > 1:\n",
    "        res = []\n",
    "        for i in range(x.shape[1]):\n",
    "            res.append(np.convolve(w, x[:, i], 'valid'))\n",
    "        return np.array(res).T\n",
    "    else:\n",
    "        return np.convolve(w, x, 'valid')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_root = pl.Path(\"data\")\n",
    "data_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "data_path = data_root / \"weather_data.csv\"\n",
    "\n",
    "if not data_path.is_file():\n",
    "    data_url = r\"https://data.open-power-system-data.org/weather_data/2020-09-16/weather_data.csv\"\n",
    "    data = pd.read_csv(data_url)\n",
    "    data.to_csv(data_path)\n",
    "else:\n",
    "    data = pd.read_csv(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains hourly geographically aggregated weather (radiation and temperature) data for Europe. It covers the European countries using a population-weighted mean across all MERRA-2 grid cells within the given country."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350640 entries, 0 to 350639\n",
      "Data columns (total 86 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   Unnamed: 0                       350640 non-null  int64  \n",
      " 1   utc_timestamp                    350640 non-null  object \n",
      " 2   AT_temperature                   350640 non-null  float64\n",
      " 3   AT_radiation_direct_horizontal   350640 non-null  float64\n",
      " 4   AT_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 5   BE_temperature                   350640 non-null  float64\n",
      " 6   BE_radiation_direct_horizontal   350640 non-null  float64\n",
      " 7   BE_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 8   BG_temperature                   350640 non-null  float64\n",
      " 9   BG_radiation_direct_horizontal   350640 non-null  float64\n",
      " 10  BG_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 11  CH_temperature                   350640 non-null  float64\n",
      " 12  CH_radiation_direct_horizontal   350640 non-null  float64\n",
      " 13  CH_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 14  CZ_temperature                   350640 non-null  float64\n",
      " 15  CZ_radiation_direct_horizontal   350640 non-null  float64\n",
      " 16  CZ_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 17  DE_temperature                   350640 non-null  float64\n",
      " 18  DE_radiation_direct_horizontal   350640 non-null  float64\n",
      " 19  DE_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 20  DK_temperature                   350640 non-null  float64\n",
      " 21  DK_radiation_direct_horizontal   350640 non-null  float64\n",
      " 22  DK_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 23  EE_temperature                   350640 non-null  float64\n",
      " 24  EE_radiation_direct_horizontal   350640 non-null  float64\n",
      " 25  EE_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 26  ES_temperature                   350640 non-null  float64\n",
      " 27  ES_radiation_direct_horizontal   350640 non-null  float64\n",
      " 28  ES_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 29  FI_temperature                   350640 non-null  float64\n",
      " 30  FI_radiation_direct_horizontal   350640 non-null  float64\n",
      " 31  FI_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 32  FR_temperature                   350640 non-null  float64\n",
      " 33  FR_radiation_direct_horizontal   350640 non-null  float64\n",
      " 34  FR_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 35  GB_temperature                   350640 non-null  float64\n",
      " 36  GB_radiation_direct_horizontal   350640 non-null  float64\n",
      " 37  GB_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 38  GR_temperature                   350640 non-null  float64\n",
      " 39  GR_radiation_direct_horizontal   350640 non-null  float64\n",
      " 40  GR_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 41  HR_temperature                   350640 non-null  float64\n",
      " 42  HR_radiation_direct_horizontal   350640 non-null  float64\n",
      " 43  HR_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 44  HU_temperature                   350640 non-null  float64\n",
      " 45  HU_radiation_direct_horizontal   350640 non-null  float64\n",
      " 46  HU_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 47  IE_temperature                   350640 non-null  float64\n",
      " 48  IE_radiation_direct_horizontal   350640 non-null  float64\n",
      " 49  IE_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 50  IT_temperature                   350640 non-null  float64\n",
      " 51  IT_radiation_direct_horizontal   350640 non-null  float64\n",
      " 52  IT_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 53  LT_temperature                   350640 non-null  float64\n",
      " 54  LT_radiation_direct_horizontal   350640 non-null  float64\n",
      " 55  LT_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 56  LU_temperature                   350640 non-null  float64\n",
      " 57  LU_radiation_direct_horizontal   350640 non-null  float64\n",
      " 58  LU_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 59  LV_temperature                   350640 non-null  float64\n",
      " 60  LV_radiation_direct_horizontal   350640 non-null  float64\n",
      " 61  LV_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 62  NL_temperature                   350640 non-null  float64\n",
      " 63  NL_radiation_direct_horizontal   350640 non-null  float64\n",
      " 64  NL_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 65  NO_temperature                   350640 non-null  float64\n",
      " 66  NO_radiation_direct_horizontal   350640 non-null  float64\n",
      " 67  NO_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 68  PL_temperature                   350640 non-null  float64\n",
      " 69  PL_radiation_direct_horizontal   350640 non-null  float64\n",
      " 70  PL_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 71  PT_temperature                   350640 non-null  float64\n",
      " 72  PT_radiation_direct_horizontal   350640 non-null  float64\n",
      " 73  PT_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 74  RO_temperature                   350640 non-null  float64\n",
      " 75  RO_radiation_direct_horizontal   350640 non-null  float64\n",
      " 76  RO_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 77  SE_temperature                   350640 non-null  float64\n",
      " 78  SE_radiation_direct_horizontal   350640 non-null  float64\n",
      " 79  SE_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 80  SI_temperature                   350640 non-null  float64\n",
      " 81  SI_radiation_direct_horizontal   350640 non-null  float64\n",
      " 82  SI_radiation_diffuse_horizontal  350640 non-null  float64\n",
      " 83  SK_temperature                   350640 non-null  float64\n",
      " 84  SK_radiation_direct_horizontal   350640 non-null  float64\n",
      " 85  SK_radiation_diffuse_horizontal  350640 non-null  float64\n",
      "dtypes: float64(84), int64(1), object(1)\n",
      "memory usage: 230.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"FI_temperature\",\n",
    "    \"DE_temperature\",\n",
    "    \"GR_temperature\"\n",
    "]\n",
    "\n",
    "test_temp = data[keys]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ten_years = 24 * 365 * 10\n",
    "tdf = test_temp[-ten_years:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(12720, 3)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weekly mask\n",
    "week_filter = [True] * 24 + [False] * 24 * 6\n",
    "# yearly mask for 52 weeks plus one day\n",
    "year_filter = week_filter * 52 + [True] * 24\n",
    "# mask for ten years\n",
    "ten_years_filter = year_filter * 10\n",
    "daydf = tdf[ten_years_filter]\n",
    "\n",
    "daydf.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "npa = daydf.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Moving average to see both trends\n",
    "N = 24\n",
    "mnpa = running_mean(npa, N)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 691.2x345.6 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, sharex=\"all\", figsize=plt.figaspect(1 / 2))\n",
    "sns.lineplot(data=npa, ax=axes[0], dashes=False, lw=0.5)\n",
    "axes[0].set_title(\"Data as given\")\n",
    "\n",
    "sns.lineplot(data=mnpa, ax=axes[1], dashes=False, lw=0.5)\n",
    "axes[1].set_title(\"Yearly trend\")\n",
    "axes[1].set_ylabel(\"Temperature $[Â°C]$\")\n",
    "\n",
    "sns.lineplot(data=npa[:-N + 1] - mnpa, ax=axes[2], dashes=False, lw=0.5)\n",
    "axes[2].set_title(\"Daily trend\")\n",
    "axes[2].set_xlabel(\"Time $[h]$\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split into train and test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "((10176, 3), (2544, 3))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = int(0.8 * len(npa))\n",
    "\n",
    "train_data, test_data = npa[:pivot], npa[pivot:]\n",
    "\n",
    "train_data.shape, test_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalize based on the training set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "mean = np.mean(train_data, axis=0)\n",
    "std = np.std(train_data, axis=0)\n",
    "\n",
    "train_data = (train_data - mean[None]) / std[None]\n",
    "test_data = (test_data - mean[None]) / std[None]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Use a deterministic LSTM coupled with a fully connected observation layer and optimize it with backpropagation through time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best hyperparameters found were: {'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
    "\"\"\"\n",
    "\n",
    "hparams = dict(\n",
    "    input_size=npa.shape[-1],\n",
    "    # choose hidden size small enough for fast training\n",
    "    hidden_size=128,\n",
    "    # choose number of layers small enough for fast training\n",
    "    num_layers=4,\n",
    "    # choose dropout small enough to not hinder network capacity\n",
    "    dropout=0.45,\n",
    "    # choose batch size large enough to smooth out gradients\n",
    "    batch_size=8,\n",
    "    optimizer=\"adam\",\n",
    "    # we use the default learning rate\n",
    "    learning_rate=2e-3,\n",
    "    # gradient clipping can help with very noisy loss landscapes\n",
    "    gradient_clip=None,\n",
    "    # minimum sequence length fed into rnn for predictions, must be greater than prediction_length\n",
    "    sequence_length=15 * 4 * 24,\n",
    "    # number of predictions made by the rnn at once, choose large enough to capture periodicities in the data\n",
    "    prediction_length=9 * 4 * 24,\n",
    "    # lr warmup can help with very noisy loss landscapes\n",
    "    lr_warmup_milestones=[],\n",
    "    lr_warmup_gamma=10.0,\n",
    "    # lr milestones help push the network deeper into the loss minimum\n",
    "    lr_milestones=[],\n",
    "    lr_gamma=0.1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, time_series: torch.Tensor, sequence_length: int):\n",
    "        self.time_series = time_series\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_series) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        start = item\n",
    "        stop = item + self.sequence_length\n",
    "        return self.time_series[start:stop]\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_data = torch.tensor(test_data, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class LSTM(lightning.LightningModule):\n",
    "    def __init__(self, train_data, val_data, **hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams, ignore=[\"train_data\", \"val_data\"])\n",
    "\n",
    "        if self.hparams.sequence_length < self.hparams.prediction_length:\n",
    "            raise ValueError(f\"The sequence length must be greater than or equal to the prediction length.\")\n",
    "\n",
    "        self.train_data = TimeSeriesDataset(train_data, self.hparams.sequence_length)\n",
    "        self.val_data = TimeSeriesDataset(val_data, self.hparams.sequence_length)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hparams.input_size, self.hparams.hidden_size, self.hparams.num_layers, dropout=self.hparams.dropout, batch_first=True)\n",
    "        self.observation_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.hidden_size, self.hparams.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hparams.hidden_size, self.hparams.input_size),\n",
    "        )\n",
    "\n",
    "        # initial predictions close to 0 are good, since we normalized the data\n",
    "        self.observation_net[-1].weight.data.fill_(0.0)\n",
    "        self.observation_net[-1].bias.data.fill_(0.0)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, _ = self.lstm.forward(x)\n",
    "        y = self.observation_net.forward(z)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y = self.forward(batch[:, :-self.hparams.prediction_length])\n",
    "        loss = self.loss(y, batch[:, self.hparams.prediction_length:])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y = self.forward(batch[:, :-self.hparams.prediction_length])\n",
    "        loss = self.loss(y, batch[:, self.hparams.prediction_length:])\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def predict(self, x:  torch.Tensor, hx=None, steps: int = 1):\n",
    "        predictions = []\n",
    "        x = x[:, -self.hparams.sequence_length:].to(self.device)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            z, hx = self.lstm.forward(x, hx)\n",
    "            x = self.observation_net(z).detach()\n",
    "\n",
    "            predictions.append(x[:, -self.hparams.prediction_length:])\n",
    "\n",
    "        return torch.cat(predictions, dim=1)\n",
    "\n",
    "    def forecast(self, steps: int = 1):\n",
    "        return self.predict(self.train_data.time_series[None], steps=steps).squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        match self.hparams.optimizer.lower():\n",
    "            case \"adam\":\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "            case \"rmsprop\":\n",
    "                optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams.learning_rate)\n",
    "            case _:\n",
    "                raise ValueError(f\"Optimizer {self.hparams.optimizer} is not supported.\")\n",
    "\n",
    "        lr_warmup = torch.optim.lr_scheduler.MultiStepLR(optimizer, self.hparams.lr_warmup_milestones, self.hparams.lr_warmup_gamma)\n",
    "        lr_step = torch.optim.lr_scheduler.MultiStepLR(optimizer, self.hparams.lr_milestones, self.hparams.lr_gamma)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ChainedScheduler([\n",
    "            lr_warmup,\n",
    "            lr_step,\n",
    "        ])\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "        )\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        return [\n",
    "            lightning.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True),\n",
    "            lightning.callbacks.LearningRateMonitor(),\n",
    "            lightning.callbacks.StochasticWeightAveraging(),\n",
    "        ]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_data,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def train(hparams, metrics, max_epochs=16, gpus=-1):\n",
    "    model = LSTM(train_data, val_data, **hparams)\n",
    "\n",
    "    logger = loggers.TensorBoardLogger(\n",
    "        save_dir=tune.get_trial_dir(), name=\"\", version=\".\"\n",
    "    )\n",
    "\n",
    "    trainer = lightning.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        gpus=gpus,\n",
    "        callbacks=[\n",
    "            TuneReportCallback(metrics=metrics, on=\"validation_end\")\n",
    "        ],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def tune_hparams(config, metrics, num_samples=10, max_epochs=16, gpus_per_trial=1):\n",
    "    scheduler = tune.schedulers.AsyncHyperBandScheduler(\n",
    "        max_t=max_epochs,\n",
    "        grace_period=2,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    reporter = tune.CLIReporter(\n",
    "        parameter_columns=list(config.keys()),\n",
    "        metric_columns=list(metrics.keys()) + [\"training_iteration\"],\n",
    "    )\n",
    "\n",
    "    train_fn = tune.with_parameters(train, metrics=metrics, max_epochs=max_epochs, gpus=gpus_per_trial)\n",
    "    resources_per_trial = dict(cpu=1, gpu=gpus_per_trial)\n",
    "    analysis = tune.run(\n",
    "        train_fn,\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"tune_asha\",\n",
    "        local_dir=\"ray_results\",\n",
    "    )\n",
    "\n",
    "    return analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run a hyperparameter search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "metrics = dict(\n",
    "    loss=\"val_loss\",\n",
    ")\n",
    "\n",
    "config = hparams.copy()\n",
    "config[\"optimizer\"] = tune.choice([\"adam\", \"rmsprop\"])\n",
    "config[\"hidden_size\"] = tune.choice([128, 256])\n",
    "config[\"num_layers\"] = tune.choice([1, 2, 4])\n",
    "config[\"learning_rate\"] = tune.loguniform(1e-4, 1e-2)\n",
    "config[\"sequence_length\"] = tune.choice([12 * 4 * 24 + 24, 15 * 4 * 24, 18 * 4 * 24, 21 * 4 * 24])\n",
    "config[\"prediction_length\"] = tune.choice([3 * 4 * 24, 6 * 4 * 24, 9 * 4 * 24, 12 * 4 * 24 - 24])\n",
    "config[\"batch_size\"] = tune.choice([4, 8, 16, 32, 64])\n",
    "config[\"dropout\"] = tune.uniform(0, 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 01:09:05,885\tWARNING function_runner.py:603 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:06 (running for 00:00:00.17)\n",
      "Memory usage on this node: 10.1/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 0 | lstm            | LSTM       | 464 K \n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 481 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 481 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m 1.925     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:12 (running for 00:00:06.67)\n",
      "Memory usage on this node: 12.3/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:17 (running for 00:00:11.68)\n",
      "Memory usage on this node: 12.3/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:22 (running for 00:00:16.69)\n",
      "Memory usage on this node: 11.6/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-09-26\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.18996626138687134\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 19.376288890838623\n",
      "  time_this_iter_s: 19.376288890838623\n",
      "  time_total_s: 19.376288890838623\n",
      "  timestamp: 1659395366\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:31 (running for 00:00:26.05)\n",
      "Memory usage on this node: 11.6/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.18996626138687134 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.189966 |                    1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:36 (running for 00:00:31.06)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.18996626138687134 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.189966 |                    1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:41 (running for 00:00:36.06)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.18996626138687134 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.189966 |                    1 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-09-44\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.2186054140329361\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 37.041980028152466\n",
      "  time_this_iter_s: 17.665691137313843\n",
      "  time_total_s: 37.041980028152466\n",
      "  timestamp: 1659395384\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:49 (running for 00:00:43.71)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2186054140329361 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.218605 |                    2 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:54 (running for 00:00:48.72)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2186054140329361 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.218605 |                    2 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:09:59 (running for 00:00:53.72)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2186054140329361 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.218605 |                    2 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-10-01\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 3\n",
      "  loss: 0.24902485311031342\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 54.265870332717896\n",
      "  time_this_iter_s: 17.22389030456543\n",
      "  time_total_s: 54.265870332717896\n",
      "  timestamp: 1659395401\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:06 (running for 00:01:00.95)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.24902485311031342 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.249025 |                    3 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:11 (running for 00:01:05.95)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.24902485311031342 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.249025 |                    3 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:16 (running for 00:01:10.96)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.24902485311031342 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.249025 |                    3 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-10-19\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.23501648008823395\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 71.9747703075409\n",
      "  time_this_iter_s: 17.708899974822998\n",
      "  time_total_s: 71.9747703075409\n",
      "  timestamp: 1659395419\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:24 (running for 00:01:18.65)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23501648008823395 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.235016 |                    4 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:29 (running for 00:01:23.65)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23501648008823395 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.235016 |                    4 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:34 (running for 00:01:28.66)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23501648008823395 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.235016 |                    4 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-10-37\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.22269797325134277\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 89.64840412139893\n",
      "  time_this_iter_s: 17.673633813858032\n",
      "  time_total_s: 89.64840412139893\n",
      "  timestamp: 1659395437\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:42 (running for 00:01:36.32)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22269797325134277 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.222698 |                    5 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:47 (running for 00:01:41.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22269797325134277 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.222698 |                    5 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:52 (running for 00:01:46.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22269797325134277 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.222698 |                    5 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-10-54\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 6\n",
      "  loss: 0.2306041121482849\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 107.30792093276978\n",
      "  time_this_iter_s: 17.65951681137085\n",
      "  time_total_s: 107.30792093276978\n",
      "  timestamp: 1659395454\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:10:59 (running for 00:01:53.98)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2306041121482849 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230604 |                    6 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:04 (running for 00:01:58.99)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2306041121482849 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230604 |                    6 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:09 (running for 00:02:03.99)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2306041121482849 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230604 |                    6 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-11-12\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 7\n",
      "  loss: 0.2134842872619629\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 125.03106546401978\n",
      "  time_this_iter_s: 17.72314453125\n",
      "  time_total_s: 125.03106546401978\n",
      "  timestamp: 1659395472\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:17 (running for 00:02:11.71)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2134842872619629 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.213484 |                    7 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:22 (running for 00:02:16.71)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2134842872619629 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.213484 |                    7 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:27 (running for 00:02:21.72)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2134842872619629 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.213484 |                    7 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-11-30\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 8\n",
      "  loss: 0.23072946071624756\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 142.70917749404907\n",
      "  time_this_iter_s: 17.678112030029297\n",
      "  time_total_s: 142.70917749404907\n",
      "  timestamp: 1659395490\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:35 (running for 00:02:29.38)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23072946071624756 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230729 |                    8 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:40 (running for 00:02:34.39)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23072946071624756 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230729 |                    8 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:45 (running for 00:02:39.39)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.23072946071624756 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.230729 |                    8 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-11-47\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 9\n",
      "  loss: 0.20655962824821472\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 160.3846628665924\n",
      "  time_this_iter_s: 17.675485372543335\n",
      "  time_total_s: 160.3846628665924\n",
      "  timestamp: 1659395507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:52 (running for 00:02:47.06)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20655962824821472 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20656 |                    9 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:11:57 (running for 00:02:52.06)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20655962824821472 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20656 |                    9 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:02 (running for 00:02:57.07)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20655962824821472 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20656 |                    9 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-12-05\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.21714909374713898\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 178.0045084953308\n",
      "  time_this_iter_s: 17.619845628738403\n",
      "  time_total_s: 178.0045084953308\n",
      "  timestamp: 1659395525\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:10 (running for 00:03:04.68)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21714909374713898 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.217149 |                   10 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:15 (running for 00:03:09.69)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21714909374713898 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.217149 |                   10 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:20 (running for 00:03:14.70)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21714909374713898 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.217149 |                   10 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-12-23\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 11\n",
      "  loss: 0.22172705829143524\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 195.66294980049133\n",
      "  time_this_iter_s: 17.658441305160522\n",
      "  time_total_s: 195.66294980049133\n",
      "  timestamp: 1659395543\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 11\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=84603)\u001B[0m Swapping scheduler `ChainedScheduler` for `SWALR`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:28 (running for 00:03:22.34)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22172705829143524 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.221727 |                   11 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:33 (running for 00:03:27.34)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22172705829143524 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.221727 |                   11 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:38 (running for 00:03:32.35)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.22172705829143524 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.221727 |                   11 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-12-40\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 12\n",
      "  loss: 0.20706523954868317\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 213.38252902030945\n",
      "  time_this_iter_s: 17.719579219818115\n",
      "  time_total_s: 213.38252902030945\n",
      "  timestamp: 1659395560\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 12\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:45 (running for 00:03:40.05)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20706523954868317 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.207065 |                   12 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:50 (running for 00:03:45.06)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20706523954868317 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.207065 |                   12 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:12:55 (running for 00:03:50.07)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20706523954868317 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.207065 |                   12 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-12-58\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 13\n",
      "  loss: 0.21588550508022308\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 231.02902054786682\n",
      "  time_this_iter_s: 17.646491527557373\n",
      "  time_total_s: 231.02902054786682\n",
      "  timestamp: 1659395578\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 13\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:03 (running for 00:03:57.70)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21588550508022308 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.215886 |                   13 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:08 (running for 00:04:02.71)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21588550508022308 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.215886 |                   13 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:13 (running for 00:04:07.71)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.21588550508022308 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.215886 |                   13 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-13-16\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 14\n",
      "  loss: 0.20274043083190918\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 248.68250799179077\n",
      "  time_this_iter_s: 17.65348744392395\n",
      "  time_total_s: 248.68250799179077\n",
      "  timestamp: 1659395596\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 14\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:21 (running for 00:04:15.35)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20274043083190918 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20274 |                   14 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:26 (running for 00:04:20.36)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20274043083190918 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20274 |                   14 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:31 (running for 00:04:25.37)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20274043083190918 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |    loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.20274 |                   14 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |         |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+---------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-13-33\n",
      "  done: false\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 15\n",
      "  loss: 0.2080530971288681\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 266.33570742607117\n",
      "  time_this_iter_s: 17.653199434280396\n",
      "  time_total_s: 266.33570742607117\n",
      "  timestamp: 1659395613\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 15\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:38 (running for 00:04:33.01)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2080530971288681 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.208053 |                   15 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:43 (running for 00:04:38.01)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2080530971288681 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.208053 |                   15 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:48 (running for 00:04:43.02)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.2080530971288681 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status   | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | RUNNING  | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.208053 |                   15 |\n",
      "| train_f0d85_00001 | PENDING  |                      |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING  |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING  |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING  |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING  |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING  |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING  |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING  |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING  |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "+-------------------+----------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00000:\n",
      "  date: 2022-08-02_01-13-51\n",
      "  done: true\n",
      "  experiment_id: f693000be3d34e088338c33313a75eb7\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.20088671147823334\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 84603\n",
      "  time_since_restore: 284.0256505012512\n",
      "  time_this_iter_s: 17.689943075180054\n",
      "  time_total_s: 284.0256505012512\n",
      "  timestamp: 1659395631\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: f0d85_00000\n",
      "  warmup_time: 0.0015590190887451172\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.40072463146195597 and num_layers=1\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 0 | lstm            | LSTM       | 267 K \n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 1 | observation_net | Sequential | 66.6 K\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 333 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 333 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=85463)\u001B[0m 1.335     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:13:57 (running for 00:04:51.45)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00001 | RUNNING    | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING    |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:02 (running for 00:04:56.46)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00001 | RUNNING    | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING    |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:07 (running for 00:05:01.46)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00001 | RUNNING    | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00002 | PENDING    |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00001:\n",
      "  date: 2022-08-02_01-14-10\n",
      "  done: false\n",
      "  experiment_id: 7c4cb57129664f328fc072aafe50998f\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.28375956416130066\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85463\n",
      "  time_since_restore: 16.355567455291748\n",
      "  time_this_iter_s: 16.355567455291748\n",
      "  time_total_s: 16.355567455291748\n",
      "  timestamp: 1659395650\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00001\n",
      "  warmup_time: 0.0017337799072265625\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:15 (running for 00:05:09.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00001 | RUNNING    | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.28376  |                    1 |\n",
      "| train_f0d85_00002 | PENDING    |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:20 (running for 00:05:14.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2186054140329361\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00001 | RUNNING    | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.28376  |                    1 |\n",
      "| train_f0d85_00002 | PENDING    |                      |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00001:\n",
      "  date: 2022-08-02_01-14-25\n",
      "  done: true\n",
      "  experiment_id: 7c4cb57129664f328fc072aafe50998f\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.23941738903522491\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85463\n",
      "  time_since_restore: 31.284640312194824\n",
      "  time_this_iter_s: 14.929072856903076\n",
      "  time_total_s: 31.284640312194824\n",
      "  timestamp: 1659395665\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00001\n",
      "  warmup_time: 0.0017337799072265625\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:25 (running for 00:05:19.45)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 0 | lstm            | LSTM       | 464 K \n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 481 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 481 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=85674)\u001B[0m 1.925     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:31 (running for 00:05:25.94)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:36 (running for 00:05:30.94)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:41 (running for 00:05:35.95)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:46 (running for 00:05:40.95)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:14:51 (running for 00:05:45.96)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00002:\n",
      "  date: 2022-08-02_01-14-55\n",
      "  done: false\n",
      "  experiment_id: 697da0bf1765420e93c5fb246a5e1f79\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3928861916065216\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85674\n",
      "  time_since_restore: 28.261614322662354\n",
      "  time_this_iter_s: 28.261614322662354\n",
      "  time_total_s: 28.261614322662354\n",
      "  timestamp: 1659395695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00002\n",
      "  warmup_time: 0.001634836196899414\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:00 (running for 00:05:54.20)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.392886 |                    1 |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:05 (running for 00:05:59.21)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.392886 |                    1 |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:10 (running for 00:06:04.21)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.392886 |                    1 |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:15 (running for 00:06:09.22)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.392886 |                    1 |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:20 (running for 00:06:14.22)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2290114015340805\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00002 | RUNNING    | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.392886 |                    1 |\n",
      "| train_f0d85_00003 | PENDING    |                      |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00002:\n",
      "  date: 2022-08-02_01-15-22\n",
      "  done: true\n",
      "  experiment_id: 697da0bf1765420e93c5fb246a5e1f79\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.23791536688804626\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85674\n",
      "  time_since_restore: 55.294490814208984\n",
      "  time_this_iter_s: 27.03287649154663\n",
      "  time_total_s: 55.294490814208984\n",
      "  timestamp: 1659395722\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00002\n",
      "  warmup_time: 0.001634836196899414\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.06017800010528307 and num_layers=1\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 0 | lstm            | LSTM       | 68.1 K\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 85.0 K    Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 85.0 K    Total params\n",
      "\u001B[2m\u001B[36m(train pid=85893)\u001B[0m 0.340     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:27 (running for 00:06:21.45)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23791536688804626\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00003 | RUNNING    | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00003:\n",
      "  date: 2022-08-02_01-15-29\n",
      "  done: false\n",
      "  experiment_id: b16eaac9a1ae41ecbe1df29c5678ae62\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7764685750007629\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85893\n",
      "  time_since_restore: 5.251362323760986\n",
      "  time_this_iter_s: 5.251362323760986\n",
      "  time_total_s: 5.251362323760986\n",
      "  timestamp: 1659395729\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00003\n",
      "  warmup_time: 0.001621246337890625\n",
      "  \n",
      "Result for train_f0d85_00003:\n",
      "  date: 2022-08-02_01-15-32\n",
      "  done: true\n",
      "  experiment_id: b16eaac9a1ae41ecbe1df29c5678ae62\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.5815042853355408\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 85893\n",
      "  time_since_restore: 9.10997200012207\n",
      "  time_this_iter_s: 3.858609676361084\n",
      "  time_total_s: 9.10997200012207\n",
      "  timestamp: 1659395732\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00003\n",
      "  warmup_time: 0.001621246337890625\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:32 (running for 00:06:27.04)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (6 PENDING, 4 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00004 | PENDING    |                      |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 0 | lstm            | LSTM       | 200 K \n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 217 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 217 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=86096)\u001B[0m 0.868     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:38 (running for 00:06:32.46)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00004 | RUNNING    | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:43 (running for 00:06:37.46)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00004 | RUNNING    | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00004:\n",
      "  date: 2022-08-02_01-15-44\n",
      "  done: false\n",
      "  experiment_id: 369c918c39ef4fd3a712446c3d6b5dc2\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.50449538230896\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86096\n",
      "  time_since_restore: 10.084832668304443\n",
      "  time_this_iter_s: 10.084832668304443\n",
      "  time_total_s: 10.084832668304443\n",
      "  timestamp: 1659395744\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00004\n",
      "  warmup_time: 0.0016884803771972656\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:49 (running for 00:06:44.02)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00004 | RUNNING    | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.504495 |                    1 |\n",
      "| train_f0d85_00005 | PENDING    |                      |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00004:\n",
      "  date: 2022-08-02_01-15-53\n",
      "  done: true\n",
      "  experiment_id: 369c918c39ef4fd3a712446c3d6b5dc2\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.3144087493419647\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86096\n",
      "  time_since_restore: 18.806347131729126\n",
      "  time_this_iter_s: 8.721514463424683\n",
      "  time_total_s: 18.806347131729126\n",
      "  timestamp: 1659395753\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00004\n",
      "  warmup_time: 0.0016884803771972656\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 0 | lstm            | LSTM       | 1.8 M \n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 1 | observation_net | Sequential | 66.6 K\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 1.9 M     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 1.9 M     Total params\n",
      "\u001B[2m\u001B[36m(train pid=86300)\u001B[0m 7.651     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:15:59 (running for 00:06:53.46)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:04 (running for 00:06:58.47)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:09 (running for 00:07:03.47)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:14 (running for 00:07:08.48)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:19 (running for 00:07:13.48)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:24 (running for 00:07:18.49)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:29 (running for 00:07:23.49)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:34 (running for 00:07:28.50)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:39 (running for 00:07:33.51)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:44 (running for 00:07:38.51)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:49 (running for 00:07:43.52)\n",
      "Memory usage on this node: 11.7/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:54 (running for 00:07:48.52)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:16:59 (running for 00:07:53.53)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:04 (running for 00:07:58.53)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:09 (running for 00:08:03.54)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:14 (running for 00:08:08.54)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:19 (running for 00:08:13.55)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:24 (running for 00:08:18.55)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:29 (running for 00:08:23.56)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:34 (running for 00:08:28.56)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:39 (running for 00:08:33.57)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:44 (running for 00:08:38.57)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:49 (running for 00:08:43.58)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:54 (running for 00:08:48.58)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:17:59 (running for 00:08:53.59)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:04 (running for 00:08:58.59)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:09 (running for 00:09:03.60)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:14 (running for 00:09:08.60)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:19 (running for 00:09:13.61)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:24 (running for 00:09:18.61)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:29 (running for 00:09:23.62)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:34 (running for 00:09:28.62)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:39 (running for 00:09:33.63)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:44 (running for 00:09:38.63)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:49 (running for 00:09:43.64)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:54 (running for 00:09:48.64)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:18:59 (running for 00:09:53.65)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:04 (running for 00:09:58.65)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:09 (running for 00:10:03.65)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:14 (running for 00:10:08.66)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:19 (running for 00:10:13.67)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:24 (running for 00:10:18.67)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:29 (running for 00:10:23.67)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:34 (running for 00:10:28.68)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:39 (running for 00:10:33.69)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:44 (running for 00:10:38.69)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:49 (running for 00:10:43.70)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:54 (running for 00:10:48.70)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:19:59 (running for 00:10:53.71)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:04 (running for 00:10:58.71)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:09 (running for 00:11:03.72)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:14 (running for 00:11:08.72)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:19 (running for 00:11:13.73)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:24 (running for 00:11:18.73)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:29 (running for 00:11:23.74)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:34 (running for 00:11:28.74)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:39 (running for 00:11:33.75)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:44 (running for 00:11:38.75)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00005:\n",
      "  date: 2022-08-02_01-20-49\n",
      "  done: false\n",
      "  experiment_id: 94b40f4a550b4d00ad8590ccfac5ed58\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.18638767302036285\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86300\n",
      "  time_since_restore: 293.36912059783936\n",
      "  time_this_iter_s: 293.36912059783936\n",
      "  time_total_s: 293.36912059783936\n",
      "  timestamp: 1659396049\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00005\n",
      "  warmup_time: 0.0015761852264404297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:54 (running for 00:11:48.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:20:59 (running for 00:11:53.33)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:04 (running for 00:11:58.34)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:09 (running for 00:12:03.34)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:14 (running for 00:12:08.35)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:19 (running for 00:12:13.35)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:24 (running for 00:12:18.36)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:29 (running for 00:12:23.36)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:34 (running for 00:12:28.37)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:39 (running for 00:12:33.37)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:44 (running for 00:12:38.38)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:49 (running for 00:12:43.38)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:54 (running for 00:12:48.39)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:21:59 (running for 00:12:53.39)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:04 (running for 00:12:58.40)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:09 (running for 00:13:03.40)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:14 (running for 00:13:08.41)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:19 (running for 00:13:13.41)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:24 (running for 00:13:18.42)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:29 (running for 00:13:23.42)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:34 (running for 00:13:28.43)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:39 (running for 00:13:33.44)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:44 (running for 00:13:38.44)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:49 (running for 00:13:43.45)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:54 (running for 00:13:48.45)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:22:59 (running for 00:13:53.46)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:04 (running for 00:13:58.46)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:09 (running for 00:14:03.47)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:14 (running for 00:14:08.47)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:19 (running for 00:14:13.48)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:24 (running for 00:14:18.49)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:29 (running for 00:14:23.49)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:34 (running for 00:14:28.50)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:39 (running for 00:14:33.50)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:44 (running for 00:14:38.51)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:49 (running for 00:14:43.51)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:54 (running for 00:14:48.51)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:23:59 (running for 00:14:53.52)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:04 (running for 00:14:58.52)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:09 (running for 00:15:03.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:14 (running for 00:15:08.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:19 (running for 00:15:13.54)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:24 (running for 00:15:18.54)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:29 (running for 00:15:23.55)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:34 (running for 00:15:28.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:39 (running for 00:15:33.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:44 (running for 00:15:38.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:49 (running for 00:15:43.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:54 (running for 00:15:48.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:24:59 (running for 00:15:53.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:04 (running for 00:15:58.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:09 (running for 00:16:03.59)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:14 (running for 00:16:08.59)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:19 (running for 00:16:13.60)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:24 (running for 00:16:18.60)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:29 (running for 00:16:23.61)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:34 (running for 00:16:28.61)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:39 (running for 00:16:33.62)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00005 with loss=0.18638767302036285 and parameters={'input_size': 3, 'hidden_size': 256, 'num_layers': 4, 'dropout': 0.23850878450810642, 'batch_size': 4, 'optimizer': 'rmsprop', 'learning_rate': 0.00020511142033739766, 'gradient_clip': None, 'sequence_length': 2016, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.186388 |                    1 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00005:\n",
      "  date: 2022-08-02_01-25-41\n",
      "  done: false\n",
      "  experiment_id: 94b40f4a550b4d00ad8590ccfac5ed58\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.22780460119247437\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86300\n",
      "  time_since_restore: 585.611748456955\n",
      "  time_this_iter_s: 292.2426278591156\n",
      "  time_total_s: 585.611748456955\n",
      "  timestamp: 1659396341\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00005\n",
      "  warmup_time: 0.0015761852264404297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:46 (running for 00:16:40.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:51 (running for 00:16:45.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:25:56 (running for 00:16:50.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:01 (running for 00:16:55.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:06 (running for 00:17:00.59)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:11 (running for 00:17:05.59)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:16 (running for 00:17:10.60)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:21 (running for 00:17:15.60)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:26 (running for 00:17:20.61)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:31 (running for 00:17:25.61)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:36 (running for 00:17:30.62)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:41 (running for 00:17:35.63)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:46 (running for 00:17:40.63)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:51 (running for 00:17:45.64)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:26:56 (running for 00:17:50.64)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:01 (running for 00:17:55.65)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:06 (running for 00:18:00.65)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:11 (running for 00:18:05.66)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:16 (running for 00:18:10.66)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:21 (running for 00:18:15.67)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:26 (running for 00:18:20.67)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:31 (running for 00:18:25.68)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:36 (running for 00:18:30.68)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:41 (running for 00:18:35.69)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:46 (running for 00:18:40.69)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:51 (running for 00:18:45.70)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:27:56 (running for 00:18:50.70)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:01 (running for 00:18:55.71)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:06 (running for 00:19:00.71)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:11 (running for 00:19:05.72)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:16 (running for 00:19:10.72)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:21 (running for 00:19:15.73)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:26 (running for 00:19:20.73)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:31 (running for 00:19:25.74)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:36 (running for 00:19:30.74)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:41 (running for 00:19:35.75)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:46 (running for 00:19:40.75)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:51 (running for 00:19:45.76)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:28:56 (running for 00:19:50.76)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:01 (running for 00:19:55.77)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:06 (running for 00:20:00.77)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:11 (running for 00:20:05.78)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:16 (running for 00:20:10.78)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:21 (running for 00:20:15.79)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:26 (running for 00:20:20.79)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:31 (running for 00:20:25.80)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:36 (running for 00:20:30.80)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:41 (running for 00:20:35.81)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:46 (running for 00:20:40.81)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:51 (running for 00:20:45.82)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:29:56 (running for 00:20:50.82)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:01 (running for 00:20:55.83)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:06 (running for 00:21:00.83)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:11 (running for 00:21:05.84)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:16 (running for 00:21:10.84)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:21 (running for 00:21:15.85)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:26 (running for 00:21:20.85)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:31 (running for 00:21:25.86)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.227805 |                    2 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00005:\n",
      "  date: 2022-08-02_01-30-36\n",
      "  done: false\n",
      "  experiment_id: 94b40f4a550b4d00ad8590ccfac5ed58\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 3\n",
      "  loss: 0.2342565357685089\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86300\n",
      "  time_since_restore: 880.843405008316\n",
      "  time_this_iter_s: 295.2316565513611\n",
      "  time_total_s: 880.843405008316\n",
      "  timestamp: 1659396636\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: f0d85_00005\n",
      "  warmup_time: 0.0015761852264404297\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:41 (running for 00:21:35.80)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:46 (running for 00:21:40.81)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:51 (running for 00:21:45.81)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:30:56 (running for 00:21:50.82)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:01 (running for 00:21:55.82)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:06 (running for 00:22:00.83)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:11 (running for 00:22:05.83)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:16 (running for 00:22:10.84)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:21 (running for 00:22:15.84)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:26 (running for 00:22:20.85)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:31 (running for 00:22:25.85)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:36 (running for 00:22:30.86)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:41 (running for 00:22:35.86)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:46 (running for 00:22:40.87)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:51 (running for 00:22:45.87)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:31:56 (running for 00:22:50.88)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:01 (running for 00:22:55.88)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:06 (running for 00:23:00.89)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:11 (running for 00:23:05.89)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:16 (running for 00:23:10.89)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:21 (running for 00:23:15.90)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:26 (running for 00:23:20.91)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:31 (running for 00:23:25.91)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:36 (running for 00:23:30.92)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:41 (running for 00:23:35.92)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:46 (running for 00:23:40.92)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:51 (running for 00:23:45.93)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:32:56 (running for 00:23:50.94)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:01 (running for 00:23:55.94)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:06 (running for 00:24:00.95)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:11 (running for 00:24:05.95)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:16 (running for 00:24:10.96)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:21 (running for 00:24:15.96)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:26 (running for 00:24:20.96)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:31 (running for 00:24:25.97)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:36 (running for 00:24:30.98)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:41 (running for 00:24:35.98)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:46 (running for 00:24:40.98)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:51 (running for 00:24:45.99)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:33:56 (running for 00:24:50.99)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:01 (running for 00:24:56.00)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:06 (running for 00:25:01.00)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:11 (running for 00:25:06.01)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:16 (running for 00:25:11.01)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:21 (running for 00:25:16.02)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:26 (running for 00:25:21.02)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:31 (running for 00:25:26.03)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:36 (running for 00:25:31.03)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:41 (running for 00:25:36.04)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:46 (running for 00:25:41.04)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:51 (running for 00:25:46.05)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:34:56 (running for 00:25:51.05)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:01 (running for 00:25:56.06)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:06 (running for 00:26:01.07)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:11 (running for 00:26:06.07)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:16 (running for 00:26:11.08)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:21 (running for 00:26:16.08)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:27 (running for 00:26:21.09)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:32 (running for 00:26:26.09)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.23501648008823395 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00005 | RUNNING    | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.234257 |                    3 |\n",
      "| train_f0d85_00006 | PENDING    |                      |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00005:\n",
      "  date: 2022-08-02_01-35-35\n",
      "  done: true\n",
      "  experiment_id: 94b40f4a550b4d00ad8590ccfac5ed58\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.26330500841140747\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 86300\n",
      "  time_since_restore: 1179.3158357143402\n",
      "  time_this_iter_s: 298.47243070602417\n",
      "  time_total_s: 1179.3158357143402\n",
      "  timestamp: 1659396935\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: f0d85_00005\n",
      "  warmup_time: 0.0015761852264404297\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.48073321197269525 and num_layers=1\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 0 | lstm            | LSTM       | 267 K \n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 1 | observation_net | Sequential | 66.6 K\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 333 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 333 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=87079)\u001B[0m 1.335     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:40 (running for 00:26:34.51)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00006 | RUNNING    | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:45 (running for 00:26:39.52)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00006 | RUNNING    | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00006:\n",
      "  date: 2022-08-02_01-35-48\n",
      "  done: false\n",
      "  experiment_id: 0f96b71bfe7545fca99ce9152a54e45f\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.287674218416214\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87079\n",
      "  time_since_restore: 11.86731481552124\n",
      "  time_this_iter_s: 11.86731481552124\n",
      "  time_total_s: 11.86731481552124\n",
      "  timestamp: 1659396948\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00006\n",
      "  warmup_time: 0.0017333030700683594\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:53 (running for 00:26:47.93)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00006 | RUNNING    | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.287674 |                    1 |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:35:58 (running for 00:26:52.94)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00006 | RUNNING    | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.287674 |                    1 |\n",
      "| train_f0d85_00007 | PENDING    |                      |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00006:\n",
      "  date: 2022-08-02_01-35-59\n",
      "  done: true\n",
      "  experiment_id: 0f96b71bfe7545fca99ce9152a54e45f\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.24524471163749695\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87079\n",
      "  time_since_restore: 22.363465070724487\n",
      "  time_this_iter_s: 10.496150255203247\n",
      "  time_total_s: 22.363465070724487\n",
      "  timestamp: 1659396959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00006\n",
      "  warmup_time: 0.0017333030700683594\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 0 | lstm            | LSTM       | 200 K \n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 217 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 217 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=87286)\u001B[0m 0.868     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:04 (running for 00:26:58.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:09 (running for 00:27:03.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:14 (running for 00:27:08.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:19 (running for 00:27:13.54)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00007:\n",
      "  date: 2022-08-02_01-36-21\n",
      "  done: false\n",
      "  experiment_id: e01dadd6f3f44825bd205827626662d3\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.21846893429756165\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87286\n",
      "  time_since_restore: 20.92972207069397\n",
      "  time_this_iter_s: 20.92972207069397\n",
      "  time_total_s: 20.92972207069397\n",
      "  timestamp: 1659396981\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00007\n",
      "  warmup_time: 0.0017385482788085938\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:26 (running for 00:27:20.98)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.218469 |                    1 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:31 (running for 00:27:25.98)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.218469 |                    1 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:36 (running for 00:27:30.99)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.218469 |                    1 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00007:\n",
      "  date: 2022-08-02_01-36-41\n",
      "  done: false\n",
      "  experiment_id: e01dadd6f3f44825bd205827626662d3\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.1770053505897522\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87286\n",
      "  time_since_restore: 40.511574268341064\n",
      "  time_this_iter_s: 19.581852197647095\n",
      "  time_total_s: 40.511574268341064\n",
      "  timestamp: 1659397001\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00007\n",
      "  warmup_time: 0.0017385482788085938\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:46 (running for 00:27:40.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1770053505897522 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.177005 |                    2 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:51 (running for 00:27:45.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1770053505897522 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.177005 |                    2 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:36:56 (running for 00:27:50.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1770053505897522 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.177005 |                    2 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00007:\n",
      "  date: 2022-08-02_01-37-00\n",
      "  done: false\n",
      "  experiment_id: e01dadd6f3f44825bd205827626662d3\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 3\n",
      "  loss: 0.1963454782962799\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87286\n",
      "  time_since_restore: 60.03986191749573\n",
      "  time_this_iter_s: 19.528287649154663\n",
      "  time_total_s: 60.03986191749573\n",
      "  timestamp: 1659397020\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: f0d85_00007\n",
      "  warmup_time: 0.0017385482788085938\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:05 (running for 00:28:00.09)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1963454782962799 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.196345 |                    3 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:11 (running for 00:28:05.09)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1963454782962799 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.196345 |                    3 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:16 (running for 00:28:10.10)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2491607442498207 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00007 with loss=0.1963454782962799 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.39024110723871963, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0003290376511455907, 'gradient_clip': None, 'sequence_length': 1728, 'prediction_length': 288, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00007 | RUNNING    | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.196345 |                    3 |\n",
      "| train_f0d85_00008 | PENDING    |                      |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00007:\n",
      "  date: 2022-08-02_01-37-20\n",
      "  done: true\n",
      "  experiment_id: e01dadd6f3f44825bd205827626662d3\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.2553507089614868\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87286\n",
      "  time_since_restore: 79.56115937232971\n",
      "  time_this_iter_s: 19.521297454833984\n",
      "  time_total_s: 79.56115937232971\n",
      "  timestamp: 1659397040\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: f0d85_00007\n",
      "  warmup_time: 0.0017385482788085938\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:21 (running for 00:28:15.52)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00008 | RUNNING    | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 0 | lstm            | LSTM       | 464 K \n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 1 | observation_net | Sequential | 16.9 K\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 481 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 481 K     Total params\n",
      "\u001B[2m\u001B[36m(train pid=87586)\u001B[0m 1.925     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:27 (running for 00:28:22.05)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00008 | RUNNING    | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:32 (running for 00:28:27.05)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00008 | RUNNING    | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00008:\n",
      "  date: 2022-08-02_01-37-35\n",
      "  done: false\n",
      "  experiment_id: 348c85338d804750b32a1321a1f46fca\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.4562973380088806\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87586\n",
      "  time_since_restore: 12.737505674362183\n",
      "  time_this_iter_s: 12.737505674362183\n",
      "  time_total_s: 12.737505674362183\n",
      "  timestamp: 1659397055\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00008\n",
      "  warmup_time: 0.0015892982482910156\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:40 (running for 00:28:34.78)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00008 | RUNNING    | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.456297 |                    1 |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:45 (running for 00:28:39.79)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.2386663779616356\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00008 | RUNNING    | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.456297 |                    1 |\n",
      "| train_f0d85_00009 | PENDING    |                      |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00008:\n",
      "  date: 2022-08-02_01-37-47\n",
      "  done: true\n",
      "  experiment_id: 348c85338d804750b32a1321a1f46fca\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.7902047038078308\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87586\n",
      "  time_since_restore: 24.164342880249023\n",
      "  time_this_iter_s: 11.42683720588684\n",
      "  time_total_s: 24.164342880249023\n",
      "  timestamp: 1659397067\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00008\n",
      "  warmup_time: 0.0015892982482910156\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m /home/lars/code/python/TSA-RNN-final-project/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m   | Name            | Type       | Params\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 0 | lstm            | LSTM       | 1.8 M \n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 1 | observation_net | Sequential | 66.6 K\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 2 | loss            | MSELoss    | 0     \n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m -----------------------------------------------\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 1.9 M     Trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 1.9 M     Total params\n",
      "\u001B[2m\u001B[36m(train pid=87793)\u001B[0m 7.651     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:52 (running for 00:28:46.52)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:37:57 (running for 00:28:51.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:02 (running for 00:28:56.53)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:07 (running for 00:29:01.54)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:12 (running for 00:29:06.54)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:17 (running for 00:29:11.55)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:22 (running for 00:29:16.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:27 (running for 00:29:21.56)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:32 (running for 00:29:26.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:37 (running for 00:29:31.57)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:42 (running for 00:29:36.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:47 (running for 00:29:41.58)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:52 (running for 00:29:46.59)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 |          |                      |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Result for train_f0d85_00009:\n",
      "  date: 2022-08-02_01-38-54\n",
      "  done: false\n",
      "  experiment_id: f6349b9d644b462dbb7373496b30f1a2\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.9199745059013367\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87793\n",
      "  time_since_restore: 65.29566717147827\n",
      "  time_this_iter_s: 65.29566717147827\n",
      "  time_total_s: 65.29566717147827\n",
      "  timestamp: 1659397134\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f0d85_00009\n",
      "  warmup_time: 0.0015716552734375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:38:59 (running for 00:29:53.31)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:04 (running for 00:29:58.31)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:09 (running for 00:30:03.32)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:14 (running for 00:30:08.32)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:19 (running for 00:30:13.33)\n",
      "Memory usage on this node: 12.0/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:24 (running for 00:30:18.33)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:29 (running for 00:30:23.34)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:34 (running for 00:30:28.34)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:39 (running for 00:30:33.35)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:44 (running for 00:30:38.35)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:49 (running for 00:30:43.36)\n",
      "Memory usage on this node: 11.9/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:54 (running for 00:30:48.37)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.23941738903522491\n",
      "Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00009 | RUNNING    | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.919975 |                    1 |\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 01:39:57,265\tINFO tune.py:747 -- Total run time: 1851.38 seconds (1851.25 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_f0d85_00009:\n",
      "  date: 2022-08-02_01-39-57\n",
      "  done: true\n",
      "  experiment_id: f6349b9d644b462dbb7373496b30f1a2\n",
      "  hostname: lars-desktop\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.9246317744255066\n",
      "  node_ip: 192.168.178.30\n",
      "  pid: 87793\n",
      "  time_since_restore: 128.23609352111816\n",
      "  time_this_iter_s: 62.94042634963989\n",
      "  time_total_s: 128.23609352111816\n",
      "  timestamp: 1659397197\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f0d85_00009\n",
      "  warmup_time: 0.0015716552734375\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-08-02 01:39:57 (running for 00:30:51.25)\n",
      "Memory usage on this node: 11.8/31.3 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 16.000: -0.20088671147823334 | Iter 8.000: -0.23072946071624756 | Iter 4.000: -0.2553507089614868 | Iter 2.000: -0.24233105033636093\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/13.5 GiB heap, 0.0/6.75 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Current best trial: f0d85_00000 with loss=0.20088671147823334 and parameters={'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n",
      "Result logdir: /home/lars/code/python/TSA-RNN-final-project/ray_results/tune_asha\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "| Trial name        | status     | loc                  |   input_size |   hidden_size |   num_layers |   dropout |   batch_size | optimizer   |   learning_rate | gradient_clip   |   sequence_length |   prediction_length | lr_warmup_milestones   |   lr_warmup_gamma | lr_milestones   |   lr_gamma |     loss |   training_iteration |\n",
      "|-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------|\n",
      "| train_f0d85_00000 | TERMINATED | 192.168.178.30:84603 |            3 |           128 |            4 |  0.447456 |            8 | adam        |     0.00214234  |                 |              1440 |                 864 | []                     |                10 | []              |        0.1 | 0.200887 |                   16 |\n",
      "| train_f0d85_00001 | TERMINATED | 192.168.178.30:85463 |            3 |           256 |            1 |  0.400725 |           16 | rmsprop     |     0.00318879  |                 |              1728 |                 864 | []                     |                10 | []              |        0.1 | 0.239417 |                    2 |\n",
      "| train_f0d85_00002 | TERMINATED | 192.168.178.30:85674 |            3 |           128 |            4 |  0.245685 |            8 | rmsprop     |     0.000125963 |                 |              1176 |                 288 | []                     |                10 | []              |        0.1 | 0.237915 |                    2 |\n",
      "| train_f0d85_00003 | TERMINATED | 192.168.178.30:85893 |            3 |           128 |            1 |  0.060178 |            8 | adam        |     0.00110079  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.581504 |                    2 |\n",
      "| train_f0d85_00004 | TERMINATED | 192.168.178.30:86096 |            3 |           128 |            2 |  0.472697 |           16 | rmsprop     |     0.000136743 |                 |              2016 |                 864 | []                     |                10 | []              |        0.1 | 0.314409 |                    2 |\n",
      "| train_f0d85_00005 | TERMINATED | 192.168.178.30:86300 |            3 |           256 |            4 |  0.238509 |            4 | rmsprop     |     0.000205111 |                 |              2016 |                 288 | []                     |                10 | []              |        0.1 | 0.263305 |                    4 |\n",
      "| train_f0d85_00006 | TERMINATED | 192.168.178.30:87079 |            3 |           256 |            1 |  0.480733 |           16 | adam        |     0.00171149  |                 |              1728 |                1128 | []                     |                10 | []              |        0.1 | 0.245245 |                    2 |\n",
      "| train_f0d85_00007 | TERMINATED | 192.168.178.30:87286 |            3 |           128 |            2 |  0.390241 |            8 | adam        |     0.000329038 |                 |              1728 |                 288 | []                     |                10 | []              |        0.1 | 0.255351 |                    4 |\n",
      "| train_f0d85_00008 | TERMINATED | 192.168.178.30:87586 |            3 |           128 |            4 |  0.318951 |           64 | adam        |     0.00112177  |                 |              1440 |                 288 | []                     |                10 | []              |        0.1 | 0.790205 |                    2 |\n",
      "| train_f0d85_00009 | TERMINATED | 192.168.178.30:87793 |            3 |           256 |            4 |  0.382931 |            4 | adam        |     0.00972333  |                 |              1176 |                 864 | []                     |                10 | []              |        0.1 | 0.924632 |                    2 |\n",
      "+-------------------+------------+----------------------+--------------+---------------+--------------+-----------+--------------+-------------+-----------------+-----------------+-------------------+---------------------+------------------------+-------------------+-----------------+------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = tune_hparams(config, metrics)\n",
    "hparams = analysis.best_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were: {'input_size': 3, 'hidden_size': 128, 'num_layers': 4, 'dropout': 0.447455675840233, 'batch_size': 8, 'optimizer': 'adam', 'learning_rate': 0.0021423382951331524, 'gradient_clip': None, 'sequence_length': 1440, 'prediction_length': 864, 'lr_warmup_milestones': [], 'lr_warmup_gamma': 10.0, 'lr_milestones': [], 'lr_gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best hyperparameters found were: {hparams}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | lstm            | LSTM       | 464 K \n",
      "1 | observation_net | Sequential | 16.9 K\n",
      "2 | loss            | MSELoss    | 0     \n",
      "-----------------------------------------------\n",
      "481 K     Trainable params\n",
      "0         Non-trainable params\n",
      "481 K     Total params\n",
      "1.925     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  89%|âââââââââ | 1093/1232 [00:17<00:02, 64.07it/s, loss=0.178, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  89%|âââââââââ | 1094/1232 [00:17<00:02, 63.67it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1095/1232 [00:17<00:02, 63.71it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1096/1232 [00:17<00:02, 63.76it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1097/1232 [00:17<00:02, 63.81it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1098/1232 [00:17<00:02, 63.86it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1099/1232 [00:17<00:02, 63.91it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1100/1232 [00:17<00:02, 63.96it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1101/1232 [00:17<00:02, 64.01it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  89%|âââââââââ | 1102/1232 [00:17<00:02, 64.05it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1103/1232 [00:17<00:02, 64.10it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1104/1232 [00:17<00:01, 64.15it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1105/1232 [00:17<00:01, 64.20it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1106/1232 [00:17<00:01, 64.24it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1107/1232 [00:17<00:01, 64.29it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1108/1232 [00:17<00:01, 64.34it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1109/1232 [00:17<00:01, 64.39it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1110/1232 [00:17<00:01, 64.44it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1111/1232 [00:17<00:01, 64.49it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1112/1232 [00:17<00:01, 64.54it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1113/1232 [00:17<00:01, 64.59it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  90%|âââââââââ | 1114/1232 [00:17<00:01, 64.64it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1115/1232 [00:17<00:01, 64.68it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1116/1232 [00:17<00:01, 64.73it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1117/1232 [00:17<00:01, 64.77it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1118/1232 [00:17<00:01, 64.82it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1119/1232 [00:17<00:01, 64.87it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1120/1232 [00:17<00:01, 64.92it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1121/1232 [00:17<00:01, 64.97it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1122/1232 [00:17<00:01, 65.02it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1123/1232 [00:17<00:01, 65.06it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|âââââââââ | 1124/1232 [00:17<00:01, 65.11it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 65.16it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 65.21it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 65.26it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 65.31it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 65.36it/s, loss=0.178, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 369.20it/s]\u001B[A\n",
      "Epoch 0:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 65.40it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 65.45it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 65.50it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 65.55it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 65.60it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 65.64it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 65.69it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 65.74it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 65.79it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 65.84it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 65.89it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 65.94it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 65.99it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 66.03it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 66.08it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 66.12it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 66.17it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 66.22it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 66.27it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 66.32it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 66.36it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 66.41it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 66.46it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 66.51it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 66.56it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 66.61it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 66.66it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 66.70it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 66.75it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 66.80it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 66.85it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 66.90it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 66.94it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 66.99it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 67.04it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1165/1232 [00:17<00:00, 67.09it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1166/1232 [00:17<00:00, 67.14it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1167/1232 [00:17<00:00, 67.19it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1168/1232 [00:17<00:00, 67.24it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1169/1232 [00:17<00:00, 67.29it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1170/1232 [00:17<00:00, 67.33it/s, loss=0.178, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 389.46it/s]\u001B[A\n",
      "Epoch 0:  95%|ââââââââââ| 1171/1232 [00:17<00:00, 67.38it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1172/1232 [00:17<00:00, 67.43it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1173/1232 [00:17<00:00, 67.48it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1174/1232 [00:17<00:00, 67.53it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1175/1232 [00:17<00:00, 67.57it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  95%|ââââââââââ| 1176/1232 [00:17<00:00, 67.62it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1177/1232 [00:17<00:00, 67.67it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1178/1232 [00:17<00:00, 67.72it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1179/1232 [00:17<00:00, 67.77it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1180/1232 [00:17<00:00, 67.81it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1181/1232 [00:17<00:00, 67.86it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1182/1232 [00:17<00:00, 67.91it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1183/1232 [00:17<00:00, 67.95it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1184/1232 [00:17<00:00, 68.00it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1185/1232 [00:17<00:00, 68.05it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1186/1232 [00:17<00:00, 68.10it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1187/1232 [00:17<00:00, 68.15it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  96%|ââââââââââ| 1188/1232 [00:17<00:00, 68.19it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1189/1232 [00:17<00:00, 68.24it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1190/1232 [00:17<00:00, 68.29it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1191/1232 [00:17<00:00, 68.34it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1192/1232 [00:17<00:00, 68.39it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1193/1232 [00:17<00:00, 68.44it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1194/1232 [00:17<00:00, 68.49it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1195/1232 [00:17<00:00, 68.53it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1196/1232 [00:17<00:00, 68.58it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1197/1232 [00:17<00:00, 68.63it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1198/1232 [00:17<00:00, 68.67it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1199/1232 [00:17<00:00, 68.72it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1200/1232 [00:17<00:00, 68.77it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  97%|ââââââââââ| 1201/1232 [00:17<00:00, 68.82it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1202/1232 [00:17<00:00, 68.87it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1203/1232 [00:17<00:00, 68.91it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1204/1232 [00:17<00:00, 68.96it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1205/1232 [00:17<00:00, 69.01it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1206/1232 [00:17<00:00, 69.06it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1207/1232 [00:17<00:00, 69.10it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1208/1232 [00:17<00:00, 69.15it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1209/1232 [00:17<00:00, 69.20it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1210/1232 [00:17<00:00, 69.25it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1211/1232 [00:17<00:00, 69.29it/s, loss=0.178, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 119/139 [00:00<00:00, 398.79it/s]\u001B[A\n",
      "Epoch 0:  98%|ââââââââââ| 1212/1232 [00:17<00:00, 69.34it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  98%|ââââââââââ| 1213/1232 [00:17<00:00, 69.38it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1214/1232 [00:17<00:00, 69.41it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1215/1232 [00:17<00:00, 69.45it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1216/1232 [00:17<00:00, 69.48it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1217/1232 [00:17<00:00, 69.52it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1218/1232 [00:17<00:00, 69.55it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1219/1232 [00:17<00:00, 69.58it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1220/1232 [00:17<00:00, 69.61it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1221/1232 [00:17<00:00, 69.65it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1222/1232 [00:17<00:00, 69.68it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1223/1232 [00:17<00:00, 69.72it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1224/1232 [00:17<00:00, 69.75it/s, loss=0.178, v_num=1]\n",
      "Epoch 0:  99%|ââââââââââ| 1225/1232 [00:17<00:00, 69.79it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1226/1232 [00:17<00:00, 69.82it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1227/1232 [00:17<00:00, 69.85it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1228/1232 [00:17<00:00, 69.89it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1229/1232 [00:17<00:00, 69.92it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1230/1232 [00:17<00:00, 69.96it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1231/1232 [00:17<00:00, 69.99it/s, loss=0.178, v_num=1]\n",
      "Epoch 0: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 68.42it/s, loss=0.178, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1093/1232 [00:17<00:02, 64.09it/s, loss=0.062, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  89%|âââââââââ | 1094/1232 [00:17<00:02, 63.63it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1095/1232 [00:17<00:02, 63.68it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1096/1232 [00:17<00:02, 63.73it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1097/1232 [00:17<00:02, 63.78it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1098/1232 [00:17<00:02, 63.83it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1099/1232 [00:17<00:02, 63.88it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1100/1232 [00:17<00:02, 63.93it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1101/1232 [00:17<00:02, 63.97it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  89%|âââââââââ | 1102/1232 [00:17<00:02, 64.02it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1103/1232 [00:17<00:02, 64.07it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1104/1232 [00:17<00:01, 64.12it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1105/1232 [00:17<00:01, 64.17it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1106/1232 [00:17<00:01, 64.22it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1107/1232 [00:17<00:01, 64.26it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1108/1232 [00:17<00:01, 64.31it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1109/1232 [00:17<00:01, 64.36it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1110/1232 [00:17<00:01, 64.41it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1111/1232 [00:17<00:01, 64.46it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1112/1232 [00:17<00:01, 64.51it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1113/1232 [00:17<00:01, 64.55it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  90%|âââââââââ | 1114/1232 [00:17<00:01, 64.60it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1115/1232 [00:17<00:01, 64.65it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1116/1232 [00:17<00:01, 64.70it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1117/1232 [00:17<00:01, 64.75it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1118/1232 [00:17<00:01, 64.80it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1119/1232 [00:17<00:01, 64.85it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1120/1232 [00:17<00:01, 64.90it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1121/1232 [00:17<00:01, 64.95it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1122/1232 [00:17<00:01, 64.99it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1123/1232 [00:17<00:01, 65.04it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|âââââââââ | 1124/1232 [00:17<00:01, 65.09it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 65.14it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 65.19it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 65.24it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 65.29it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 65.34it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 65.39it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 65.44it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 65.49it/s, loss=0.062, v_num=1]\n",
      "Validation DataLoader 0:  29%|âââ       | 40/139 [00:00<00:00, 392.76it/s]\u001B[A\n",
      "Epoch 1:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 65.54it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 65.58it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 65.63it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 65.68it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 65.73it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 65.78it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 65.83it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 65.88it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 65.92it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 65.97it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 66.02it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 66.06it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 66.11it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 66.16it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 66.21it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 66.26it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 66.31it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 66.36it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 66.41it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 66.45it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 66.50it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 66.55it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 66.60it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 66.65it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 66.70it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 66.75it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 66.80it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 66.85it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 66.89it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 66.94it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 66.99it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 67.04it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1165/1232 [00:17<00:00, 67.08it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1166/1232 [00:17<00:00, 67.13it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1167/1232 [00:17<00:00, 67.18it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1168/1232 [00:17<00:00, 67.22it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1169/1232 [00:17<00:00, 67.27it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1170/1232 [00:17<00:00, 67.32it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1171/1232 [00:17<00:00, 67.36it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1172/1232 [00:17<00:00, 67.41it/s, loss=0.062, v_num=1]\n",
      "Validation DataLoader 0:  58%|ââââââ    | 80/139 [00:00<00:00, 395.01it/s]\u001B[A\n",
      "Epoch 1:  95%|ââââââââââ| 1173/1232 [00:17<00:00, 67.45it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1174/1232 [00:17<00:00, 67.50it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1175/1232 [00:17<00:00, 67.55it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  95%|ââââââââââ| 1176/1232 [00:17<00:00, 67.59it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1177/1232 [00:17<00:00, 67.64it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1178/1232 [00:17<00:00, 67.69it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1179/1232 [00:17<00:00, 67.74it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1180/1232 [00:17<00:00, 67.78it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1181/1232 [00:17<00:00, 67.83it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1182/1232 [00:17<00:00, 67.88it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1183/1232 [00:17<00:00, 67.92it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1184/1232 [00:17<00:00, 67.97it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1185/1232 [00:17<00:00, 68.02it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1186/1232 [00:17<00:00, 68.06it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1187/1232 [00:17<00:00, 68.11it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  96%|ââââââââââ| 1188/1232 [00:17<00:00, 68.16it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1189/1232 [00:17<00:00, 68.20it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1190/1232 [00:17<00:00, 68.25it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1191/1232 [00:17<00:00, 68.29it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1192/1232 [00:17<00:00, 68.34it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1193/1232 [00:17<00:00, 68.39it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1194/1232 [00:17<00:00, 68.44it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1195/1232 [00:17<00:00, 68.48it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1196/1232 [00:17<00:00, 68.53it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1197/1232 [00:17<00:00, 68.58it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1198/1232 [00:17<00:00, 68.63it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1199/1232 [00:17<00:00, 68.68it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1200/1232 [00:17<00:00, 68.73it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  97%|ââââââââââ| 1201/1232 [00:17<00:00, 68.77it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1202/1232 [00:17<00:00, 68.82it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1203/1232 [00:17<00:00, 68.87it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1204/1232 [00:17<00:00, 68.92it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1205/1232 [00:17<00:00, 68.97it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1206/1232 [00:17<00:00, 69.02it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1207/1232 [00:17<00:00, 69.07it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1208/1232 [00:17<00:00, 69.11it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1209/1232 [00:17<00:00, 69.16it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1210/1232 [00:17<00:00, 69.21it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1211/1232 [00:17<00:00, 69.26it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  98%|ââââââââââ| 1212/1232 [00:17<00:00, 69.31it/s, loss=0.062, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 120/139 [00:00<00:00, 395.62it/s]\u001B[A\n",
      "Epoch 1:  98%|ââââââââââ| 1213/1232 [00:17<00:00, 69.35it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1214/1232 [00:17<00:00, 69.40it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1215/1232 [00:17<00:00, 69.45it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1216/1232 [00:17<00:00, 69.50it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1217/1232 [00:17<00:00, 69.54it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1218/1232 [00:17<00:00, 69.59it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1219/1232 [00:17<00:00, 69.64it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1220/1232 [00:17<00:00, 69.67it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1221/1232 [00:17<00:00, 69.71it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1222/1232 [00:17<00:00, 69.74it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1223/1232 [00:17<00:00, 69.78it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1224/1232 [00:17<00:00, 69.81it/s, loss=0.062, v_num=1]\n",
      "Epoch 1:  99%|ââââââââââ| 1225/1232 [00:17<00:00, 69.85it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1226/1232 [00:17<00:00, 69.88it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1227/1232 [00:17<00:00, 69.91it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1228/1232 [00:17<00:00, 69.95it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1229/1232 [00:17<00:00, 69.98it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1230/1232 [00:17<00:00, 70.01it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1231/1232 [00:17<00:00, 70.05it/s, loss=0.062, v_num=1]\n",
      "Epoch 1: 100%|ââââââââââ| 1232/1232 [00:17<00:00, 68.47it/s, loss=0.062, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1093/1232 [00:17<00:02, 62.40it/s, loss=0.0433, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.98it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1095/1232 [00:17<00:02, 62.02it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1096/1232 [00:17<00:02, 62.07it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1097/1232 [00:17<00:02, 62.12it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1098/1232 [00:17<00:02, 62.17it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1099/1232 [00:17<00:02, 62.22it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1100/1232 [00:17<00:02, 62.26it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1101/1232 [00:17<00:02, 62.31it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  89%|âââââââââ | 1102/1232 [00:17<00:02, 62.36it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1103/1232 [00:17<00:02, 62.40it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1104/1232 [00:17<00:02, 62.45it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1105/1232 [00:17<00:02, 62.50it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1106/1232 [00:17<00:02, 62.55it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1107/1232 [00:17<00:01, 62.59it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.64it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.69it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.73it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.78it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.83it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.88it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.93it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.97it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1116/1232 [00:17<00:01, 63.02it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1117/1232 [00:17<00:01, 63.07it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1118/1232 [00:17<00:01, 63.12it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1119/1232 [00:17<00:01, 63.16it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1120/1232 [00:17<00:01, 63.21it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1121/1232 [00:17<00:01, 63.26it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1122/1232 [00:17<00:01, 63.31it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1123/1232 [00:17<00:01, 63.35it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|âââââââââ | 1124/1232 [00:17<00:01, 63.40it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 63.45it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 63.49it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 63.54it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 63.59it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.63it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.68it/s, loss=0.0433, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 369.44it/s]\u001B[A\n",
      "Epoch 2:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.72it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.77it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.82it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.86it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.91it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.96it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 64.01it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 64.05it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 64.10it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 64.15it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 64.19it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 64.24it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 64.29it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 64.33it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 64.38it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 64.43it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 64.48it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 64.53it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 64.57it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 64.62it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.67it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.72it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.77it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.81it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.86it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.90it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.95it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 65.00it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 65.04it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 65.09it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 65.14it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 65.18it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 65.23it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 65.28it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1165/1232 [00:17<00:01, 65.33it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1166/1232 [00:17<00:01, 65.37it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1167/1232 [00:17<00:00, 65.42it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1168/1232 [00:17<00:00, 65.47it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1169/1232 [00:17<00:00, 65.52it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1170/1232 [00:17<00:00, 65.56it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1171/1232 [00:17<00:00, 65.61it/s, loss=0.0433, v_num=1]\n",
      "Validation DataLoader 0:  57%|ââââââ    | 79/139 [00:00<00:00, 388.15it/s]\u001B[A\n",
      "Epoch 2:  95%|ââââââââââ| 1172/1232 [00:17<00:00, 65.66it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1173/1232 [00:17<00:00, 65.70it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1174/1232 [00:17<00:00, 65.75it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1175/1232 [00:17<00:00, 65.80it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  95%|ââââââââââ| 1176/1232 [00:17<00:00, 65.84it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1177/1232 [00:17<00:00, 65.89it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1178/1232 [00:17<00:00, 65.93it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1179/1232 [00:17<00:00, 65.98it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1180/1232 [00:17<00:00, 66.02it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1181/1232 [00:17<00:00, 66.07it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1182/1232 [00:17<00:00, 66.12it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1183/1232 [00:17<00:00, 66.17it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1184/1232 [00:17<00:00, 66.21it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1185/1232 [00:17<00:00, 66.26it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1186/1232 [00:17<00:00, 66.31it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1187/1232 [00:17<00:00, 66.35it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  96%|ââââââââââ| 1188/1232 [00:17<00:00, 66.40it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1189/1232 [00:17<00:00, 66.45it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1190/1232 [00:17<00:00, 66.49it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1191/1232 [00:17<00:00, 66.53it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1192/1232 [00:17<00:00, 66.58it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1193/1232 [00:17<00:00, 66.63it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1194/1232 [00:17<00:00, 66.67it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1195/1232 [00:17<00:00, 66.72it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1196/1232 [00:17<00:00, 66.77it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1197/1232 [00:17<00:00, 66.82it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1198/1232 [00:17<00:00, 66.86it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1199/1232 [00:17<00:00, 66.91it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1200/1232 [00:17<00:00, 66.96it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  97%|ââââââââââ| 1201/1232 [00:17<00:00, 67.00it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1202/1232 [00:17<00:00, 67.05it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1203/1232 [00:17<00:00, 67.10it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1204/1232 [00:17<00:00, 67.14it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1205/1232 [00:17<00:00, 67.19it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1206/1232 [00:17<00:00, 67.24it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1207/1232 [00:17<00:00, 67.28it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1208/1232 [00:17<00:00, 67.33it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1209/1232 [00:17<00:00, 67.38it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1210/1232 [00:17<00:00, 67.42it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1211/1232 [00:17<00:00, 67.47it/s, loss=0.0433, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 119/139 [00:00<00:00, 389.97it/s]\u001B[A\n",
      "Epoch 2:  98%|ââââââââââ| 1212/1232 [00:17<00:00, 67.51it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  98%|ââââââââââ| 1213/1232 [00:17<00:00, 67.56it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1214/1232 [00:17<00:00, 67.60it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1215/1232 [00:17<00:00, 67.65it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1216/1232 [00:17<00:00, 67.69it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1217/1232 [00:17<00:00, 67.74it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1218/1232 [00:17<00:00, 67.78it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1219/1232 [00:17<00:00, 67.82it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1220/1232 [00:17<00:00, 67.85it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1221/1232 [00:17<00:00, 67.89it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1222/1232 [00:17<00:00, 67.92it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1223/1232 [00:17<00:00, 67.95it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.98it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 68.02it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 68.05it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 68.08it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 68.11it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 68.14it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 68.18it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 68.21it/s, loss=0.0433, v_num=1]\n",
      "Epoch 2: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.67it/s, loss=0.0433, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.87it/s, loss=0.0414, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.42it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.47it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.51it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.56it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.61it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.65it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.70it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.75it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.79it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.84it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.89it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.93it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.98it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1107/1232 [00:17<00:02, 62.03it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.07it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.12it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.16it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.21it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.25it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.30it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.34it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.39it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.44it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.49it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.53it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.58it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.63it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.67it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.72it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.77it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.81it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.86it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.91it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.96it/s, loss=0.0414, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 345.47it/s]\u001B[A\n",
      "Epoch 3:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 63.00it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.05it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.10it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.14it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.19it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.24it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.28it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.33it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.38it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.43it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.47it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.52it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.57it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.62it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.67it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.71it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.76it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.81it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.86it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.90it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.95it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 64.00it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 64.05it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.09it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.14it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.19it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.23it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.28it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.33it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.38it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 64.42it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 64.47it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 64.51it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 64.56it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 64.61it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 64.65it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 64.70it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1165/1232 [00:17<00:01, 64.75it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1166/1232 [00:17<00:01, 64.79it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1167/1232 [00:17<00:01, 64.84it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.89it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.93it/s, loss=0.0414, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 385.47it/s]\u001B[A\n",
      "Epoch 3:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.98it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 65.02it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 65.07it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 65.12it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.17it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.21it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.26it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.30it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.35it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.40it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.44it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.49it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.54it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.59it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.63it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.68it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.72it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.77it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.82it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.87it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.91it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.96it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 66.01it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 66.05it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 66.10it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.14it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.19it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.23it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.28it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.32it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.37it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.41it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.46it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.51it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.55it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.60it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.64it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.69it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.74it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.78it/s, loss=0.0414, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 391.29it/s]\u001B[A\n",
      "Epoch 3:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.83it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.87it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.92it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.97it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 67.01it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 67.06it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 67.10it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.15it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.19it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.22it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.25it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.28it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.32it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.35it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.39it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.42it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.45it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.49it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.52it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.55it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.58it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.62it/s, loss=0.0414, v_num=1]\n",
      "Epoch 3: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.10it/s, loss=0.0414, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.95it/s, loss=0.0286, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.52it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.57it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.62it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.67it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.72it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.76it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.81it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.86it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.91it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.95it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1104/1232 [00:17<00:02, 62.00it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1105/1232 [00:17<00:02, 62.05it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1106/1232 [00:17<00:02, 62.10it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1107/1232 [00:17<00:02, 62.14it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.19it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.24it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.28it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.33it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.38it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.43it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.47it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.52it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.57it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.62it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.66it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.71it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.76it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.81it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.85it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.90it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.95it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 63.00it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 63.04it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 63.09it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 63.14it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.19it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.23it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.28it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.33it/s, loss=0.0286, v_num=1]\n",
      "Validation DataLoader 0:  29%|âââ       | 40/139 [00:00<00:00, 390.79it/s]\u001B[A\n",
      "Epoch 4:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.38it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.42it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.47it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.51it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.56it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.61it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.66it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.70it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.75it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.80it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.84it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.89it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.94it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.98it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 64.03it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 64.08it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 64.13it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 64.17it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.22it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.27it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.32it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.36it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.41it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.46it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.50it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 64.55it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 64.59it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 64.64it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 64.69it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 64.73it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 64.78it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 64.83it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1165/1232 [00:17<00:01, 64.87it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1166/1232 [00:17<00:01, 64.92it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1167/1232 [00:17<00:01, 64.97it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1168/1232 [00:17<00:00, 65.01it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1169/1232 [00:17<00:00, 65.06it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1170/1232 [00:17<00:00, 65.11it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1171/1232 [00:17<00:00, 65.15it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1172/1232 [00:17<00:00, 65.20it/s, loss=0.0286, v_num=1]\n",
      "Validation DataLoader 0:  58%|ââââââ    | 80/139 [00:00<00:00, 393.49it/s]\u001B[A\n",
      "Epoch 4:  95%|ââââââââââ| 1173/1232 [00:17<00:00, 65.24it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1174/1232 [00:17<00:00, 65.29it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1175/1232 [00:17<00:00, 65.33it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  95%|ââââââââââ| 1176/1232 [00:17<00:00, 65.38it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1177/1232 [00:17<00:00, 65.43it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1178/1232 [00:17<00:00, 65.47it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1179/1232 [00:17<00:00, 65.52it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1180/1232 [00:17<00:00, 65.57it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1181/1232 [00:17<00:00, 65.61it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.66it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.70it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.75it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.79it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.84it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.88it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.93it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.97it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 66.02it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 66.06it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 66.11it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 66.16it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 66.20it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.25it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.30it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.35it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.39it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.44it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.48it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.53it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.58it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.62it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.67it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.71it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.76it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.81it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.85it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.90it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.95it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.99it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 67.04it/s, loss=0.0286, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 120/139 [00:00<00:00, 390.51it/s]\u001B[A\n",
      "Epoch 4:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 67.08it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 67.13it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 67.17it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 67.22it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.26it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.29it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.32it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.36it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.39it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.42it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.46it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.49it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.52it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.56it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.59it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.62it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.65it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.69it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.72it/s, loss=0.0286, v_num=1]\n",
      "Epoch 4: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.20it/s, loss=0.0286, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.75it/s, loss=0.0321, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.35it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.40it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.44it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.49it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.54it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.59it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.63it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.68it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.73it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.78it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.82it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.87it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.92it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.96it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.01it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.06it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.11it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.15it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.20it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.25it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.29it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.34it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.39it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.43it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.48it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.52it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.57it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.62it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.66it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.71it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.76it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.81it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.85it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.90it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.95it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.00it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.05it/s, loss=0.0321, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 375.68it/s]\u001B[A\n",
      "Epoch 5:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.09it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.14it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.19it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.23it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.28it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.32it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.37it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.41it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.46it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.51it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.55it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.60it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.65it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.69it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.74it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.79it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.84it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.89it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 63.93it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 63.98it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.02it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.07it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.11it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.16it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.21it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.25it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.30it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 64.35it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 64.39it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.44it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.49it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.53it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.58it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.62it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.67it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.71it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.76it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.80it/s, loss=0.0321, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 376.92it/s]\u001B[A\n",
      "Epoch 5:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.85it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.89it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.94it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.98it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 65.03it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.08it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.12it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.16it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.21it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.25it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.30it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.34it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.39it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.44it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.48it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.53it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.57it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.62it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.67it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.71it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.76it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.80it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.85it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.90it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.95it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.99it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.04it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.08it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.13it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.18it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.22it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.27it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.32it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.36it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.41it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.45it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.50it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.54it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.59it/s, loss=0.0321, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 378.99it/s]\u001B[A\n",
      "Epoch 5:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.63it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.68it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.73it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.77it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.82it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.86it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.91it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.95it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.99it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.04it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.08it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.13it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.18it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.21it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.24it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.28it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.31it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.34it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.38it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.41it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.44it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.47it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.51it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.54it/s, loss=0.0321, v_num=1]\n",
      "Epoch 5: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.01it/s, loss=0.0321, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.50it/s, loss=0.0326, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.10it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.14it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.19it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.24it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.28it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.33it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.37it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.42it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.46it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.51it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.55it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.60it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.65it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.69it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.74it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.79it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.83it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.88it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.92it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.97it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.01it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.06it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.10it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.15it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.19it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.24it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.29it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.33it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.38it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.43it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.47it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.52it/s, loss=0.0326, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 33/139 [00:00<00:00, 327.86it/s]\u001B[A\n",
      "Epoch 6:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.56it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.60it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.65it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.70it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.74it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.79it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.83it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.88it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.92it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.97it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.02it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.07it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.11it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.16it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.20it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.25it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.30it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.35it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.39it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.44it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.48it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.53it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.58it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.63it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.67it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.72it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.77it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.81it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.86it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.91it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.95it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.00it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.05it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.10it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.14it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.19it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.24it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.28it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.33it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.38it/s, loss=0.0326, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 73/139 [00:00<00:00, 367.61it/s]\u001B[A\n",
      "Epoch 6:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.42it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.46it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.51it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.56it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.60it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.65it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.69it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.74it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.79it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.83it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.88it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.93it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.97it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.02it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.07it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.11it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.16it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.21it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.26it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.30it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.35it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.40it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.44it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.49it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.54it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.58it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.63it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.67it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.72it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.77it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.81it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.86it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.90it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.95it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.00it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.04it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.09it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.14it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.18it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.23it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.28it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.32it/s, loss=0.0326, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 387.33it/s]\u001B[A\n",
      "Epoch 6:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.37it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.41it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.46it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.50it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.55it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.60it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.64it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.69it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.73it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.78it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.82it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.87it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.92it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.96it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.99it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.02it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.06it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.09it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.12it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.15it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.18it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.22it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.25it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.28it/s, loss=0.0326, v_num=1]\n",
      "Epoch 6: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.77it/s, loss=0.0326, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.70it/s, loss=0.0261, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.26it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.30it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.35it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.40it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.44it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.49it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.54it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.58it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.63it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.68it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.72it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.77it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.82it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.87it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.92it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.96it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.01it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.06it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.11it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.15it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.20it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.25it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.29it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.34it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.38it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.43it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.48it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.52it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.57it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.62it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.66it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.71it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.76it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.81it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.85it/s, loss=0.0261, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 357.93it/s]\u001B[A\n",
      "Epoch 7:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.90it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.94it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.99it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.03it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.08it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.13it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.17it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.22it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.27it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.32it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.36it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.41it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.46it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.51it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.55it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.60it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.65it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.70it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.74it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.79it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 63.83it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.88it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.92it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.97it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 64.01it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.06it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.11it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.15it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.20it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.25it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.29it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.34it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.39it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.43it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.48it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.53it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.57it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.62it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.67it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.71it/s, loss=0.0261, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 379.50it/s]\u001B[A\n",
      "Epoch 7:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.76it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.80it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.85it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.90it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.94it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.99it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.04it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.08it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.13it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.17it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.22it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.26it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.31it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.35it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.40it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.45it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.49it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.54it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.58it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.63it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.67it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.72it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.77it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.82it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.86it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.91it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.96it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.00it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.05it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.10it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.14it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.19it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.24it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.28it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.33it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.38it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.42it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.47it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.51it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.56it/s, loss=0.0261, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 387.62it/s]\u001B[A\n",
      "Epoch 7:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.60it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.65it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.69it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.74it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.78it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.83it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.87it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.91it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.95it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.00it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.04it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.08it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.11it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.15it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.18it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.21it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.25it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.28it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.31it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.34it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.38it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.41it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.44it/s, loss=0.0261, v_num=1]\n",
      "Epoch 7: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.93it/s, loss=0.0261, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.46it/s, loss=0.0222, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.03it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.08it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.12it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.17it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.22it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.26it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.31it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.36it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.40it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.45it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.49it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.54it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.58it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.63it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.67it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.72it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.76it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.81it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.86it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.91it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.95it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.00it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.05it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.10it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.14it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.19it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.24it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.28it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.33it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.38it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.43it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.47it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.52it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.57it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.62it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.66it/s, loss=0.0222, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 365.39it/s]\u001B[A\n",
      "Epoch 8:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.71it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.76it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.80it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.85it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.89it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.94it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.98it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.03it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.07it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.12it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.17it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.21it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.26it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.31it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.35it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.40it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.45it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.50it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.54it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.59it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.64it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.69it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.73it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.78it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.83it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.87it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.92it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.97it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.01it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.06it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.11it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.16it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.20it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.25it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.30it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.34it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.39it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.44it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.48it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.53it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.57it/s, loss=0.0222, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 386.37it/s]\u001B[A\n",
      "Epoch 8:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.61it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.66it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.71it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.75it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.80it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.85it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.89it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.94it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.99it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.04it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.08it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.13it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.18it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.22it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.27it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.32it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.37it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.41it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.46it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.50it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.55it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.59it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.63it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.68it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.72it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.76it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.81it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.85it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.90it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.94it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.99it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.03it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.08it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.12it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.16it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.21it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.25it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.30it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.34it/s, loss=0.0222, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 378.58it/s]\u001B[A\n",
      "Epoch 8:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.38it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.42it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.47it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.51it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.55it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.60it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.64it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.69it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.73it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.78it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.82it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.86it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.89it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.92it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.96it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.99it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.03it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.06it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.09it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.13it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.16it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.19it/s, loss=0.0222, v_num=1]\n",
      "Epoch 8: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.70it/s, loss=0.0222, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.93it/s, loss=0.0207, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.50it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.55it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.59it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.64it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.69it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.74it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.79it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.83it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.88it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.93it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.97it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1105/1232 [00:17<00:02, 62.02it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1106/1232 [00:17<00:02, 62.07it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1107/1232 [00:17<00:02, 62.12it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.16it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.21it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.26it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.31it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.36it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.40it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.45it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.50it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.55it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.59it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.64it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.68it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.73it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.77it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.82it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.87it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.91it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.96it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 63.00it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 63.05it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 63.10it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.14it/s, loss=0.0207, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 362.29it/s]\u001B[A\n",
      "Epoch 9:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.18it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.23it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.27it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.32it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.36it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.41it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.46it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.50it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.55it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.60it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.64it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.69it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.74it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.79it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.83it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.88it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.93it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.98it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 64.02it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 64.07it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 64.11it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.16it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.20it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.25it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.29it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.34it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.38it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.43it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 64.47it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 64.52it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1160/1232 [00:17<00:01, 64.56it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1161/1232 [00:17<00:01, 64.61it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1162/1232 [00:17<00:01, 64.65it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1163/1232 [00:17<00:01, 64.70it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  94%|ââââââââââ| 1164/1232 [00:17<00:01, 64.74it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1165/1232 [00:17<00:01, 64.79it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1166/1232 [00:17<00:01, 64.84it/s, loss=0.0207, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 74/139 [00:00<00:00, 359.72it/s]\u001B[A\n",
      "Epoch 9:  95%|ââââââââââ| 1167/1232 [00:17<00:01, 64.88it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1168/1232 [00:17<00:00, 64.92it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1169/1232 [00:17<00:00, 64.97it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1170/1232 [00:17<00:00, 65.01it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1171/1232 [00:17<00:00, 65.06it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 65.11it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 65.16it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.20it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.25it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.30it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.35it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.39it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.44it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.48it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.53it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.57it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.62it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.67it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.71it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.76it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.81it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.85it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.90it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.95it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.99it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 66.04it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 66.09it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 66.13it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.18it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.22it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.27it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.31it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.36it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.40it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.45it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.49it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.54it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.58it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.63it/s, loss=0.0207, v_num=1]\n",
      "Validation DataLoader 0:  81%|âââââââââ | 113/139 [00:00<00:00, 371.95it/s]\u001B[A\n",
      "Epoch 9:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.67it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.71it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.76it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.80it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.85it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.89it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.94it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.98it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 67.03it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 67.08it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 67.13it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.17it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.22it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.26it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.31it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.36it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.40it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.44it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.47it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.50it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.54it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.57it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.60it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.64it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.67it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.70it/s, loss=0.0207, v_num=1]\n",
      "Epoch 9: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.19it/s, loss=0.0207, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.82it/s, loss=0.0346, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.40it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.44it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.49it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.54it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.59it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.63it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.68it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.72it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.77it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.81it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.86it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.90it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.95it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.99it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1108/1232 [00:17<00:01, 62.04it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.09it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.14it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.18it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.23it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.28it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.32it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.37it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.41it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.46it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.51it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.55it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.60it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.64it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.69it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.74it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.78it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.83it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.88it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.92it/s, loss=0.0346, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 343.82it/s]\u001B[A\n",
      "Epoch 10:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.96it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 63.01it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.05it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.10it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.14it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.19it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.23it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.28it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.33it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.37it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.42it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.46it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.51it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.55it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.60it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.65it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.69it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.74it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.79it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.83it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.88it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 63.93it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 63.97it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 64.02it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.07it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.12it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.16it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1155/1232 [00:17<00:01, 64.21it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1156/1232 [00:17<00:01, 64.26it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1157/1232 [00:17<00:01, 64.30it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1158/1232 [00:17<00:01, 64.35it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1159/1232 [00:17<00:01, 64.40it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.44it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.49it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.53it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.58it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.63it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.67it/s, loss=0.0346, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 73/139 [00:00<00:00, 360.41it/s]\u001B[A\n",
      "Epoch 10:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.72it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.76it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.81it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.86it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.91it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.95it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 65.00it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 65.05it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.10it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.14it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.19it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.24it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.28it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.33it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.38it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.42it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.47it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.51it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.56it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.61it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.65it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.70it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.75it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.79it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.84it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.89it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.94it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.98it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 66.03it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.07it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.11it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.16it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.20it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.25it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.29it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.34it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.38it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.43it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.47it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.52it/s, loss=0.0346, v_num=1]\n",
      "Validation DataLoader 0:  81%|âââââââââ | 113/139 [00:00<00:00, 375.72it/s]\u001B[A\n",
      "Epoch 10:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.56it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.61it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.65it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.70it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.75it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.79it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.84it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.89it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.93it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.98it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 67.03it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.07it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.12it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.16it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.21it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.26it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.29it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.32it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.36it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.39it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.42it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.45it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.49it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.52it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.56it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.59it/s, loss=0.0346, v_num=1]\n",
      "Epoch 10: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.07it/s, loss=0.0346, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.65it/s, loss=0.0216, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.21it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.26it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.31it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.35it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.40it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.45it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.49it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.54it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.59it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.64it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.69it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.73it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.78it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.83it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.88it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.93it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.97it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.02it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.07it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.11it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.16it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.21it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.26it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.31it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.35it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.40it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.45it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.49it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.54it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.59it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.64it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.68it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.73it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.78it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.82it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.87it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.91it/s, loss=0.0216, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 378.28it/s]\u001B[A\n",
      "Epoch 11:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.96it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.00it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.05it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.09it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.14it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.19it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.24it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.28it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.33it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.38it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.42it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.47it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.52it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.56it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.61it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.66it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.70it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.75it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.80it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.84it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.89it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.93it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.98it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.02it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.07it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.12it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.16it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.21it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.26it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.30it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.35it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.40it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.44it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.49it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.54it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.58it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.63it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.68it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.72it/s, loss=0.0216, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 383.54it/s]\u001B[A\n",
      "Epoch 11:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.77it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.81it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.86it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.90it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.95it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.00it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.04it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.09it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.13it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.18it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.23it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.27it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.32it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.37it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.41it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.46it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.51it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.55it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.60it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.65it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.69it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.74it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.78it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.83it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.87it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.92it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.97it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.01it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.06it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.10it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.15it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.20it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.24it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.29it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.34it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.38it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.43it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.48it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.52it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.57it/s, loss=0.0216, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 390.86it/s]\u001B[A\n",
      "Epoch 11:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.61it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.66it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.70it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.75it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.79it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.83it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.88it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.93it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.97it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.00it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.04it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.07it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.10it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.14it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.17it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.20it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.24it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.27it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.30it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.33it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.37it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.40it/s, loss=0.0216, v_num=1]\n",
      "Epoch 11: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.88it/s, loss=0.0216, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.76it/s, loss=0.0264, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.33it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.37it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.42it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.46it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.51it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.56it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.61it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.66it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.70it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.75it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.80it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.84it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.89it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.94it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.03it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.08it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.13it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.17it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.32it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.36it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.41it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.46it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.50it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.55it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.60it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.65it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.69it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.74it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.79it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.83it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.88it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.92it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.97it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.02it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 373.46it/s]\u001B[A\n",
      "Epoch 12:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.06it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.11it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.15it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.20it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.24it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.29it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.34it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.39it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.43it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.48it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.52it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.57it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.61it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.66it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.70it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.75it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.80it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.84it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 63.89it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 63.93it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 63.98it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.02it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.07it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.11it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.16it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.20it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.25it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.30it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.34it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.39it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.44it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.48it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.53it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.57it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.62it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.67it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.71it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.76it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 368.44it/s]\u001B[A\n",
      "Epoch 12:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.80it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.85it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.89it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.94it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.03it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.08it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.12it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.17it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.26it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.35it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.40it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.45it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.49it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.54it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.59it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.68it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.73it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.77it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.82it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.87it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.91it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.96it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 66.01it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.05it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.10it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.14it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.19it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.23it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.28it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.32it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.37it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.41it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.46it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.50it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.55it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 375.53it/s]\u001B[A\n",
      "Epoch 12:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.59it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.64it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.68it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.73it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.78it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.82it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.87it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.91it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.96it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.01it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.05it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.10it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.14it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.21it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.25it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.28it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.35it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.38it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.41it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.44it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.47it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.51it/s, loss=0.0264, v_num=1]\n",
      "Epoch 12: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.59it/s, loss=0.0203, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.18it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.23it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.28it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.32it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.37it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.42it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.46it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.51it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.56it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.61it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.66it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.70it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.75it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.80it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.84it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.89it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.94it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.98it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.03it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.08it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.13it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.17it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.22it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.27it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.32it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.36it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.41it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.46it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.50it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.55it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.60it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.65it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.69it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.74it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.79it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.84it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.88it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.93it/s, loss=0.0203, v_num=1]\n",
      "Validation DataLoader 0:  28%|âââ       | 39/139 [00:00<00:00, 387.87it/s]\u001B[A\n",
      "Epoch 13:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.98it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.02it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.07it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.12it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.16it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.21it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.26it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.30it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.35it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.40it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.44it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.49it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.54it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.58it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.63it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.68it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.72it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.77it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.81it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.86it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.91it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.95it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.00it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.04it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.09it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.13it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.18it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.23it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.27it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.32it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.37it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.41it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.46it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.50it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.55it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.59it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.64it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.69it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.73it/s, loss=0.0203, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 385.05it/s]\u001B[A\n",
      "Epoch 13:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.78it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.83it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.87it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.92it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.97it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.01it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.06it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.11it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.16it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.20it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.25it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.30it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.35it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.39it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.44it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.48it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.53it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.57it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.62it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.66it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.71it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.76it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.80it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.85it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.90it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.94it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.99it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.03it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.08it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.12it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.17it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.21it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.26it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.30it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.35it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.39it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.44it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.48it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.53it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.57it/s, loss=0.0203, v_num=1]\n",
      "Validation DataLoader 0:  85%|âââââââââ | 118/139 [00:00<00:00, 386.92it/s]\u001B[A\n",
      "Epoch 13:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.61it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.66it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.70it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.74it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.79it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.83it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.88it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.92it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.96it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.00it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.04it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.07it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.11it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.14it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.17it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.20it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.24it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.27it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.30it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.34it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.37it/s, loss=0.0203, v_num=1]\n",
      "Epoch 13: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.86it/s, loss=0.0203, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.74it/s, loss=0.0166, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 14:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.33it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.38it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.42it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.47it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.51it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.56it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.61it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.66it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.70it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.75it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.80it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.84it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.89it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.93it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.98it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1109/1232 [00:17<00:01, 62.03it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.08it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.12it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.17it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.22it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.27it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.31it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.36it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.40it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.45it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.50it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.54it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.59it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.64it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.68it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.73it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.78it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.82it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.87it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.92it/s, loss=0.0166, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 357.64it/s]\u001B[A\n",
      "Epoch 14:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.96it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 63.01it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 63.05it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.10it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.15it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.20it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.24it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.29it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.34it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.38it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.43it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.47it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.52it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.57it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.61it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.66it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.71it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.75it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.80it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1148/1232 [00:17<00:01, 63.85it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1149/1232 [00:17<00:01, 63.89it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1150/1232 [00:17<00:01, 63.94it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  93%|ââââââââââ| 1151/1232 [00:17<00:01, 63.98it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1152/1232 [00:17<00:01, 64.03it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1153/1232 [00:17<00:01, 64.07it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1154/1232 [00:17<00:01, 64.12it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.16it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.21it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.26it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.30it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.35it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.40it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.44it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.49it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.54it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.58it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.63it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.68it/s, loss=0.0166, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 74/139 [00:00<00:00, 370.29it/s]\u001B[A\n",
      "Epoch 14:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.72it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.76it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.81it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.85it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.90it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.95it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.99it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 65.04it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.08it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.13it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.18it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.22it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.27it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.31it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.36it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.40it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.45it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.49it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.54it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.58it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.63it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.68it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.72it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.76it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.81it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.86it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.90it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.95it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.99it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.04it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.09it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.13it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.18it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.22it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.27it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.32it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.36it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.41it/s, loss=0.0166, v_num=1]\n",
      "Validation DataLoader 0:  81%|ââââââââ  | 112/139 [00:00<00:00, 372.30it/s]\u001B[A\n",
      "Epoch 14:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.45it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.50it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.54it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.59it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.63it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.68it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.73it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.77it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.82it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.86it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.91it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.96it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 67.00it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.05it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.09it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.14it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.18it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.23it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.27it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.31it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.34it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.37it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.41it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.44it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.47it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.50it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.53it/s, loss=0.0166, v_num=1]\n",
      "Epoch 14: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 66.02it/s, loss=0.0166, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.64it/s, loss=0.0163, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.26it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.30it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.35it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.40it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.44it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.49it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.54it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.58it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.63it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.68it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.72it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.77it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.81it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.86it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.91it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.95it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1110/1232 [00:17<00:01, 62.00it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.05it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.09it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.14it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.18it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.23it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.28it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.32it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.37it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.42it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.46it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.51it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.55it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.60it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.64it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.69it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.74it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.78it/s, loss=0.0163, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 348.07it/s]\u001B[A\n",
      "Epoch 15:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.83it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.88it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.92it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.97it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.02it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.06it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.11it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.16it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.20it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.25it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.30it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.34it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.39it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.44it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.49it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.54it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.58it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.63it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.68it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1147/1232 [00:17<00:01, 63.72it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.77it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.82it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.86it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.91it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.96it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 64.00it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.05it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.10it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.14it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.19it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.24it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.29it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.33it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.38it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.43it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.47it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.52it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.57it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.61it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.66it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.71it/s, loss=0.0163, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 384.20it/s]\u001B[A\n",
      "Epoch 15:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.75it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.80it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.84it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.89it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.94it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.98it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.03it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.07it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.12it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.17it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.21it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.26it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.31it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.35it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.40it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.45it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.49it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.54it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.59it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.63it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.68it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.73it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.77it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.82it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.86it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.91it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.96it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 66.00it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.05it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.09it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.14it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.19it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.23it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.28it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.32it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.37it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.41it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.46it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.51it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.55it/s, loss=0.0163, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 389.26it/s]\u001B[A\n",
      "Epoch 15:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.59it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.64it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.68it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.73it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.77it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.82it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.86it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.91it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.95it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 67.00it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.04it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.08it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.11it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.14it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.17it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.21it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.24it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.27it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.30it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.34it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.37it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.41it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.44it/s, loss=0.0163, v_num=1]\n",
      "Epoch 15: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.93it/s, loss=0.0163, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.55it/s, loss=0.0172, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 16:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.14it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.19it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.23it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.28it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.33it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.38it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.42it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.47it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.52it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.56it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.61it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.66it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.71it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.75it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.80it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.85it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.89it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.94it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.99it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.03it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.08it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.13it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.18it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.22it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.27it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.32it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.37it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.41it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.46it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.51it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.56it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.60it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.65it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.70it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.74it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.79it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.84it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.88it/s, loss=0.0172, v_num=1]\n",
      "Validation DataLoader 0:  28%|âââ       | 39/139 [00:00<00:00, 385.06it/s]\u001B[A\n",
      "Epoch 16:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.93it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.98it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.02it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.07it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.12it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.16it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.21it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.26it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.31it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.35it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.40it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.45it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.49it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.54it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.59it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.64it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.68it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.73it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.78it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.82it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.87it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.92it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.96it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.00it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.05it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.10it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.14it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.19it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.23it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.28it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.32it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.37it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.42it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.46it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.51it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.55it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.60it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.65it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.69it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.74it/s, loss=0.0172, v_num=1]\n",
      "Validation DataLoader 0:  57%|ââââââ    | 79/139 [00:00<00:00, 389.57it/s]\u001B[A\n",
      "Epoch 16:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.79it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.83it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.88it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.93it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.97it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.02it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.07it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.11it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.16it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.21it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.25it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.30it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.34it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.39it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.43it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.48it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.53it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.57it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.62it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.67it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.71it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.76it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.81it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.85it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.90it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.94it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.99it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.04it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.08it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.13it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.18it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.22it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.27it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.32it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.37it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.41it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.46it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.50it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.55it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.59it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.64it/s, loss=0.0172, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 120/139 [00:00<00:00, 397.19it/s]\u001B[A\n",
      "Epoch 16:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.68it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.72it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.77it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.82it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.86it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.90it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.93it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.97it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.00it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.03it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.07it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.10it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.13it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.16it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.20it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.23it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.26it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.29it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.33it/s, loss=0.0172, v_num=1]\n",
      "Epoch 16: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.82it/s, loss=0.0172, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.57it/s, loss=0.015, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 17:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.16it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.21it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.25it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.30it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.35it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.39it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.44it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.49it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.54it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.58it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.63it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.68it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.73it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.77it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.82it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.87it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.91it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.96it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.01it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.05it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.10it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.14it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.19it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.24it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.28it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.33it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.38it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.43it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.47it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.52it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.57it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.61it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.66it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.71it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.76it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.80it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 368.87it/s]\u001B[A\n",
      "Epoch 17:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.85it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.89it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.94it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.99it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.08it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.13it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.17it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.22it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.27it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.32it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.37it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.41it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.46it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.51it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.55it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.60it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.65it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.69it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.74it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.79it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.83it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.88it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.92it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.97it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.02it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.06it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.11it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.16it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.20it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.25it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.30it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.34it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.39it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.44it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.49it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.53it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.58it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.63it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.67it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.72it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 391.62it/s]\u001B[A\n",
      "Epoch 17:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.77it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.81it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.86it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.91it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.95it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.00it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.05it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.09it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.14it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.19it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.24it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.28it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.33it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.37it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.42it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.46it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.51it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.55it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.60it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.64it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.69it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.73it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.78it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.82it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.86it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.91it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.95it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.00it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.04it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.08it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.13it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.17it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.22it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.27it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.31it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.36it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.40it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.45it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.49it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.54it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  85%|âââââââââ | 118/139 [00:00<00:00, 380.94it/s]\u001B[A\n",
      "Epoch 17:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.58it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.62it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.66it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.71it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.75it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.80it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.84it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.88it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.93it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.97it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.01it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.05it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.08it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.11it/s, loss=0.015, v_num=1]\n",
      "Epoch 17:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.14it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.18it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.21it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.24it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.28it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.31it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.34it/s, loss=0.015, v_num=1]\n",
      "Epoch 17: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.83it/s, loss=0.015, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.48it/s, loss=0.0903, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.07it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.12it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.16it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.21it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.26it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.30it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.35it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.40it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.44it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.49it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.53it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.58it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.63it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.67it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.72it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.76it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.81it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.85it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.90it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.95it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.99it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.04it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.09it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.14it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.18it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.23it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.27it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.32it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.36it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.41it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.45it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.50it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.54it/s, loss=0.0903, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 34/139 [00:00<00:00, 334.84it/s]\u001B[A\n",
      "Epoch 18:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.58it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.63it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.68it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.72it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.77it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.81it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.86it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.90it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.95it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.99it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.04it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.08it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.13it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.17it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.22it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.27it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.31it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.36it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.40it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.45it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.49it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.54it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.58it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.63it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.67it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.72it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.76it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.81it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.85it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.90it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.94it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.99it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.03it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.08it/s, loss=0.0903, v_num=1]\n",
      "Validation DataLoader 0:  49%|âââââ     | 68/139 [00:00<00:00, 337.74it/s]\u001B[A\n",
      "Epoch 18:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.12it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.16it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.21it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.25it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.30it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.34it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.38it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.43it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.47it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.52it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.56it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.61it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.65it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.70it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.74it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.79it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.83it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.87it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.92it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.96it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.01it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.05it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.10it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.14it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.19it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.23it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.28it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.32it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.37it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.41it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.45it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.50it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.54it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.59it/s, loss=0.0903, v_num=1]\n",
      "Validation DataLoader 0:  73%|ââââââââ  | 102/139 [00:00<00:00, 336.88it/s]\u001B[A\n",
      "Epoch 18:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.63it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.67it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.71it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.76it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.80it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.85it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.89it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.94it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.98it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.03it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.07it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.11it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.16it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.20it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.24it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.29it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.33it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.37it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.42it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.46it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.51it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.55it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.59it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.64it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.68it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.72it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.77it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.81it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.86it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.90it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.94it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.99it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.04it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.08it/s, loss=0.0903, v_num=1]\n",
      "Validation DataLoader 0:  98%|ââââââââââ| 136/139 [00:00<00:00, 337.59it/s]\u001B[A\n",
      "Epoch 18: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.13it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.17it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.22it/s, loss=0.0903, v_num=1]\n",
      "Epoch 18: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.73it/s, loss=0.0903, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.48it/s, loss=0.0239, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.06it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.11it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.16it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.20it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.25it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.30it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.34it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.39it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.44it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.48it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.53it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.58it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.62it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.67it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.72it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.76it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.81it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.86it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.90it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.95it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.00it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.04it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.09it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.13it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.18it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.23it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.27it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.32it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.37it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.41it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.46it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.51it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.55it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.60it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.65it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.69it/s, loss=0.0239, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 365.24it/s]\u001B[A\n",
      "Epoch 19:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.74it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.79it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.83it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.88it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.93it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.97it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.02it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.07it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.11it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.16it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.21it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.26it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.31it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.35it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.40it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.44it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.49it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.54it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.58it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.63it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.67it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.72it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.76it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.81it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.86it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.90it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.95it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.00it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.05it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.09it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.14it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.19it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.23it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.28it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.33it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.37it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.42it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.47it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.51it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.56it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.61it/s, loss=0.0239, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 387.51it/s]\u001B[A\n",
      "Epoch 19:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.65it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.70it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.74it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.79it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.84it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.88it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.93it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.97it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.02it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.07it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.11it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.16it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.21it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.25it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.30it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.35it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.39it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.44it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.49it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.53it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.58it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.63it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.67it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.72it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.77it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.81it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.86it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.91it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.96it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.00it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.05it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.09it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.14it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.19it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.23it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.28it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.33it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.37it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.42it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.47it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.51it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.56it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.61it/s, loss=0.0239, v_num=1]\n",
      "Validation DataLoader 0:  87%|âââââââââ | 121/139 [00:00<00:00, 403.23it/s]\u001B[A\n",
      "Epoch 19:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.65it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.70it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.74it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.79it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.82it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.85it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.89it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.92it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.95it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.98it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.02it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.05it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.08it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.11it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.14it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.18it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.21it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.24it/s, loss=0.0239, v_num=1]\n",
      "Epoch 19: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.73it/s, loss=0.0239, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.47it/s, loss=0.0201, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.04it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.09it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.14it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.18it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.23it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.27it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.32it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.37it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.42it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.47it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.51it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.56it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.60it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.65it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.70it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.74it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.79it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.83it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.88it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.93it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.97it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.02it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.07it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.11it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.16it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.20it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.25it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.30it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.34it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.39it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.44it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.48it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.53it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.57it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.62it/s, loss=0.0201, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 353.96it/s]\u001B[A\n",
      "Epoch 20:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.66it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.71it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.75it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.80it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.85it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.89it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.94it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.99it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.03it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.08it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.13it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.17it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.22it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.27it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.31it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.36it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.40it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.45it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.49it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.54it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.59it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.64it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.68it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.73it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.78it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.82it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.87it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.91it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.96it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.01it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.05it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.10it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.15it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.19it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.24it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.28it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.33it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.37it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.42it/s, loss=0.0201, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 371.16it/s]\u001B[A\n",
      "Epoch 20:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.46it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.51it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.55it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.60it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.64it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.69it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.73it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.78it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.83it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.87it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.92it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.96it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.01it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.06it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.10it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.15it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.19it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.24it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.29it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.33it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.38it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.43it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.47it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.52it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.57it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.61it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.66it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.71it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.75it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.80it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.84it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.89it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.93it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.98it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.02it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.07it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.11it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.16it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.20it/s, loss=0.0201, v_num=1]\n",
      "Validation DataLoader 0:  82%|âââââââââ | 114/139 [00:00<00:00, 378.45it/s]\u001B[A\n",
      "Epoch 20:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.25it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.29it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.34it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.39it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.43it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.48it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.53it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.57it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.62it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.67it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.71it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.76it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.80it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.85it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.90it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.94it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.97it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.00it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.04it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.07it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.11it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.14it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.17it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.20it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.24it/s, loss=0.0201, v_num=1]\n",
      "Epoch 20: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.73it/s, loss=0.0201, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.47it/s, loss=0.0167, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 21:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.05it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.09it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.14it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.18it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.23it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.28it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.32it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.37it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.41it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.46it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.51it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.55it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.60it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.64it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.69it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.74it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.78it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.83it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.88it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.92it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.97it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.02it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.06it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.11it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.15it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.20it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.25it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.29it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.34it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.39it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.43it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.48it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.53it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.57it/s, loss=0.0167, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 348.72it/s]\u001B[A\n",
      "Epoch 21:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.62it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.67it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.71it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.76it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.81it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.85it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.90it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.94it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.99it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.04it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.08it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.13it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.18it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.22it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.27it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.32it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.36it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.41it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.46it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.50it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.55it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.60it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.64it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.69it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.74it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.78it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.83it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.88it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.92it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.97it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.02it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.06it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.11it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.16it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.20it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.25it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.29it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.34it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.39it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.43it/s, loss=0.0167, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 376.05it/s]\u001B[A\n",
      "Epoch 21:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.48it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.52it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.57it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.62it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.66it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.71it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.76it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.80it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.85it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.89it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.94it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.99it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.03it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.08it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.12it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.17it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.22it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.26it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.31it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.36it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.40it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.45it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.49it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.54it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.59it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.63it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.68it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.72it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.77it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.82it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.86it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.91it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.95it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.00it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.04it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.09it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.14it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.18it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.23it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.27it/s, loss=0.0167, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 385.09it/s]\u001B[A\n",
      "Epoch 21:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.32it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.36it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.41it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.45it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.50it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.54it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.59it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.63it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.68it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.73it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.77it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.81it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.86it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.90it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.93it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.97it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.00it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.03it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.06it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.10it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.13it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.16it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.20it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.23it/s, loss=0.0167, v_num=1]\n",
      "Epoch 21: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.73it/s, loss=0.0167, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.53it/s, loss=0.0171, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 22:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.11it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.16it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.21it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.25it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.30it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.34it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.39it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.44it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.48it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.53it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.58it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.62it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.67it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.72it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.77it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.81it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.86it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.96it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.00it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.10it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.14it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.19it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.23it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.28it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.33it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.37it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.42it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.47it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.52it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.56it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.61it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.65it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.70it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.75it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 365.92it/s]\u001B[A\n",
      "Epoch 22:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.79it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.83it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.88it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.93it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.97it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.02it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.06it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.11it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.16it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.20it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.25it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.30it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.34it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.39it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.44it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.49it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.53it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.58it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.62it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.67it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.72it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.76it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.81it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.85it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.90it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.94it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.99it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.03it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.08it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.12it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.17it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.21it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.25it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.30it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.34it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.38it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.43it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 74/139 [00:00<00:00, 355.62it/s]\u001B[A\n",
      "Epoch 22:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.46it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.51it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.55it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.60it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.64it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.69it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.78it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.82it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.86it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.95it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.00it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.09it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.14it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.19it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.23it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.28it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.33it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.37it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.42it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.47it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.51it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.56it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.60it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.64it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.69it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.78it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.82it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.87it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.96it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.01it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.10it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  80%|ââââââââ  | 111/139 [00:00<00:00, 359.24it/s]\u001B[A\n",
      "Epoch 22:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.14it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.18it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.23it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.28it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.32it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.37it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.41it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.46it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.50it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.55it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.59it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.64it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.68it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.77it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.82it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.86it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.96it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.00it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.09it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.13it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.16it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.20it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.23it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.26it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.30it/s, loss=0.0171, v_num=1]\n",
      "Epoch 22: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.79it/s, loss=0.0171, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.48it/s, loss=0.0144, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 23:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.06it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.11it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.15it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.20it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.24it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.29it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.34it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.38it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.43it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.47it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.52it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.57it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.61it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.66it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.70it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.75it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.80it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.84it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.89it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.94it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.99it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.03it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.08it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.13it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.18it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.22it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.27it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.32it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.36it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.41it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.46it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.50it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.55it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.59it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.64it/s, loss=0.0144, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 352.99it/s]\u001B[A\n",
      "Epoch 23:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.69it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.73it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.78it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.82it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.87it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.92it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.96it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.01it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.05it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.10it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.14it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.19it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.24it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.28it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.33it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.38it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.42it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.47it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.52it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.56it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.61it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.66it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.70it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.75it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.80it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.84it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.89it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.94it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.99it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.03it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.08it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.12it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.17it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.22it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.26it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.31it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.36it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.40it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.45it/s, loss=0.0144, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 374.20it/s]\u001B[A\n",
      "Epoch 23:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.49it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.54it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.59it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.63it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.68it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.72it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.77it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.81it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.86it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.90it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.95it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.00it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.04it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.09it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.13it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.18it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.22it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.27it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.31it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.36it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.40it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.45it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.49it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.53it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.58it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.62it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.67it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.71it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.76it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.80it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.85it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.89it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.94it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.98it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.02it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.07it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.11it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.16it/s, loss=0.0144, v_num=1]\n",
      "Validation DataLoader 0:  81%|âââââââââ | 113/139 [00:00<00:00, 366.74it/s]\u001B[A\n",
      "Epoch 23:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.20it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.25it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.30it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.34it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.39it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.43it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.48it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.53it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.57it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.62it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.66it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.71it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.75it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.79it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.83it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.88it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.92it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.97it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.01it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.05it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.08it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.12it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.15it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.18it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.21it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.25it/s, loss=0.0144, v_num=1]\n",
      "Epoch 23: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.75it/s, loss=0.0144, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.29it/s, loss=0.0152, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 24:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.91it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.95it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.00it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.05it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.09it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.14it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.19it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.24it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.29it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.33it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.38it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.42it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.47it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.52it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.57it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.61it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.66it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.71it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.75it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.80it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.85it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.90it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.94it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.99it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.04it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.08it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.13it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.18it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.22it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.27it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.31it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.36it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.41it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.45it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.50it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.55it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.59it/s, loss=0.0152, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 379.05it/s]\u001B[A\n",
      "Epoch 24:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.64it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.68it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.73it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.78it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.82it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.86it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.91it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.95it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.00it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.05it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.09it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.14it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.19it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.23it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.28it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.33it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.38it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.42it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.47it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.51it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.56it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.60it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.65it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.69it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.74it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.79it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.83it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.88it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.93it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.97it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.02it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.07it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.11it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.16it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.21it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.25it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.30it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.35it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.39it/s, loss=0.0152, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 381.44it/s]\u001B[A\n",
      "Epoch 24:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.43it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.48it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.52it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.57it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.62it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.66it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.71it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.75it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.80it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.84it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.89it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.93it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.98it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.03it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.07it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.11it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.16it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.20it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.25it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.29it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.34it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.39it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.43it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.48it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.52it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.57it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.62it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.66it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.71it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.76it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.80it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.85it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.89it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.94it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.98it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.03it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.08it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.12it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.17it/s, loss=0.0152, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 383.83it/s]\u001B[A\n",
      "Epoch 24:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.21it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.26it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.30it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.35it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.39it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.43it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.47it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.52it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.56it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.60it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.65it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.69it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.74it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.78it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.81it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.85it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.88it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.91it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.94it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.98it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.01it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.04it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.07it/s, loss=0.0152, v_num=1]\n",
      "Epoch 24: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.57it/s, loss=0.0152, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.40it/s, loss=0.015, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.99it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.08it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.13it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.17it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.22it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.27it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.31it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.36it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.41it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.45it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.50it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.55it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.60it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.64it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.69it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.74it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.78it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.83it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.88it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.92it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.97it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.02it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.06it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.11it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.16it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.20it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.25it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.30it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.34it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.39it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.44it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.48it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.53it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.58it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.62it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.67it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 374.40it/s]\u001B[A\n",
      "Epoch 25:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.71it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.76it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.81it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.85it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.89it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.94it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.99it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.08it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.13it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.17it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.22it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.27it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.32it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.36it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.41it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.46it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.50it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.55it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.60it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.64it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.69it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.73it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.78it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.83it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.87it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.92it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.97it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.01it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.06it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.11it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.15it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.20it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.24it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.29it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.34it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.38it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.43it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.47it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 383.39it/s]\u001B[A\n",
      "Epoch 25:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.52it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.56it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.61it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.66it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.70it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.75it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.79it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.84it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.89it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.93it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.98it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.07it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.11it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.16it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.20it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.25it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.30it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.34it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.39it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.43it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.48it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.53it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.57it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.62it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.66it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.71it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.75it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.80it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.84it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.89it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.94it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.98it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.07it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.12it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.17it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.21it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.26it/s, loss=0.015, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 385.26it/s]\u001B[A\n",
      "Epoch 25:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.30it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.35it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.39it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.44it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.49it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.53it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.58it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.62it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.67it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.72it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.76it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.80it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.83it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.87it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.90it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.93it/s, loss=0.015, v_num=1]\n",
      "Epoch 25:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.97it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.00it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.03it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.06it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.10it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.13it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.16it/s, loss=0.015, v_num=1]\n",
      "Epoch 25: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.65it/s, loss=0.015, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.55it/s, loss=0.0146, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.15it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.19it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.24it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.28it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.33it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.38it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.42it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.47it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.51it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.56it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.60it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.65it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.70it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.74it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.79it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.83it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.88it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.93it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.97it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.02it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.07it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.11it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.16it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.21it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.25it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.30it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.34it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.39it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.44it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.48it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.53it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.58it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.63it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.68it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.72it/s, loss=0.0146, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 354.32it/s]\u001B[A\n",
      "Epoch 26:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.77it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.81it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.86it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.91it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.95it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 63.00it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.05it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.09it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.14it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.18it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.23it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.28it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.33it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.37it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.42it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.47it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.52it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.57it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.61it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.66it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.71it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.75it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.80it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.84it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.89it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.94it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.98it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.03it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.07it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.12it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.17it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.21it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.26it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.31it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.35it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.40it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.45it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.49it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.54it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.58it/s, loss=0.0146, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 377.67it/s]\u001B[A\n",
      "Epoch 26:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.63it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.67it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.72it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.76it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.81it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.86it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.90it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.95it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.00it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.04it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.09it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.14it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.18it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.23it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.28it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.32it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.37it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.42it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.46it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.51it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.56it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.60it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.65it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.70it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.74it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.79it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.84it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.88it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.93it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.98it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.03it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.07it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.12it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.17it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.21it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.26it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.31it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.35it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.40it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.44it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.48it/s, loss=0.0146, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 391.41it/s]\u001B[A\n",
      "Epoch 26:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.52it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.57it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.61it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.66it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.70it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.74it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.78it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.83it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.87it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.91it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.96it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.00it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.04it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.08it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.11it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.15it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.18it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.21it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.24it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.28it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.31it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.35it/s, loss=0.0146, v_num=1]\n",
      "Epoch 26: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.83it/s, loss=0.0146, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.51it/s, loss=0.012, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.09it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.13it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.18it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.22it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.27it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.32it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.36it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.41it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.46it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.50it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.55it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.60it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.65it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.69it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.74it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.79it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.84it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.88it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.93it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.98it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.02it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.07it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.12it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.16it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.21it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.26it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.30it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.35it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.40it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.44it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.49it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.54it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.58it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.63it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.68it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.73it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.77it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 373.39it/s]\u001B[A\n",
      "Epoch 27:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.82it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.86it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.91it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.95it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.00it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.05it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.09it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.14it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.19it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.23it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.28it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.33it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.37it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.47it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.51it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.56it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.61it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.65it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.70it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.74it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.79it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.83it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.88it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.92it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.97it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.02it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.06it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.11it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.16it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.20it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.25it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.29it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.34it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.38it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.43it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.47it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.52it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 374.34it/s]\u001B[A\n",
      "Epoch 27:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.56it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.61it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.66it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.70it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.75it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.80it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.84it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.89it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.94it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.98it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.03it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.07it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.11it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.16it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.21it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.25it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.30it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.34it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.39it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.43it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.48it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.53it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.57it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.62it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.67it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.72it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.76it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.81it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.85it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.90it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.95it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.99it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.04it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.08it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.13it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.17it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.22it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.26it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.31it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 380.19it/s]\u001B[A\n",
      "Epoch 27:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.35it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.39it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.44it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.48it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.53it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.57it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.62it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.67it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.71it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.76it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.80it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.85it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.89it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.93it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.97it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.00it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.04it/s, loss=0.012, v_num=1]\n",
      "Epoch 27:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.07it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.10it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.13it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.17it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.20it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.23it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.27it/s, loss=0.012, v_num=1]\n",
      "Epoch 27: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.76it/s, loss=0.012, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.34it/s, loss=0.016, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.88it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.93it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.97it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.02it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.06it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.11it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.16it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.20it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.25it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.29it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.34it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.38it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.43it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.48it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.52it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.57it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.61it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.66it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.70it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.75it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.80it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.84it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.89it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.93it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.98it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.03it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.07it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.12it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.16it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.21it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.26it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.30it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.35it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.40it/s, loss=0.016, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 344.16it/s]\u001B[A\n",
      "Epoch 28:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.44it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.48it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.53it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.58it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.62it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.67it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.71it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.75it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.80it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.85it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.89it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.94it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.98it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.03it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.08it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.13it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.17it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.22it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.27it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.32it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.36it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.41it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.46it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.50it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.55it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.60it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.64it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.69it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.73it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.78it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.83it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.87it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.92it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.96it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.01it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.05it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.10it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.14it/s, loss=0.016, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 73/139 [00:00<00:00, 363.69it/s]\u001B[A\n",
      "Epoch 28:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.18it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.23it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.28it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.32it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.37it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.41it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.46it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.51it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.55it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.60it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.65it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.69it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.74it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.78it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.83it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.88it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.92it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.97it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.01it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.06it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.10it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.15it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.20it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.24it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.29it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.34it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.38it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.43it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.47it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.52it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.56it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.61it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.65it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.70it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.74it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.79it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.83it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.88it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.92it/s, loss=0.016, v_num=1]\n",
      "Validation DataLoader 0:  81%|ââââââââ  | 112/139 [00:00<00:00, 373.65it/s]\u001B[A\n",
      "Epoch 28:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.96it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.01it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.06it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.10it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.15it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.19it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.24it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.28it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.32it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.37it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.41it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.46it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.50it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.55it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.59it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.64it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.69it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.73it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.78it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.82it/s, loss=0.016, v_num=1]\n",
      "Epoch 28:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.86it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.89it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.92it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.95it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.98it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.02it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.05it/s, loss=0.016, v_num=1]\n",
      "Epoch 28: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.56it/s, loss=0.016, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.07it/s, loss=0.0148, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 29:  89%|âââââââââ | 1094/1232 [00:18<00:02, 60.66it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1095/1232 [00:18<00:02, 60.71it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1096/1232 [00:18<00:02, 60.75it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.80it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.85it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1099/1232 [00:18<00:02, 60.89it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1100/1232 [00:18<00:02, 60.94it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1101/1232 [00:18<00:02, 60.99it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.03it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.08it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.12it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.17it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.22it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.27it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.31it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.36it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.40it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.45it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.50it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.54it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.59it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.64it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.68it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.73it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.77it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.82it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1120/1232 [00:18<00:01, 61.86it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1121/1232 [00:18<00:01, 61.91it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1122/1232 [00:18<00:01, 61.95it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.00it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.05it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.09it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.14it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.19it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.23it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.28it/s, loss=0.0148, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 361.32it/s]\u001B[A\n",
      "Epoch 29:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.32it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.37it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.42it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.46it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.51it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.55it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.60it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.64it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.69it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.74it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.79it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.83it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 62.88it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 62.93it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 62.97it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.02it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.07it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.11it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.16it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.21it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.25it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.30it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.35it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.39it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.44it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.49it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.53it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.58it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.63it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.67it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.72it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.77it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.81it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 63.86it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 63.91it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 63.95it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.00it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.05it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.09it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.14it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.19it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.23it/s, loss=0.0148, v_num=1]\n",
      "Validation DataLoader 0:  57%|ââââââ    | 79/139 [00:00<00:00, 390.08it/s]\u001B[A\n",
      "Epoch 29:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.28it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.32it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.37it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.41it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.46it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.51it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.55it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.60it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.65it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.69it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.74it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.78it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.83it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 64.88it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 64.92it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 64.97it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.01it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.06it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.11it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.15it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.20it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.24it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.29it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.33it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.38it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.42it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.47it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.52it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.56it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.61it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.65it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.70it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.74it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.79it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.84it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 65.88it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 65.93it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 65.97it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.02it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.07it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.11it/s, loss=0.0148, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 120/139 [00:00<00:00, 394.23it/s]\u001B[A\n",
      "Epoch 29:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.15it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.20it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.24it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.28it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.33it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.37it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.41it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.45it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.49it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.52it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.55it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.59it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.62it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.66it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.69it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.72it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.75it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.79it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.82it/s, loss=0.0148, v_num=1]\n",
      "Epoch 29: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.33it/s, loss=0.0148, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.40it/s, loss=0.0112, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 30:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.96it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.00it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.05it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.10it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.14it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.19it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.23it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.28it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.33it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.37it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.42it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.47it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.51it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.56it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.61it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.65it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.70it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.74it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.79it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.84it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.88it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.93it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.97it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1117/1232 [00:18<00:01, 62.02it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.07it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.11it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.16it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.21it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.25it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.30it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.35it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.39it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.44it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.49it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.54it/s, loss=0.0112, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 356.99it/s]\u001B[A\n",
      "Epoch 30:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.58it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.63it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.67it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.72it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.76it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.81it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.85it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.90it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.95it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.99it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.04it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.09it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.14it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.18it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.23it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.28it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.32it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.37it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.41it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.46it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.50it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.55it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.59it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.64it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.68it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.73it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.77it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.82it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.86it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.91it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.95it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.00it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.04it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.09it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.14it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.19it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.23it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.28it/s, loss=0.0112, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 74/139 [00:00<00:00, 363.39it/s]\u001B[A\n",
      "Epoch 30:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.32it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.36it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.41it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.45it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.50it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.54it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.59it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.64it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.68it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.73it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.77it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.82it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.86it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.91it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.95it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.00it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.05it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.09it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.14it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.18it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.23it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.27it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.32it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.37it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.41it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.46it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.51it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.55it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.60it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.64it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.69it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.73it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.78it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.83it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.87it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.92it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.96it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.01it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.06it/s, loss=0.0112, v_num=1]\n",
      "Validation DataLoader 0:  81%|âââââââââ | 113/139 [00:00<00:00, 373.74it/s]\u001B[A\n",
      "Epoch 30:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.10it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.15it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.19it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.24it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.28it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.33it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.37it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.42it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.46it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.51it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.55it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.60it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.64it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.69it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.74it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.78it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.83it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.87it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.91it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.94it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.98it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.01it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.04it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.07it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.11it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.14it/s, loss=0.0112, v_num=1]\n",
      "Epoch 30: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.64it/s, loss=0.0112, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.33it/s, loss=0.0151, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 31:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.90it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.94it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.99it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.04it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.08it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.13it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.17it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.22it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.26it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.31it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.35it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.40it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.44it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.49it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.54it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.58it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.63it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.68it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.72it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.77it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.82it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.86it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.91it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.96it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.00it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.05it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.10it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.14it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.19it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.24it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.28it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.33it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.38it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.42it/s, loss=0.0151, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 349.13it/s]\u001B[A\n",
      "Epoch 31:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.47it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.51it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.56it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.61it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.65it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.70it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.75it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.80it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.84it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.89it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.94it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.98it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.03it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.08it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.12it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.17it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.22it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.26it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.31it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.36it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.40it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.45it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.49it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.54it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.59it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.63it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.68it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.73it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.77it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.82it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.87it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.91it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.96it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.01it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.05it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.10it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.15it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.19it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.24it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.29it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.34it/s, loss=0.0151, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 384.52it/s]\u001B[A\n",
      "Epoch 31:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.38it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.43it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.47it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.52it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.56it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.61it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.66it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.70it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.75it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.80it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.84it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.89it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.94it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.98it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.03it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.07it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.12it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.17it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.21it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.26it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.30it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.35it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.39it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.44it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.48it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.53it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.57it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.62it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.66it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.71it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.76it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.80it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.85it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.90it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.94it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.99it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.03it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.08it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.12it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.17it/s, loss=0.0151, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 387.57it/s]\u001B[A\n",
      "Epoch 31:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.21it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.26it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.30it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.35it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.39it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.44it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.48it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.53it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.58it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.62it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.67it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.70it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.74it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.77it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.80it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.83it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.87it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.90it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.93it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.97it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.00it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.03it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.06it/s, loss=0.0151, v_num=1]\n",
      "Epoch 31: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.56it/s, loss=0.0151, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.42it/s, loss=0.0141, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 32:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.00it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.04it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.09it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.13it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.18it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.23it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.28it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.32it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.37it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.42it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.47it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.51it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.56it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.61it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.65it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.70it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.74it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.79it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.83it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.88it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.93it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.97it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.02it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.07it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.11it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.16it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.21it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.26it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.30it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.35it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.40it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.44it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.49it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.53it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.58it/s, loss=0.0141, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 358.35it/s]\u001B[A\n",
      "Epoch 32:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.62it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.67it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.71it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.76it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.80it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.85it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.89it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.94it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.99it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.03it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.08it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.12it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.17it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.21it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.26it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.31it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.35it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.40it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.45it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.49it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.54it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.59it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.63it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.68it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.73it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.78it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.82it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.87it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.91it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.96it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.01it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.05it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.10it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.15it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.19it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.24it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.29it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.33it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.38it/s, loss=0.0141, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 375.41it/s]\u001B[A\n",
      "Epoch 32:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.42it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.47it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.51it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.56it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.60it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.64it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.69it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.74it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.78it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.83it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.87it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.92it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.97it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.01it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.06it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.11it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.15it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.20it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.25it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.29it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.34it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.39it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.43it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.48it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.52it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.57it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.61it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.66it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.71it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.75it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.80it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.84it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.89it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.93it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.98it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.02it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.07it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.11it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.16it/s, loss=0.0141, v_num=1]\n",
      "Validation DataLoader 0:  82%|âââââââââ | 114/139 [00:00<00:00, 378.25it/s]\u001B[A\n",
      "Epoch 32:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.20it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.25it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.29it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.34it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.38it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.43it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.47it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.52it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.57it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.61it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.65it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.70it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.74it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.78it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.82it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.87it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.90it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.93it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.97it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.00it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.04it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.07it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.10it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.13it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.17it/s, loss=0.0141, v_num=1]\n",
      "Epoch 32: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.66it/s, loss=0.0141, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.63it/s, loss=0.0121, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 33:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.22it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.27it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.31it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.36it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.41it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.46it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.50it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.55it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.60it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.65it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.69it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.74it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.79it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.83it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.88it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.92it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.97it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.01it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.06it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.11it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.15it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.20it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.24it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.29it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.34it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.38it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.43it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.48it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.52it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.57it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.62it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.67it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.72it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.76it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.81it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.86it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.91it/s, loss=0.0121, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 371.78it/s]\u001B[A\n",
      "Epoch 33:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.95it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.99it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.04it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.08it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.13it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.17it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.22it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.27it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.32it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.36it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.41it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.45it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.50it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.55it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.59it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.64it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.68it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.73it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.78it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.82it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.87it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.91it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.96it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.01it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.05it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.10it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.15it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.20it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.25it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.29it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.34it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.39it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.43it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.48it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.52it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.57it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.62it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.67it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.71it/s, loss=0.0121, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 379.94it/s]\u001B[A\n",
      "Epoch 33:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.76it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.80it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.85it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.89it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.94it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.99it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.03it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.08it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.12it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.17it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.21it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.26it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.30it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.34it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.39it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.43it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.48it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.52it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.57it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.61it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.65it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.70it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.75it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.79it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.84it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.88it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.93it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.97it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.02it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.07it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.11it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.16it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.20it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.25it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.30it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.34it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.39it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.43it/s, loss=0.0121, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 373.94it/s]\u001B[A\n",
      "Epoch 33:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.48it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.52it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.56it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.61it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.65it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.69it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.74it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.78it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.82it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.87it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.91it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.96it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.01it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.05it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.10it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.15it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.19it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.23it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.26it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.29it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.33it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.36it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.39it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.43it/s, loss=0.0121, v_num=1]\n",
      "Epoch 33: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.91it/s, loss=0.0121, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.49it/s, loss=0.0117, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 34:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.01it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.05it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.10it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.15it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.19it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.24it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.29it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.33it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.38it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.42it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.47it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.51it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.56it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.61it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.65it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.70it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.75it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.79it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.84it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.88it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.93it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.97it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.02it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.07it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.11it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.16it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.20it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.25it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.29it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.34it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.39it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.44it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.48it/s, loss=0.0117, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 34/139 [00:00<00:00, 333.95it/s]\u001B[A\n",
      "Epoch 34:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.52it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.57it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.61it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.66it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.70it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.75it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.79it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.84it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.88it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.93it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.98it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.02it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.07it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.12it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.16it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.21it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.25it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.30it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.35it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.40it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.44it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.49it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.53it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.58it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.63it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.67it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.72it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.77it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.81it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.86it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.90it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.95it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.00it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.04it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.09it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.14it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.18it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.23it/s, loss=0.0117, v_num=1]\n",
      "Validation DataLoader 0:  52%|ââââââ    | 72/139 [00:00<00:00, 357.50it/s]\u001B[A\n",
      "Epoch 34:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.27it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.32it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.36it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.41it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.45it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.50it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.55it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.60it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.64it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.69it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.74it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.78it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.83it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.87it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.91it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.96it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.00it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.05it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.09it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.14it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.19it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.23it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.28it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.32it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.37it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.41it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.45it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.50it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.54it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.59it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.63it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.68it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.73it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.77it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.82it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.86it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.91it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.96it/s, loss=0.0117, v_num=1]\n",
      "Validation DataLoader 0:  79%|ââââââââ  | 110/139 [00:00<00:00, 366.01it/s]\u001B[A\n",
      "Epoch 34:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.00it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.05it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.09it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.14it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.18it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.23it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.27it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.32it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.36it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.41it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.45it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.50it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.54it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.59it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.63it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.68it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.72it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.77it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.81it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.86it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.90it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.95it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.99it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.03it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.06it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.10it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.13it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.16it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.20it/s, loss=0.0117, v_num=1]\n",
      "Epoch 34: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.70it/s, loss=0.0117, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.48it/s, loss=0.012, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 35:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.09it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.14it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.18it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.23it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.28it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.32it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.37it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.46it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.51it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.55it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.60it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.65it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.69it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.74it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.79it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.83it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.88it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.93it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.97it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.02it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.06it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.11it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.15it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.19it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.24it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.29it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.33it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.37it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.46it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.51it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 33/139 [00:00<00:00, 326.70it/s]\u001B[A\n",
      "Epoch 35:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.55it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.59it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.64it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.68it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.73it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.77it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.82it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.86it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.90it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.95it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.00it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.05it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.09it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.14it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.19it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.24it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.28it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.33it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.38it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.47it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.52it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.57it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.61it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.66it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.70it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.75it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.79it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.84it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.89it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.93it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.98it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.03it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.07it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.12it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.17it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.21it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.26it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  51%|âââââ     | 71/139 [00:00<00:00, 355.68it/s]\u001B[A\n",
      "Epoch 35:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.30it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.35it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.39it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.44it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.49it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.53it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.58it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.62it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.67it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.71it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.76it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.81it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.85it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.90it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.94it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.98it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.03it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.07it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.12it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.16it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.20it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.25it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.29it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.33it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.38it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.46it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.51it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.55it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.60it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.64it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.69it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.73it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.78it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.83it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.87it/s, loss=0.012, v_num=1]\n",
      "Validation DataLoader 0:  77%|ââââââââ  | 107/139 [00:00<00:00, 353.53it/s]\u001B[A\n",
      "Epoch 35:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.92it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.96it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.01it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.06it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.10it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.15it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.19it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.24it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.29it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.33it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.38it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.42it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.47it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.51it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.56it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.60it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.65it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.69it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.74it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.78it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.83it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.88it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.92it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.97it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.01it/s, loss=0.012, v_num=1]\n",
      "Epoch 35:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.06it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.10it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.15it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.18it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.22it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.25it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.28it/s, loss=0.012, v_num=1]\n",
      "Epoch 35: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.78it/s, loss=0.012, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.43it/s, loss=0.0127, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 36:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.03it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.08it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.12it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.17it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.21it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.26it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.31it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.35it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.40it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.45it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.49it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.54it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.58it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.63it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.67it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.72it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.76it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.81it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.85it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.90it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.95it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.99it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.04it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.08it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.13it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.17it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.22it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.26it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.31it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.35it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.40it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.45it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.49it/s, loss=0.0127, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 34/139 [00:00<00:00, 332.48it/s]\u001B[A\n",
      "Epoch 36:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.53it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.58it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.62it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.67it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.71it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.76it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.80it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.85it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.89it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.94it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.98it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.03it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.07it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.12it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.16it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.21it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.25it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.30it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.34it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.39it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.43it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.48it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.52it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.57it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.61it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.66it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.70it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.75it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.79it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.84it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.88it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.93it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.97it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.02it/s, loss=0.0127, v_num=1]\n",
      "Validation DataLoader 0:  49%|âââââ     | 68/139 [00:00<00:00, 333.45it/s]\u001B[A\n",
      "Epoch 36:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.06it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.10it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.15it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.19it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.24it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.28it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.33it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.37it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.42it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.46it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.50it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.55it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.59it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.64it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.68it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.73it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.77it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.82it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.86it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.90it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.95it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.00it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.04it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.09it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.13it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.17it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.22it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.26it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.31it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.35it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.40it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.44it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.48it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.53it/s, loss=0.0127, v_num=1]\n",
      "Validation DataLoader 0:  73%|ââââââââ  | 102/139 [00:00<00:00, 335.26it/s]\u001B[A\n",
      "Epoch 36:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.57it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.61it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.66it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.70it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.74it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.79it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.83it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.87it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.92it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.96it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.00it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.05it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.09it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.14it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.18it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.23it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.27it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.32it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.36it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.41it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.45it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.50it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.54it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.59it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.63it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.68it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.73it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.77it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.82it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.86it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.91it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.95it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.00it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.05it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.09it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.14it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.19it/s, loss=0.0127, v_num=1]\n",
      "Epoch 36: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.70it/s, loss=0.0127, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.42it/s, loss=0.026, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 37:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.00it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.05it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.09it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.14it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.19it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.23it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.28it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.33it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.38it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.42it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.47it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.52it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.57it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.62it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.66it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.71it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.75it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.80it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.85it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.89it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.94it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.99it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.03it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.08it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.13it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.17it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.22it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.26it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.31it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.35it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.40it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.44it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.49it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.53it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.58it/s, loss=0.026, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 353.53it/s]\u001B[A\n",
      "Epoch 37:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.62it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.66it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.71it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.75it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.80it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.85it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.89it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.94it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.98it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.03it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.07it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.12it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.16it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.21it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.25it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.30it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.35it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.39it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.44it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.48it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.53it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.57it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.62it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.67it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.71it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.76it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.81it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.85it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.90it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.95it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.99it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.04it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.08it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.13it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.18it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.22it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.27it/s, loss=0.026, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 73/139 [00:00<00:00, 361.48it/s]\u001B[A\n",
      "Epoch 37:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.31it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.36it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.40it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.45it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.50it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.54it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.59it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.64it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.68it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.73it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.77it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.82it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.87it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.91it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.96it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.01it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.05it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.10it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.15it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.19it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.24it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.28it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.33it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.37it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.42it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.46it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.50it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.55it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.59it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.64it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.68it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.73it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.78it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.82it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.86it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.91it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.95it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.99it/s, loss=0.026, v_num=1]\n",
      "Validation DataLoader 0:  80%|ââââââââ  | 111/139 [00:00<00:00, 366.48it/s]\u001B[A\n",
      "Epoch 37:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.04it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.08it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.13it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.17it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.22it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.26it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.31it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.35it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.39it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.44it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.48it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.53it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.57it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.61it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.65it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.70it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.74it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.78it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.82it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.86it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.91it/s, loss=0.026, v_num=1]\n",
      "Epoch 37:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.95it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.00it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.04it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.08it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.11it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.14it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.17it/s, loss=0.026, v_num=1]\n",
      "Epoch 37: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.68it/s, loss=0.026, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.37it/s, loss=0.0118, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 38:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.97it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.01it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.06it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.10it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.15it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.20it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.24it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.29it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.34it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.38it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.42it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.47it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.52it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.56it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.61it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.65it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.69it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.74it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.78it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.83it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.87it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.92it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.96it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1117/1232 [00:18<00:01, 62.01it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.06it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.10it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.15it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.20it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.24it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.29it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.33it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.38it/s, loss=0.0118, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 33/139 [00:00<00:00, 323.53it/s]\u001B[A\n",
      "Epoch 38:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.42it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.46it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.50it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.55it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.59it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.64it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.68it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.73it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.77it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.82it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.86it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.91it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.96it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.00it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.05it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.10it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.14it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.19it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.23it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.28it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.33it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.37it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.42it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.47it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.51it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.56it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.61it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.65it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.70it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.74it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.78it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.83it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.88it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.92it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.97it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.01it/s, loss=0.0118, v_num=1]\n",
      "Validation DataLoader 0:  50%|âââââ     | 69/139 [00:00<00:00, 342.38it/s]\u001B[A\n",
      "Epoch 38:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.06it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.11it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.15it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.20it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.25it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.29it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.33it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.38it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.42it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.46it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.51it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.55it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.60it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.64it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.69it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.73it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.78it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.83it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.87it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.92it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.96it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.01it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.05it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.10it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.14it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.19it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.24it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.28it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.33it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.37it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.42it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.46it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.51it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.55it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.60it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.65it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.69it/s, loss=0.0118, v_num=1]\n",
      "Validation DataLoader 0:  76%|ââââââââ  | 106/139 [00:00<00:00, 354.65it/s]\u001B[A\n",
      "Epoch 38:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.73it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.78it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.82it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.87it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.91it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.96it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.00it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.05it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.10it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.14it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.19it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.23it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.28it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.32it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.37it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.41it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.46it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.50it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.55it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.59it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.64it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.68it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.73it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.78it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.82it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.87it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.92it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.96it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.01it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.04it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.08it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.11it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.14it/s, loss=0.0118, v_num=1]\n",
      "Epoch 38: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.63it/s, loss=0.0118, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.33it/s, loss=0.189, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 39:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.92it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.97it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.01it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.06it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.11it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.15it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.20it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.25it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.29it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.34it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.38it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.43it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.47it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.52it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.56it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.61it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.66it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.70it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.75it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.80it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.84it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.89it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.94it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.98it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.03it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.08it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.12it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.17it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.21it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.26it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.31it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.36it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.40it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.45it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.50it/s, loss=0.189, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 356.20it/s]\u001B[A\n",
      "Epoch 39:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.54it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.59it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.64it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.69it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.73it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.78it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.83it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.87it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.91it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.96it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.01it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.05it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.10it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.15it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.19it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.24it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.29it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.33it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.38it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.42it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.47it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.52it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.56it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.61it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.65it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.70it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.74it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.79it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.84it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.88it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.93it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.98it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.02it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.07it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.12it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.16it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.21it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.25it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.30it/s, loss=0.189, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 373.18it/s]\u001B[A\n",
      "Epoch 39:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.34it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.39it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.43it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.48it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.53it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.57it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.62it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.67it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.71it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.76it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.80it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.85it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.89it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.94it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.99it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.03it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.08it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.12it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.17it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.21it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.26it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.31it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.35it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.40it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.44it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.49it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.54it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.58it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.63it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.67it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.72it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.76it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.81it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.85it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.89it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.94it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.98it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.02it/s, loss=0.189, v_num=1]\n",
      "Validation DataLoader 0:  81%|âââââââââ | 113/139 [00:00<00:00, 373.64it/s]\u001B[A\n",
      "Epoch 39:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.06it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.11it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.15it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.19it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.24it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.28it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.33it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.37it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.42it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.46it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.50it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.55it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.59it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.64it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.68it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.73it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.77it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.82it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.86it/s, loss=0.189, v_num=1]\n",
      "Epoch 39:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.91it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.94it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.97it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.01it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.04it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.07it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.11it/s, loss=0.189, v_num=1]\n",
      "Epoch 39: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.61it/s, loss=0.189, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.53it/s, loss=0.154, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 40:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.11it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.15it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.20it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.25it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.29it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.34it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.38it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.43it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.48it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.52it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.57it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.62it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.66it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.71it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.75it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.80it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.84it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.89it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.93it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.98it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.03it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.07it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.12it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.16it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.21it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.26it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.31it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.35it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.40it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.45it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.50it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.54it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.59it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.64it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.69it/s, loss=0.154, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 354.51it/s]\u001B[A\n",
      "Epoch 40:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.73it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.78it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.83it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.88it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.92it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.97it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.01it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.05it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.10it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.14it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.19it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.23it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.28it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.32it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.37it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.41it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.46it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.51it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.56it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.60it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.65it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.69it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.74it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.78it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.83it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.87it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.92it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.96it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.01it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.05it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.10it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.14it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.18it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.23it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.27it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.32it/s, loss=0.154, v_num=1]\n",
      "Validation DataLoader 0:  52%|ââââââ    | 72/139 [00:00<00:00, 349.82it/s]\u001B[A\n",
      "Epoch 40:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.36it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.40it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.45it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.49it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.54it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.58it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.62it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.67it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.71it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.76it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.80it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.84it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.89it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.93it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.98it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.02it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.07it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.11it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.16it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.20it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.25it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.29it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.34it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.39it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.43it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.48it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.52it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.57it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.61it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.66it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.70it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.75it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.79it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.83it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.88it/s, loss=0.154, v_num=1]\n",
      "Validation DataLoader 0:  77%|ââââââââ  | 107/139 [00:00<00:00, 345.45it/s]\u001B[A\n",
      "Epoch 40:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.92it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.96it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.01it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.06it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.10it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.14it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.19it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.23it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.28it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.32it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.37it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.42it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.46it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.51it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.55it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.60it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.64it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.69it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.73it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.78it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.83it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.87it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.92it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.96it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.01it/s, loss=0.154, v_num=1]\n",
      "Epoch 40:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.05it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.10it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.14it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.19it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.23it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.28it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.31it/s, loss=0.154, v_num=1]\n",
      "Epoch 40: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.80it/s, loss=0.154, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.45it/s, loss=0.0948, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 41:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.00it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.04it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.09it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.14it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.18it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.23it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.28it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.33it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.37it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.42it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.47it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.51it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.56it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.61it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.65it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.70it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.75it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.79it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.84it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.89it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.93it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.98it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.03it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.08it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.12it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.17it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.22it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.27it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.31it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.36it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.41it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.46it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.50it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.55it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.60it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.64it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.69it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.74it/s, loss=0.0948, v_num=1]\n",
      "Validation DataLoader 0:  28%|âââ       | 39/139 [00:00<00:00, 389.22it/s]\u001B[A\n",
      "Epoch 41:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.78it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.83it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.87it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.92it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.97it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.01it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.06it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.11it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.15it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.20it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.24it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.29it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.33it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.38it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.43it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.47it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.52it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.57it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.61it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.66it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.71it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.75it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.80it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.84it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.89it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.93it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.98it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.02it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.07it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.12it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.16it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.21it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.26it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.30it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.35it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.39it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.44it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.48it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.53it/s, loss=0.0948, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 379.92it/s]\u001B[A\n",
      "Epoch 41:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.57it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.62it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.66it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.71it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.76it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.80it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.84it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.89it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.93it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.98it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.02it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.07it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.11it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.16it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.21it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.25it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.30it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.34it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.39it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.44it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.48it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.53it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.57it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.62it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.66it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.71it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.75it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.80it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.84it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.89it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.93it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.98it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.02it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.07it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.11it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.16it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.20it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.25it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.30it/s, loss=0.0948, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 376.09it/s]\u001B[A\n",
      "Epoch 41:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.34it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.39it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.43it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.48it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.52it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.57it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.62it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.66it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.70it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.75it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.79it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.84it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.88it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.93it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.96it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.99it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.02it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.05it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.09it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.12it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.15it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.18it/s, loss=0.0948, v_num=1]\n",
      "Epoch 41: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.69it/s, loss=0.0948, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.20it/s, loss=0.0718, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 42:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.78it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1095/1232 [00:18<00:02, 60.83it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1096/1232 [00:18<00:02, 60.87it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.92it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.97it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1099/1232 [00:18<00:02, 61.01it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1100/1232 [00:18<00:02, 61.06it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.11it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.15it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.20it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.25it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.29it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.34it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.39it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.44it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.48it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.53it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.58it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.62it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.67it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.72it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.77it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.81it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.86it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.90it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.95it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1120/1232 [00:18<00:01, 61.99it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.04it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.09it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.13it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.18it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.23it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.28it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.32it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.37it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.42it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.47it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.51it/s, loss=0.0718, v_num=1]\n",
      "Validation DataLoader 0:  28%|âââ       | 39/139 [00:00<00:00, 383.05it/s]\u001B[A\n",
      "Epoch 42:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.56it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.60it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.65it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.70it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.74it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.79it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.84it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.88it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.93it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.98it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.02it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.07it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.12it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.16it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.21it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.26it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.31it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.35it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.40it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.44it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.49it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.53it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.58it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.63it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.67it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.72it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.77it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.81it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.86it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.91it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.95it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.00it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.05it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.09it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.14it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.19it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.23it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.28it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.32it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.37it/s, loss=0.0718, v_num=1]\n",
      "Validation DataLoader 0:  57%|ââââââ    | 79/139 [00:00<00:00, 390.88it/s]\u001B[A\n",
      "Epoch 42:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.41it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.46it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.50it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.55it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.59it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.64it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.68it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.73it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.78it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.82it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.87it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.91it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.96it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.00it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.05it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.10it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.14it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.19it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.24it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.28it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.32it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.37it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.42it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.46it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.51it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.55it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.60it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.64it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.69it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.73it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.78it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.83it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.87it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.92it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.96it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.01it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.05it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.10it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.15it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.19it/s, loss=0.0718, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 119/139 [00:00<00:00, 390.33it/s]\u001B[A\n",
      "Epoch 42:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.23it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.28it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.33it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.37it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.42it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.46it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.51it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.56it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.59it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.62it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.66it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.69it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.72it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.76it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.79it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.82it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.85it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.89it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.92it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.95it/s, loss=0.0718, v_num=1]\n",
      "Epoch 42: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.46it/s, loss=0.0718, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.24it/s, loss=0.0604, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 43:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.83it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.88it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.93it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1097/1232 [00:17<00:02, 60.98it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.02it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.07it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.12it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.17it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.21it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.26it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.31it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.35it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.40it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.45it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.49it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.54it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.59it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.63it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.68it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.73it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.77it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.82it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.87it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.91it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.96it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.01it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.06it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.10it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.15it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.20it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.25it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.29it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.34it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.39it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.44it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.48it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.53it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.58it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.63it/s, loss=0.0604, v_num=1]\n",
      "Validation DataLoader 0:  29%|âââ       | 40/139 [00:00<00:00, 397.91it/s]\u001B[A\n",
      "Epoch 43:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.67it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.72it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.76it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.81it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.86it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.90it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.95it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.00it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.05it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.09it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.14it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.19it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.23it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.28it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.33it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.37it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.42it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.47it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.51it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.56it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.60it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.65it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.70it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.74it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.79it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.84it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.89it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.93it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.98it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.03it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.08it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.12it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.17it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.22it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.27it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.31it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.36it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.41it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.45it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.50it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.55it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.59it/s, loss=0.0604, v_num=1]\n",
      "Validation DataLoader 0:  59%|ââââââ    | 82/139 [00:00<00:00, 410.12it/s]\u001B[A\n",
      "Epoch 43:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.63it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.68it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.72it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.77it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.82it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.86it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.91it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.95it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.00it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.05it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.09it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.14it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.18it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.23it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.27it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.32it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.37it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.41it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.46it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.50it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.55it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.59it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.64it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.68it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.73it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.77it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.82it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.87it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.91it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.96it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.00it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.05it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.09it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.14it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.18it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.23it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.27it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.32it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.36it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.41it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.45it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.50it/s, loss=0.0604, v_num=1]\n",
      "Validation DataLoader 0:  89%|âââââââââ | 124/139 [00:00<00:00, 394.22it/s]\u001B[A\n",
      "Epoch 43:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.54it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.58it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.63it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.66it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.69it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.72it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.76it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.79it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.82it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.86it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.89it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.92it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.95it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.99it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.02it/s, loss=0.0604, v_num=1]\n",
      "Epoch 43: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.52it/s, loss=0.0604, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.23it/s, loss=0.0577, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 44:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.80it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.85it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.89it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.94it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.99it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1099/1232 [00:18<00:02, 61.03it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1100/1232 [00:18<00:02, 61.08it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.12it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.17it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.22it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.26it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.31it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.35it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.40it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.45it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.49it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.54it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.59it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.64it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.68it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.73it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.77it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.82it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.87it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.91it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.96it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.01it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.06it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.10it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.15it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.19it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.24it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.29it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.34it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.38it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.43it/s, loss=0.0577, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 363.55it/s]\u001B[A\n",
      "Epoch 44:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.47it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.52it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.57it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.61it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.66it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.70it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.75it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.80it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.85it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.89it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.94it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.99it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.03it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.08it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.13it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.17it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.22it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.27it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.31it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.36it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.41it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.45it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.50it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.55it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.59it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.64it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.69it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.73it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.78it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.83it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.87it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.92it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.97it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.01it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.06it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.10it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.15it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.20it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.24it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.29it/s, loss=0.0577, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 384.75it/s]\u001B[A\n",
      "Epoch 44:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.33it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.38it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.42it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.47it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.52it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.56it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.61it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.65it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.70it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.74it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.79it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.83it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.88it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.92it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.97it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.02it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.06it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.11it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.15it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.20it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.24it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.29it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.33it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.38it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.43it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.47it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.52it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.56it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.61it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.65it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.70it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.74it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.79it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.84it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.88it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.93it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.97it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.02it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.07it/s, loss=0.0577, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 386.20it/s]\u001B[A\n",
      "Epoch 44:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.11it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.15it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.20it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.25it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.29it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.34it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.38it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.43it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.47it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.52it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.56it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.61it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.64it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.68it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.71it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.75it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.78it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.81it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.84it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.88it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.91it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.94it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.98it/s, loss=0.0577, v_num=1]\n",
      "Epoch 44: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.48it/s, loss=0.0577, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.36it/s, loss=0.059, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 45:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.92it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.96it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.01it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.05it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.10it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.14it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.19it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.23it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.28it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.32it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.36it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.41it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.45it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.50it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.55it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.59it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.64it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.68it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.73it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.77it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.82it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.87it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.91it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.96it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1118/1232 [00:18<00:01, 62.00it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.05it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.10it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.14it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.19it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.23it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.28it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.33it/s, loss=0.059, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 33/139 [00:00<00:00, 329.22it/s]\u001B[A\n",
      "Epoch 45:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.37it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.42it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.47it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.52it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.56it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.61it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.65it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.70it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.75it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.79it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.84it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.89it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.93it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.98it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.02it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.07it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.12it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.16it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.21it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.25it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.30it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.35it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.39it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.44it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.48it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.53it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.58it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.62it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.67it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.71it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.76it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.80it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.85it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.90it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.94it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.99it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.04it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.08it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.13it/s, loss=0.059, v_num=1]\n",
      "Validation DataLoader 0:  52%|ââââââ    | 72/139 [00:00<00:00, 361.36it/s]\u001B[A\n",
      "Epoch 45:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.17it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.22it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.27it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.31it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.36it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.40it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.45it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.49it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.53it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.58it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.62it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.67it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.71it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.75it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.80it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.85it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.89it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.94it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.98it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.02it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.07it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.11it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.16it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.20it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.25it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.29it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.34it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.39it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.43it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.48it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.52it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.57it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.61it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.66it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.70it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.75it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.79it/s, loss=0.059, v_num=1]\n",
      "Validation DataLoader 0:  78%|ââââââââ  | 109/139 [00:00<00:00, 358.29it/s]\u001B[A\n",
      "Epoch 45:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.83it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.88it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.92it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.97it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.01it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.06it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.10it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.15it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.19it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.24it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.29it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.33it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.38it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.42it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.47it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.51it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.56it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.60it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.65it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.69it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.74it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.78it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.83it/s, loss=0.059, v_num=1]\n",
      "Epoch 45:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.87it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.92it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.96it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.99it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.03it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.05it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.09it/s, loss=0.059, v_num=1]\n",
      "Epoch 45: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.59it/s, loss=0.059, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.19it/s, loss=0.0448, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 46:  89%|âââââââââ | 1094/1232 [00:18<00:02, 60.76it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1095/1232 [00:18<00:02, 60.80it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1096/1232 [00:18<00:02, 60.85it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.89it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.94it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1099/1232 [00:18<00:02, 60.99it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1100/1232 [00:18<00:02, 61.03it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.08it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.13it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.17it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.22it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.26it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.31it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.35it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.40it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.45it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.49it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.54it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.59it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.63it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.68it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.72it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.77it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.81it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.86it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.91it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1120/1232 [00:18<00:01, 61.95it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.00it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.05it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.09it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.14it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.18it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.23it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.27it/s, loss=0.0448, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 346.17it/s]\u001B[A\n",
      "Epoch 46:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.31it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.36it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.40it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.45it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.49it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.54it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.58it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.63it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.67it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.72it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.76it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.81it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.85it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.90it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 62.94it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 62.99it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.03it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.08it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.12it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.17it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.21it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.26it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.30it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.34it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.39it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.43it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.48it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.52it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.57it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.61it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.66it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.70it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.75it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.79it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.84it/s, loss=0.0448, v_num=1]\n",
      "Validation DataLoader 0:  50%|âââââ     | 70/139 [00:00<00:00, 339.07it/s]\u001B[A\n",
      "Epoch 46:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 63.88it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 63.92it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 63.96it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.01it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.05it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.10it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.14it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.19it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.23it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.27it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.32it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.36it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.40it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.45it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.49it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.54it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.59it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.63it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.68it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.72it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.77it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.81it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 64.86it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 64.91it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 64.95it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.00it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.04it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.09it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.13it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.18it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.23it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.27it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.32it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.36it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.41it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.45it/s, loss=0.0448, v_num=1]\n",
      "Validation DataLoader 0:  76%|ââââââââ  | 106/139 [00:00<00:00, 348.28it/s]\u001B[A\n",
      "Epoch 46:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.50it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.54it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.59it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.63it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.68it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.72it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.77it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.81it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 65.85it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 65.90it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 65.94it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 65.98it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.03it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.07it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.12it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.16it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.21it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.25it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.29it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.33it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.38it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.42it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.47it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.51it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.55it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.60it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.64it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.69it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.73it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.77it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.82it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.86it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.91it/s, loss=0.0448, v_num=1]\n",
      "Epoch 46: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.42it/s, loss=0.0448, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.32it/s, loss=0.0333, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 47:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.89it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.93it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.98it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.03it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.07it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.12it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.17it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.22it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.26it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.31it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.35it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.40it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.44it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.49it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.53it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.58it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.63it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.67it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.72it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.76it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.81it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.85it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.90it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.94it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.98it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.03it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.07it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.12it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.16it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.21it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.25it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.29it/s, loss=0.0333, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 33/139 [00:00<00:00, 320.63it/s]\u001B[A\n",
      "Epoch 47:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.33it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.38it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.42it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.47it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.51it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.56it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.60it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.64it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.68it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.73it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.77it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.82it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.86it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.91it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.95it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.00it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.04it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.09it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.13it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.18it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.22it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.27it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.31it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.36it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.40it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.45it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.50it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.54it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.59it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.63it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.68it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.72it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.77it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.82it/s, loss=0.0333, v_num=1]\n",
      "Validation DataLoader 0:  48%|âââââ     | 67/139 [00:00<00:00, 329.66it/s]\u001B[A\n",
      "Epoch 47:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.86it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.91it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.95it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.00it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.05it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.09it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.14it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.18it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.23it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.28it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.32it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.37it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.41it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.46it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.50it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.55it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.59it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.64it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.68it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.73it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.77it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.81it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.86it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.90it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.95it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 64.99it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.04it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.08it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.13it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.17it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.22it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.26it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.31it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.35it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.40it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.45it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.49it/s, loss=0.0333, v_num=1]\n",
      "Validation DataLoader 0:  75%|ââââââââ  | 104/139 [00:00<00:00, 347.78it/s]\u001B[A\n",
      "Epoch 47:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.54it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.58it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.63it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.67it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.72it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.77it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.81it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.85it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.90it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.94it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 65.98it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.03it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.07it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.12it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.16it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.21it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.25it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.30it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.34it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.38it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.43it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.47it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.52it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.56it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.61it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.65it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.70it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.74it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.78it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.83it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.87it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.92it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.96it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.01it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.06it/s, loss=0.0333, v_num=1]\n",
      "Epoch 47: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.57it/s, loss=0.0333, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.19it/s, loss=0.0311, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 48:  89%|âââââââââ | 1094/1232 [00:18<00:02, 60.77it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1095/1232 [00:18<00:02, 60.82it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1096/1232 [00:18<00:02, 60.86it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.91it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.96it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1099/1232 [00:18<00:02, 61.00it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1100/1232 [00:18<00:02, 61.04it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.09it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.13it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.18it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.23it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.28it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.32it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.37it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.42it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.46it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.51it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.56it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.60it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.65it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.70it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.74it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.79it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.84it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.88it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.93it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1120/1232 [00:18<00:01, 61.98it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.02it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.07it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.11it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.16it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.21it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.25it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.30it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.35it/s, loss=0.0311, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 355.49it/s]\u001B[A\n",
      "Epoch 48:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.39it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.43it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.48it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.53it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.57it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.62it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.66it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.71it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.76it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.80it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.85it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.90it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.94it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 62.99it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.04it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.08it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.13it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.18it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.22it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.27it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.31it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.36it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.40it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.45it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.50it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.54it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.59it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.63it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.68it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.73it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.77it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.82it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.86it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.91it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 63.96it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.00it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.05it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.09it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.14it/s, loss=0.0311, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 371.01it/s]\u001B[A\n",
      "Epoch 48:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.18it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.23it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.27it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.32it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.37it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.41it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.46it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.51it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.55it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.60it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.65it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.69it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.74it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.78it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.83it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.87it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.92it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 64.96it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.01it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.05it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.10it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.15it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.19it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.24it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.28it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.33it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.38it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.42it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.47it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.51it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.55it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.60it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.65it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.69it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.74it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.78it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.83it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.87it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.92it/s, loss=0.0311, v_num=1]\n",
      "Validation DataLoader 0:  82%|âââââââââ | 114/139 [00:00<00:00, 376.83it/s]\u001B[A\n",
      "Epoch 48:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 65.96it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.00it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.05it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.09it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.14it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.18it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.22it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.26it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.31it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.35it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.40it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.44it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.49it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.53it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.58it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.62it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.67it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.71it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.75it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.78it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.82it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.85it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.88it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.92it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.95it/s, loss=0.0311, v_num=1]\n",
      "Epoch 48: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.46it/s, loss=0.0311, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.11it/s, loss=0.031, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 49:  89%|âââââââââ | 1094/1232 [00:18<00:02, 60.68it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1095/1232 [00:18<00:02, 60.72it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1096/1232 [00:18<00:02, 60.77it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1097/1232 [00:18<00:02, 60.82it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1098/1232 [00:18<00:02, 60.86it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1099/1232 [00:18<00:02, 60.91it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1100/1232 [00:18<00:02, 60.95it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1101/1232 [00:18<00:02, 61.00it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  89%|âââââââââ | 1102/1232 [00:18<00:02, 61.04it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.09it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.13it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.18it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.22it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.27it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.31it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1109/1232 [00:18<00:02, 61.36it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.40it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.45it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.50it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.54it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.59it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.63it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.68it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.72it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.77it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1119/1232 [00:18<00:01, 61.82it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1120/1232 [00:18<00:01, 61.87it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1121/1232 [00:18<00:01, 61.91it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1122/1232 [00:18<00:01, 61.96it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.01it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.05it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.10it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.15it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.19it/s, loss=0.031, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 349.63it/s]\u001B[A\n",
      "Epoch 49:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.24it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.29it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.33it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.38it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.43it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.47it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.52it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.56it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.61it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.66it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.70it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.75it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.80it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 62.84it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 62.89it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 62.94it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 62.98it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.03it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.08it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.12it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.16it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.21it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.25it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.30it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.34it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.39it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.43it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.48it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.52it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.57it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.61it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.66it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.70it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.75it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 63.79it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 63.84it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 63.88it/s, loss=0.031, v_num=1]\n",
      "Validation DataLoader 0:  52%|ââââââ    | 72/139 [00:00<00:00, 359.68it/s]\u001B[A\n",
      "Epoch 49:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 63.92it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 63.97it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.01it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.06it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.10it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.15it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.19it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.24it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.29it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.33it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.37it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.42it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.46it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.51it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.56it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.60it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.65it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.69it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.74it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 64.78it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 64.83it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 64.88it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 64.92it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 64.97it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.02it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.06it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.11it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.16it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.20it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.24it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.29it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.33it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.38it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.42it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.47it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.51it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.55it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.60it/s, loss=0.031, v_num=1]\n",
      "Validation DataLoader 0:  79%|ââââââââ  | 110/139 [00:00<00:00, 366.18it/s]\u001B[A\n",
      "Epoch 49:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.64it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.69it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.73it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 65.77it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 65.82it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 65.86it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 65.91it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 65.95it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 65.99it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.04it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.08it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.13it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.17it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.22it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.26it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.31it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.35it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.40it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.44it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.49it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.53it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.58it/s, loss=0.031, v_num=1]\n",
      "Epoch 49:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.62it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.67it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.72it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.75it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.78it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.81it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 66.85it/s, loss=0.031, v_num=1]\n",
      "Epoch 49: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.36it/s, loss=0.031, v_num=1]\n",
      "Epoch 50:   0%|          | 0/1232 [00:00<?, ?it/s, loss=0.031, v_num=1]           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Swapping scheduler `ChainedScheduler` for `SWALR`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.58it/s, loss=0.0287, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 50:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.17it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.21it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.26it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.30it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.35it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.40it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.44it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.49it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.54it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.58it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.63it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.68it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.72it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.77it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.82it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.86it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.91it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.96it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.01it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.06it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.10it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.15it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.20it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.25it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.29it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.34it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.39it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.43it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.48it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.53it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.58it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.62it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.67it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.72it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.76it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.81it/s, loss=0.0287, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 368.01it/s]\u001B[A\n",
      "Epoch 50:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.85it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.90it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.95it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.99it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.04it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.09it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.14it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.18it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.23it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.27it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.32it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.37it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.41it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.46it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.51it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.55it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.60it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.65it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.69it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.74it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.79it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.83it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.88it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.93it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.98it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.02it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.07it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.11it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.16it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.20it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.25it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.29it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.34it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.39it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.43it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.48it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.53it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.57it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.62it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.67it/s, loss=0.0287, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 382.32it/s]\u001B[A\n",
      "Epoch 50:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.71it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.76it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.80it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.85it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.90it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.94it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.99it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.03it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.08it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.12it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.17it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.21it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.26it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.30it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.35it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.40it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.44it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.49it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.53it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.58it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.62it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.67it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.72it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.77it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.81it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.86it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.90it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.95it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.99it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.04it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.09it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.13it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.18it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.22it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.27it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.31it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.36it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.40it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.45it/s, loss=0.0287, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 381.60it/s]\u001B[A\n",
      "Epoch 50:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.49it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.54it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.58it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.63it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.67it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.70it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.74it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.77it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.80it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.84it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.87it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.90it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.94it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.97it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.00it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.03it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.07it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.10it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.13it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.16it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.20it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.23it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.26it/s, loss=0.0287, v_num=1]\n",
      "Epoch 50: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.75it/s, loss=0.0287, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.62it/s, loss=0.0264, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 51:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.19it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.24it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.29it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.33it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.38it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.43it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.47it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.52it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.57it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.62it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.66it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.71it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.76it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.81it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.86it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.90it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.95it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.00it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.04it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.09it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.13it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.36it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.40it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.45it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.49it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.54it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.58it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.67it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.72it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 348.84it/s]\u001B[A\n",
      "Epoch 51:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.76it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.81it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.85it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.90it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.95it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.04it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.09it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.14it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.23it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.32it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.36it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.41it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.45it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.50it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.54it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.59it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.68it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.73it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.77it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.82it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.86it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.91it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.96it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.00it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.05it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.09it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.13it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.36it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  51%|âââââ     | 71/139 [00:00<00:00, 353.00it/s]\u001B[A\n",
      "Epoch 51:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.40it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.45it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.50it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.54it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.59it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.68it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.73it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.77it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.82it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.87it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.91it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.96it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.00it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.05it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.09it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.13it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.36it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.40it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.44it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.49it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.53it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.58it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.67it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.72it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.77it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.81it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.86it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.90it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.95it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.04it/s, loss=0.0264, v_num=1]\n",
      "Validation DataLoader 0:  78%|ââââââââ  | 108/139 [00:00<00:00, 359.96it/s]\u001B[A\n",
      "Epoch 51:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.08it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.13it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.17it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.22it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.36it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.40it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.45it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.49it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.54it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.58it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.63it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.68it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.72it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.77it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.81it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.85it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.90it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.94it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.99it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.03it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.07it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.11it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.14it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.18it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.21it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.24it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.27it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.31it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.34it/s, loss=0.0264, v_num=1]\n",
      "Epoch 51: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.84it/s, loss=0.0264, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.60it/s, loss=0.0233, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 52:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.19it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.24it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.29it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.33it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.38it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.42it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.47it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.51it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.56it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.61it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.65it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.70it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.75it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.80it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.84it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.89it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.94it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.98it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.03it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.08it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.13it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.17it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.22it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.27it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.31it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.36it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.41it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.46it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.50it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.55it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.60it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.65it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.69it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.74it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.79it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.84it/s, loss=0.0233, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 369.54it/s]\u001B[A\n",
      "Epoch 52:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.88it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.93it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.97it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.02it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.07it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.11it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.16it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.21it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.25it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.30it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.35it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.39it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.44it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.49it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.54it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.58it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.63it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.68it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.72it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.77it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.82it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.87it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.92it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.96it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.01it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.06it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.10it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.15it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.20it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.24it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.29it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.34it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.38it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.43it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.48it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.53it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.57it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.62it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.66it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.71it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.75it/s, loss=0.0233, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 388.19it/s]\u001B[A\n",
      "Epoch 52:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.79it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.84it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.88it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.93it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.97it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.02it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.06it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.11it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.15it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.20it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.25it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.29it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.34it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.38it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.43it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.47it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.52it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.56it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.61it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.65it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.70it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.75it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.79it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.83it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.88it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.92it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.97it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.01it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.06it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.10it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.15it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.20it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.24it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.29it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.34it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.38it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.43it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.48it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.52it/s, loss=0.0233, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 381.00it/s]\u001B[A\n",
      "Epoch 52:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.57it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.61it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.66it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.70it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.74it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.79it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.84it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.88it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.92it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.96it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.99it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.02it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.06it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.09it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.12it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.15it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.19it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.22it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.26it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.29it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.32it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.35it/s, loss=0.0233, v_num=1]\n",
      "Epoch 52: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.84it/s, loss=0.0233, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.59it/s, loss=0.0219, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 53:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.14it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.19it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.23it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.28it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.33it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.37it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.42it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.47it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.52it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.56it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.61it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.66it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.70it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.75it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.80it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.85it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.90it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.94it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.99it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.04it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.08it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.13it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.18it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.23it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.27it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.32it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.36it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.41it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.46it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.51it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.55it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.60it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.65it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.70it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.74it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.79it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.84it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.89it/s, loss=0.0219, v_num=1]\n",
      "Validation DataLoader 0:  28%|âââ       | 39/139 [00:00<00:00, 388.16it/s]\u001B[A\n",
      "Epoch 53:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.93it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.98it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.02it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.07it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.11it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.16it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.21it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.25it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.30it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.34it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.39it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.44it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.48it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.53it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.58it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.63it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.67it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.72it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.77it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.82it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.86it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.91it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.96it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.01it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.05it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.10it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.15it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.19it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.24it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.29it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.34it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.38it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.43it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.48it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.53it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.57it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.62it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.67it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.71it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.76it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.81it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.85it/s, loss=0.0219, v_num=1]\n",
      "Validation DataLoader 0:  58%|ââââââ    | 81/139 [00:00<00:00, 400.43it/s]\u001B[A\n",
      "Epoch 53:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.89it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.94it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.98it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.02it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.07it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.12it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.16it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.21it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.26it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.31it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.35it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.40it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.44it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.49it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.54it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.58it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.63it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.68it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.72it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.77it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.82it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.86it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.91it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.96it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.00it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.04it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.09it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.13it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.18it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.22it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.27it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.32it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.36it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.41it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.45it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.50it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.54it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.59it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.64it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.68it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.73it/s, loss=0.0219, v_num=1]\n",
      "Validation DataLoader 0:  88%|âââââââââ | 122/139 [00:00<00:00, 391.50it/s]\u001B[A\n",
      "Epoch 53:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.77it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.80it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.83it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.86it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.90it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.93it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.96it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.00it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.03it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.06it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.09it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.13it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.16it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.20it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.23it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.26it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.29it/s, loss=0.0219, v_num=1]\n",
      "Epoch 53: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.77it/s, loss=0.0219, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.56it/s, loss=0.023, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 54:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.12it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.17it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.21it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.26it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.31it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.36it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.40it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.45it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.50it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.54it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.59it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.64it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.68it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.73it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.78it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.82it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.87it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.92it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.96it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.01it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.05it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.10it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.14it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.19it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.23it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.28it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.32it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.37it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.41it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.46it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.50it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.55it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.59it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.64it/s, loss=0.023, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 337.71it/s]\u001B[A\n",
      "Epoch 54:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.68it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.72it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.77it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.82it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.86it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.91it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.96it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.00it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.05it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.09it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.14it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.19it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.23it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.28it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.33it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.37it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.42it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.47it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.51it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.56it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.61it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.65it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.70it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.75it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.79it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.84it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.89it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.93it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.98it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.02it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.07it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.12it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.17it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.21it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.26it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.31it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.35it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.40it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.45it/s, loss=0.023, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 74/139 [00:00<00:00, 367.75it/s]\u001B[A\n",
      "Epoch 54:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.49it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.53it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.58it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.63it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.67it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.72it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.77it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.81it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.86it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.90it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.95it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.00it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.04it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.09it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.14it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.18it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.23it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.28it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.32it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.37it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.41it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.45it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.50it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.54it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.59it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.64it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.68it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.73it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.77it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.82it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.86it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.91it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.96it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.00it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.05it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.10it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.14it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.19it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.23it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.28it/s, loss=0.023, v_num=1]\n",
      "Validation DataLoader 0:  82%|âââââââââ | 114/139 [00:00<00:00, 379.05it/s]\u001B[A\n",
      "Epoch 54:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.32it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.37it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.41it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.46it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.50it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.55it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.59it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.64it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.69it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.73it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.78it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.82it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.87it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.92it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.96it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.99it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.03it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.06it/s, loss=0.023, v_num=1]\n",
      "Epoch 54:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.09it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.12it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.16it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.19it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.22it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.26it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.29it/s, loss=0.023, v_num=1]\n",
      "Epoch 54: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.78it/s, loss=0.023, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.51it/s, loss=0.021, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 55:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.09it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.13it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.18it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.23it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.27it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.32it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.36it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.41it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.46it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.50it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.55it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.59it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.64it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.69it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.73it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.78it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.83it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.87it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.92it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.97it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.01it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.06it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.11it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.16it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.20it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.25it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.30it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.34it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.39it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.43it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.48it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.53it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.57it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.62it/s, loss=0.021, v_num=1]\n",
      "Validation DataLoader 0:  25%|âââ       | 35/139 [00:00<00:00, 345.66it/s]\u001B[A\n",
      "Epoch 55:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.67it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.71it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.76it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.81it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.85it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.90it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.95it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.99it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.04it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.09it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.14it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.18it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.23it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.27it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.32it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.37it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.41it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.46it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.51it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.55it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.60it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.65it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.69it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.74it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.79it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.83it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.88it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.92it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.97it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.02it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.06it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.11it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.16it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.20it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.25it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.29it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.34it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.38it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.43it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.48it/s, loss=0.021, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 373.31it/s]\u001B[A\n",
      "Epoch 55:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.52it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.57it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.62it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.66it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.71it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.76it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.80it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.85it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.89it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.94it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.99it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.03it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.08it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.13it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.17it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.22it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.27it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.31it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.36it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.41it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.45it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.50it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.55it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.59it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.64it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.68it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.73it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.78it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.82it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.87it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.92it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.96it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.01it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.06it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.11it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.15it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.20it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.24it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.29it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.34it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.38it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.43it/s, loss=0.021, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 391.74it/s]\u001B[A\n",
      "Epoch 55:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.47it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.51it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.56it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.61it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.65it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.69it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.74it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.78it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.83it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.87it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.91it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.94it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.97it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.01it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.04it/s, loss=0.021, v_num=1]\n",
      "Epoch 55:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.07it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.11it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.14it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.17it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.20it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.24it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.27it/s, loss=0.021, v_num=1]\n",
      "Epoch 55: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.76it/s, loss=0.021, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.55it/s, loss=0.0174, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 56:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.12it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.16it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.21it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.26it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.30it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.35it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.39it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.44it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.49it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.54it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.58it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.63it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.68it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.72it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.77it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.82it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.86it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.91it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.95it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.00it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.05it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.09it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.14it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.19it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.24it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.28it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.33it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.38it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.42it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.47it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.52it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.56it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.61it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.66it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.71it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.75it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.80it/s, loss=0.0174, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 373.20it/s]\u001B[A\n",
      "Epoch 56:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.85it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.89it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.94it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.99it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.03it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.08it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.12it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.17it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.22it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.27it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.31it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.36it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.41it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.45it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.50it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.55it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.59it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.64it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.69it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.73it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.78it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.82it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.87it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.92it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.96it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.01it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.05it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.10it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.15it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.19it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.24it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.29it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.33it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.38it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.43it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.47it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.52it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.57it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.61it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.66it/s, loss=0.0174, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 384.30it/s]\u001B[A\n",
      "Epoch 56:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.70it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.75it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.79it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.84it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.89it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.93it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.98it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.02it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.07it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.12it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.16it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.21it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.25it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.30it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.35it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.39it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.44it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.49it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.53it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.58it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.62it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.67it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.72it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.77it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.81it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.86it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.90it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.95it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.00it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.04it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.09it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.14it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.18it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.23it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.27it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.32it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.36it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.41it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.45it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.50it/s, loss=0.0174, v_num=1]\n",
      "Validation DataLoader 0:  85%|âââââââââ | 118/139 [00:00<00:00, 389.00it/s]\u001B[A\n",
      "Epoch 56:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.54it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.59it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.63it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.68it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.72it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.77it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.82it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.86it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.90it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.93it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.96it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.99it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.03it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.06it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.10it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.13it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.16it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.20it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.23it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.26it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.30it/s, loss=0.0174, v_num=1]\n",
      "Epoch 56: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.79it/s, loss=0.0174, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.45it/s, loss=0.0175, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 57:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.02it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.06it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.11it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.15it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.20it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.25it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.30it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.34it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.39it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.44it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.48it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.53it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.58it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.63it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.68it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.72it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.77it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.82it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.86it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.91it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.95it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.00it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.05it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.10it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.14it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.19it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.23it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.28it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.32it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.37it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.42it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.46it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.51it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.56it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.60it/s, loss=0.0175, v_num=1]\n",
      "Validation DataLoader 0:  26%|âââ       | 36/139 [00:00<00:00, 357.17it/s]\u001B[A\n",
      "Epoch 57:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.64it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.69it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.74it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.78it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.83it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.88it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.92it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.97it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.02it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.07it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.12it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.16it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.21it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.26it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.30it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.35it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.40it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.44it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.49it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.54it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.58it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.63it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.67it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.72it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.77it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.82it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.86it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.91it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.96it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.00it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.05it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.10it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.14it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.19it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.23it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.28it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.32it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.37it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.42it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.46it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.51it/s, loss=0.0175, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 383.68it/s]\u001B[A\n",
      "Epoch 57:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.56it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.60it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.65it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.70it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.74it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.79it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.83it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.88it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.93it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.97it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.02it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.06it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.11it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.15it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.20it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.24it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.29it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.33it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.38it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.43it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.48it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.52it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.57it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.61it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.66it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.71it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.75it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.80it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.84it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.89it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.94it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.98it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.03it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.07it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.12it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.16it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.21it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.25it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.30it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.35it/s, loss=0.0175, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 388.25it/s]\u001B[A\n",
      "Epoch 57:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.39it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.43it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.48it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.52it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.57it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.61it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.66it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.71it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.75it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.80it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.84it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.87it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.91it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.94it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.97it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.00it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.04it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.07it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.10it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.13it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.16it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.19it/s, loss=0.0175, v_num=1]\n",
      "Epoch 57: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.69it/s, loss=0.0175, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.65it/s, loss=0.0209, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 58:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.24it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.29it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.33it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.38it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.43it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.47it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.52it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.57it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.62it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.66it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.71it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.76it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.80it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.85it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.90it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.95it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.99it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.04it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.09it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.13it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.18it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.23it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.27it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.32it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.37it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.41it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.46it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.51it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.56it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.60it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.65it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.70it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.74it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.79it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.83it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.88it/s, loss=0.0209, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 368.81it/s]\u001B[A\n",
      "Epoch 58:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.92it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.97it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.02it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.06it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.11it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.15it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.20it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.25it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.29it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.34it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.39it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.43it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.48it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.53it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1144/1232 [00:17<00:01, 63.57it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1145/1232 [00:17<00:01, 63.62it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1146/1232 [00:17<00:01, 63.67it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.72it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.76it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.81it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.86it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.90it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.95it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 64.00it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.05it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.10it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.14it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.19it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.23it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.28it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.33it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.37it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.42it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.47it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.51it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.56it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.61it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.66it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.70it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.75it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.80it/s, loss=0.0209, v_num=1]\n",
      "Validation DataLoader 0:  56%|ââââââ    | 78/139 [00:00<00:00, 389.25it/s]\u001B[A\n",
      "Epoch 58:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.84it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.88it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.93it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.97it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 65.02it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.06it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.11it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.15it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.20it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.24it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.29it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.34it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.38it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.43it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.48it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.52it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.57it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.61it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.66it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.70it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.75it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.79it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.84it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.88it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.92it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.97it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.01it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.06it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.10it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.15it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.19it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.24it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.28it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.33it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.37it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.42it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.46it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.51it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.55it/s, loss=0.0209, v_num=1]\n",
      "Validation DataLoader 0:  84%|âââââââââ | 117/139 [00:00<00:00, 373.87it/s]\u001B[A\n",
      "Epoch 58:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.60it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.64it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.69it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.73it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.78it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.83it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.87it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.92it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.97it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 67.01it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.05it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.10it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.13it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.16it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.19it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.23it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.26it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.30it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.33it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.36it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.39it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.43it/s, loss=0.0209, v_num=1]\n",
      "Epoch 58: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.92it/s, loss=0.0209, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.68it/s, loss=0.0199, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 59:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.25it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.29it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.34it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.38it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.43it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.48it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.53it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.57it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.62it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.67it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.72it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.76it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.81it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.85it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.90it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.94it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.99it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1111/1232 [00:17<00:01, 62.04it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.08it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.13it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.17it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.22it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.26it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.31it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.35it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.40it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.45it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.49it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.54it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.58it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.63it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.67it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.72it/s, loss=0.0199, v_num=1]\n",
      "Validation DataLoader 0:  24%|âââ       | 34/139 [00:00<00:00, 337.53it/s]\u001B[A\n",
      "Epoch 59:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.76it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.81it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.86it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.90it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.95it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 63.00it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 63.05it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.09it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.14it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.18it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1137/1232 [00:17<00:01, 63.23it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1138/1232 [00:17<00:01, 63.28it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  92%|ââââââââââ| 1139/1232 [00:17<00:01, 63.33it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1140/1232 [00:17<00:01, 63.37it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1141/1232 [00:17<00:01, 63.42it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1142/1232 [00:17<00:01, 63.47it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1143/1232 [00:17<00:01, 63.51it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.55it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.60it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.64it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.69it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.74it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.78it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.83it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.88it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.92it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.96it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 64.01it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.06it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.10it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.15it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.20it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.24it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.29it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.34it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.38it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.43it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.48it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.52it/s, loss=0.0199, v_num=1]\n",
      "Validation DataLoader 0:  53%|ââââââ    | 73/139 [00:00<00:00, 363.98it/s]\u001B[A\n",
      "Epoch 59:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.57it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.62it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.66it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.71it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.75it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.80it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.85it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.89it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.94it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.99it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 65.03it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.08it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.12it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.17it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.21it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.26it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.31it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.35it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.40it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.44it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.49it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.53it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.58it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.63it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.67it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.72it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.77it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.81it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.86it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.91it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.95it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 66.00it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 66.04it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.09it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.13it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.18it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.22it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.26it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.31it/s, loss=0.0199, v_num=1]\n",
      "Validation DataLoader 0:  81%|ââââââââ  | 112/139 [00:00<00:00, 371.94it/s]\u001B[A\n",
      "Epoch 59:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.35it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.40it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.44it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.49it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.53it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.58it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.62it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.67it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.71it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.76it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.80it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.85it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.89it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.94it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.98it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 67.03it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 67.07it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.12it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.16it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.20it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.23it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.27it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.30it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.33it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.36it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.39it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.42it/s, loss=0.0199, v_num=1]\n",
      "Epoch 59: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.91it/s, loss=0.0199, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.54it/s, loss=0.027, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 60:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.16it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.20it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.25it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.29it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.34it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.39it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.44it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.48it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.53it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.58it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.62it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.67it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.72it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.77it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.81it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.86it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.90it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.95it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1112/1232 [00:17<00:01, 62.00it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1113/1232 [00:17<00:01, 62.04it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.09it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.14it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.19it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.23it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.28it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.32it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.37it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.41it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.46it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.51it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.55it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.60it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.65it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.70it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.74it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1129/1232 [00:17<00:01, 62.79it/s, loss=0.027, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 365.94it/s]\u001B[A\n",
      "Epoch 60:  92%|ââââââââââ| 1130/1232 [00:17<00:01, 62.84it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1131/1232 [00:17<00:01, 62.88it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1132/1232 [00:17<00:01, 62.93it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1133/1232 [00:17<00:01, 62.98it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1134/1232 [00:17<00:01, 63.02it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1135/1232 [00:17<00:01, 63.07it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1136/1232 [00:17<00:01, 63.12it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.17it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.21it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.26it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.31it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.35it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.40it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.45it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.49it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.54it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.59it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.63it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.68it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.73it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.77it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.82it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.87it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.91it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.95it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 64.00it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 64.05it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.09it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.14it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.18it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.23it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.27it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.32it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.36it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.41it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.46it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.51it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.55it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.60it/s, loss=0.027, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 76/139 [00:00<00:00, 378.10it/s]\u001B[A\n",
      "Epoch 60:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.64it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.69it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.73it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.77it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.82it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.87it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.91it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.96it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 65.01it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 65.05it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.10it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.14it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.19it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.23it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.28it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.33it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.37it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.42it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.46it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.51it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.56it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.60it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.65it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.69it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.74it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.79it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.83it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.88it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.93it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.97it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 66.02it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 66.06it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.11it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.15it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.19it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.24it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.28it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.32it/s, loss=0.027, v_num=1]\n",
      "Validation DataLoader 0:  82%|âââââââââ | 114/139 [00:00<00:00, 372.34it/s]\u001B[A\n",
      "Epoch 60:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.36it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.41it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.45it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.50it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.54it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.59it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.64it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.68it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.72it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.77it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.81it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.86it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.90it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.95it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.99it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 67.03it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.06it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.10it/s, loss=0.027, v_num=1]\n",
      "Epoch 60:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.13it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.17it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.20it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.23it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.26it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.29it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.33it/s, loss=0.027, v_num=1]\n",
      "Epoch 60: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.82it/s, loss=0.027, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.25it/s, loss=0.0177, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 61:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.86it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1095/1232 [00:17<00:02, 60.90it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1096/1232 [00:17<00:02, 60.95it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.00it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.04it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.09it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.13it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.18it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.23it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1103/1232 [00:18<00:02, 61.28it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1104/1232 [00:18<00:02, 61.32it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1105/1232 [00:18<00:02, 61.37it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1106/1232 [00:18<00:02, 61.42it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1107/1232 [00:18<00:02, 61.46it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1108/1232 [00:18<00:02, 61.51it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1109/1232 [00:18<00:01, 61.56it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1110/1232 [00:18<00:01, 61.60it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1111/1232 [00:18<00:01, 61.65it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1112/1232 [00:18<00:01, 61.70it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1113/1232 [00:18<00:01, 61.74it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  90%|âââââââââ | 1114/1232 [00:18<00:01, 61.79it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1115/1232 [00:18<00:01, 61.84it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1116/1232 [00:18<00:01, 61.88it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1117/1232 [00:18<00:01, 61.93it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1118/1232 [00:18<00:01, 61.97it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.02it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.06it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.11it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.16it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.21it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.25it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.30it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.35it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.39it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.44it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.48it/s, loss=0.0177, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 37/139 [00:00<00:00, 364.95it/s]\u001B[A\n",
      "Epoch 61:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.53it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.58it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.62it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.67it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.71it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.76it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.81it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 62.85it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 62.90it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 62.95it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 62.99it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.04it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.09it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.13it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.18it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.22it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.27it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.32it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.36it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.41it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.45it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.50it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.54it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.59it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.63it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.68it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.72it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.76it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.81it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 63.86it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 63.90it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 63.95it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.00it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.04it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.09it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.13it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.18it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.23it/s, loss=0.0177, v_num=1]\n",
      "Validation DataLoader 0:  54%|ââââââ    | 75/139 [00:00<00:00, 371.08it/s]\u001B[A\n",
      "Epoch 61:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.27it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.32it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.36it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.41it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.46it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.50it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.55it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.60it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.64it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.69it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.73it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.78it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 64.83it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 64.87it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 64.92it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 64.97it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.01it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.06it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.10it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.15it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.20it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.24it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.29it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.33it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.38it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.43it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.47it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.52it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.56it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.61it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.65it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.70it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.74it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.79it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 65.84it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 65.88it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 65.93it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 65.97it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.02it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.06it/s, loss=0.0177, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 115/139 [00:00<00:00, 379.57it/s]\u001B[A\n",
      "Epoch 61:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.10it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.14it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.18it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.23it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.27it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.31it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.36it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.40it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.45it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.49it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.54it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.58it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.62it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.66it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.71it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.75it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.78it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.82it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 66.85it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 66.88it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 66.92it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 66.95it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 66.98it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.02it/s, loss=0.0177, v_num=1]\n",
      "Epoch 61: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.52it/s, loss=0.0177, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.48it/s, loss=0.017, v_num=1] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 62:  89%|âââââââââ | 1094/1232 [00:17<00:02, 61.08it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.13it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.17it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.22it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.27it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.32it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.37it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.41it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.46it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.51it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.55it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.60it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.64it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.69it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.74it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.79it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.83it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.88it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.93it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.97it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  90%|âââââââââ | 1114/1232 [00:17<00:01, 62.02it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1115/1232 [00:17<00:01, 62.07it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.11it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.16it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.21it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1119/1232 [00:17<00:01, 62.25it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1120/1232 [00:17<00:01, 62.30it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1121/1232 [00:17<00:01, 62.35it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1122/1232 [00:17<00:01, 62.39it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1123/1232 [00:17<00:01, 62.44it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|âââââââââ | 1124/1232 [00:17<00:01, 62.49it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|ââââââââââ| 1125/1232 [00:17<00:01, 62.53it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|ââââââââââ| 1126/1232 [00:17<00:01, 62.58it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  91%|ââââââââââ| 1127/1232 [00:17<00:01, 62.63it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1128/1232 [00:17<00:01, 62.67it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.72it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.77it/s, loss=0.017, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 372.87it/s]\u001B[A\n",
      "Epoch 62:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.81it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.86it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.91it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.95it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 63.00it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 63.04it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.09it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.14it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.18it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.23it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.28it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.32it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.37it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.42it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.46it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.51it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.56it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.60it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.65it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.70it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.74it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.79it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.84it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.88it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.93it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.98it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 64.02it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 64.07it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.11it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.16it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.21it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.25it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.30it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.34it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.39it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.44it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.48it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.53it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.57it/s, loss=0.017, v_num=1]\n",
      "Validation DataLoader 0:  55%|ââââââ    | 77/139 [00:00<00:00, 381.11it/s]\u001B[A\n",
      "Epoch 62:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.62it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.66it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.71it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.75it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.80it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.84it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.89it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.93it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.98it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 65.03it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.07it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.12it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.16it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.21it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.26it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.30it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.35it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.39it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.44it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.49it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.53it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.58it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.63it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.67it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.72it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.76it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.81it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.85it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.90it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.95it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.99it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 66.04it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.08it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.13it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.17it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.22it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.26it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.31it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.35it/s, loss=0.017, v_num=1]\n",
      "Validation DataLoader 0:  83%|âââââââââ | 116/139 [00:00<00:00, 382.02it/s]\u001B[A\n",
      "Epoch 62:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.40it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.44it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.49it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.53it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.58it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.62it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.67it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.72it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.76it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.81it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.85it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.90it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.94it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.97it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 67.00it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 67.03it/s, loss=0.017, v_num=1]\n",
      "Epoch 62:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 67.07it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.10it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.13it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.16it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.20it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.23it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.26it/s, loss=0.017, v_num=1]\n",
      "Epoch 62: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.75it/s, loss=0.017, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1093/1232 [00:17<00:02, 61.40it/s, loss=0.0171, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/139 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 63:  89%|âââââââââ | 1094/1232 [00:17<00:02, 60.99it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1095/1232 [00:17<00:02, 61.04it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1096/1232 [00:17<00:02, 61.09it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1097/1232 [00:17<00:02, 61.13it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1098/1232 [00:17<00:02, 61.18it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1099/1232 [00:17<00:02, 61.23it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1100/1232 [00:17<00:02, 61.27it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1101/1232 [00:17<00:02, 61.32it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  89%|âââââââââ | 1102/1232 [00:17<00:02, 61.37it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1103/1232 [00:17<00:02, 61.41it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1104/1232 [00:17<00:02, 61.46it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1105/1232 [00:17<00:02, 61.51it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1106/1232 [00:17<00:02, 61.55it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1107/1232 [00:17<00:02, 61.60it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1108/1232 [00:17<00:02, 61.65it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1109/1232 [00:17<00:01, 61.69it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1110/1232 [00:17<00:01, 61.74it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1111/1232 [00:17<00:01, 61.79it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1112/1232 [00:17<00:01, 61.83it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1113/1232 [00:17<00:01, 61.88it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  90%|âââââââââ | 1114/1232 [00:17<00:01, 61.93it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1115/1232 [00:17<00:01, 61.98it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1116/1232 [00:17<00:01, 62.02it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1117/1232 [00:17<00:01, 62.07it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1118/1232 [00:17<00:01, 62.12it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1119/1232 [00:18<00:01, 62.17it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1120/1232 [00:18<00:01, 62.21it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1121/1232 [00:18<00:01, 62.26it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1122/1232 [00:18<00:01, 62.31it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1123/1232 [00:18<00:01, 62.35it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|âââââââââ | 1124/1232 [00:18<00:01, 62.40it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|ââââââââââ| 1125/1232 [00:18<00:01, 62.45it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|ââââââââââ| 1126/1232 [00:18<00:01, 62.49it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  91%|ââââââââââ| 1127/1232 [00:18<00:01, 62.54it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1128/1232 [00:18<00:01, 62.59it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1129/1232 [00:18<00:01, 62.63it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1130/1232 [00:18<00:01, 62.68it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  27%|âââ       | 38/139 [00:00<00:00, 373.20it/s]\u001B[A\n",
      "Epoch 63:  92%|ââââââââââ| 1131/1232 [00:18<00:01, 62.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1132/1232 [00:18<00:01, 62.77it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1133/1232 [00:18<00:01, 62.82it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1134/1232 [00:18<00:01, 62.86it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1135/1232 [00:18<00:01, 62.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1136/1232 [00:18<00:01, 62.96it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1137/1232 [00:18<00:01, 63.01it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1138/1232 [00:18<00:01, 63.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  92%|ââââââââââ| 1139/1232 [00:18<00:01, 63.10it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1140/1232 [00:18<00:01, 63.15it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1141/1232 [00:18<00:01, 63.20it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1142/1232 [00:18<00:01, 63.24it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1143/1232 [00:18<00:01, 63.29it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1144/1232 [00:18<00:01, 63.34it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1145/1232 [00:18<00:01, 63.38it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1146/1232 [00:18<00:01, 63.43it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1147/1232 [00:18<00:01, 63.48it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1148/1232 [00:18<00:01, 63.53it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1149/1232 [00:18<00:01, 63.57it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1150/1232 [00:18<00:01, 63.62it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  93%|ââââââââââ| 1151/1232 [00:18<00:01, 63.67it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1152/1232 [00:18<00:01, 63.71it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1153/1232 [00:18<00:01, 63.76it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1154/1232 [00:18<00:01, 63.81it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1155/1232 [00:18<00:01, 63.85it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1156/1232 [00:18<00:01, 63.90it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1157/1232 [00:18<00:01, 63.95it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1158/1232 [00:18<00:01, 63.99it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1159/1232 [00:18<00:01, 64.04it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1160/1232 [00:18<00:01, 64.08it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1161/1232 [00:18<00:01, 64.13it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1162/1232 [00:18<00:01, 64.17it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1163/1232 [00:18<00:01, 64.22it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  94%|ââââââââââ| 1164/1232 [00:18<00:01, 64.27it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1165/1232 [00:18<00:01, 64.31it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1166/1232 [00:18<00:01, 64.36it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1167/1232 [00:18<00:01, 64.41it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1168/1232 [00:18<00:00, 64.45it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1169/1232 [00:18<00:00, 64.50it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1170/1232 [00:18<00:00, 64.55it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1171/1232 [00:18<00:00, 64.60it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  57%|ââââââ    | 79/139 [00:00<00:00, 394.18it/s]\u001B[A\n",
      "Epoch 63:  95%|ââââââââââ| 1172/1232 [00:18<00:00, 64.64it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1173/1232 [00:18<00:00, 64.69it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1174/1232 [00:18<00:00, 64.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1175/1232 [00:18<00:00, 64.78it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  95%|ââââââââââ| 1176/1232 [00:18<00:00, 64.82it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1177/1232 [00:18<00:00, 64.87it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1178/1232 [00:18<00:00, 64.92it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1179/1232 [00:18<00:00, 64.96it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1180/1232 [00:18<00:00, 65.01it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1181/1232 [00:18<00:00, 65.05it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1182/1232 [00:18<00:00, 65.10it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1183/1232 [00:18<00:00, 65.14it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1184/1232 [00:18<00:00, 65.19it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1185/1232 [00:18<00:00, 65.24it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1186/1232 [00:18<00:00, 65.28it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1187/1232 [00:18<00:00, 65.33it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  96%|ââââââââââ| 1188/1232 [00:18<00:00, 65.37it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1189/1232 [00:18<00:00, 65.42it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1190/1232 [00:18<00:00, 65.47it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1191/1232 [00:18<00:00, 65.51it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1192/1232 [00:18<00:00, 65.56it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1193/1232 [00:18<00:00, 65.60it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1194/1232 [00:18<00:00, 65.65it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1195/1232 [00:18<00:00, 65.70it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1196/1232 [00:18<00:00, 65.74it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1197/1232 [00:18<00:00, 65.79it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1198/1232 [00:18<00:00, 65.83it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1199/1232 [00:18<00:00, 65.88it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1200/1232 [00:18<00:00, 65.93it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  97%|ââââââââââ| 1201/1232 [00:18<00:00, 65.97it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1202/1232 [00:18<00:00, 66.02it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1203/1232 [00:18<00:00, 66.07it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1204/1232 [00:18<00:00, 66.11it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1205/1232 [00:18<00:00, 66.16it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1206/1232 [00:18<00:00, 66.21it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1207/1232 [00:18<00:00, 66.25it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1208/1232 [00:18<00:00, 66.30it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1209/1232 [00:18<00:00, 66.34it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1210/1232 [00:18<00:00, 66.39it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1211/1232 [00:18<00:00, 66.43it/s, loss=0.0171, v_num=1]\n",
      "Validation DataLoader 0:  86%|âââââââââ | 119/139 [00:00<00:00, 393.82it/s]\u001B[A\n",
      "Epoch 63:  98%|ââââââââââ| 1212/1232 [00:18<00:00, 66.47it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  98%|ââââââââââ| 1213/1232 [00:18<00:00, 66.51it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1214/1232 [00:18<00:00, 66.56it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1215/1232 [00:18<00:00, 66.60it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1216/1232 [00:18<00:00, 66.64it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1217/1232 [00:18<00:00, 66.69it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1218/1232 [00:18<00:00, 66.73it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1219/1232 [00:18<00:00, 66.78it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1220/1232 [00:18<00:00, 66.81it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1221/1232 [00:18<00:00, 66.84it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1222/1232 [00:18<00:00, 66.88it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1223/1232 [00:18<00:00, 66.91it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1224/1232 [00:18<00:00, 66.94it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63:  99%|ââââââââââ| 1225/1232 [00:18<00:00, 66.97it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1226/1232 [00:18<00:00, 67.00it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1227/1232 [00:18<00:00, 67.04it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1228/1232 [00:18<00:00, 67.07it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1229/1232 [00:18<00:00, 67.10it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1230/1232 [00:18<00:00, 67.14it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1231/1232 [00:18<00:00, 67.17it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.67it/s, loss=0.0171, v_num=1]\n",
      "Epoch 63: 100%|ââââââââââ| 1232/1232 [00:18<00:00, 65.58it/s, loss=0.0171, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "# use the best hyperparameters\n",
    "model = LSTM(train_data, val_data, **hparams)\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    max_epochs=64,\n",
    "    gpus=-1,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: LearningRateMonitor, ModelCheckpoint, StochasticWeightAveraging\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|ââââââââââ| 139/139 [00:00<00:00, 167.73it/s]\n",
      "ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
      "     Validate metric           DataLoader 0\n",
      "ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
      "        val_loss            0.1795630306005478\n",
      "ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'val_loss': 0.1795630306005478}]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "model = model.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, train_data=train_data, val_data=val_data)\n",
    "\n",
    "model.eval()\n",
    "trainer.validate(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the fit on the train set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f106aa1c910>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = 4\n",
    "\n",
    "x = model.train_data.time_series[None, 1000:1000 + model.hparams.sequence_length + steps * model.hparams.prediction_length]\n",
    "\n",
    "xhat = x[:, :-4 * model.hparams.prediction_length]\n",
    "yhat = model.predict(xhat, steps=steps)\n",
    "yhat = yhat.detach().cpu().numpy()\n",
    "\n",
    "plt.plot(np.arange(x.shape[1]), x[0, :, 0], lw=2, label=\"actual\")\n",
    "plt.plot(np.arange(xhat.shape[1], xhat.shape[1] + yhat.shape[1]), yhat[0, :, 0], lw=1, label=\"predicted\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now on the validation set, first locally:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f106a99ab60>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAB2z0lEQVR4nO2debgkVX2w31PV693vzFxmhlkY1oYB2UEUUFBBVERFRcVd4xIxxi9GE7doEk1MjBrjFjUaRQU1ICKKCoKIoLLKDj0DwwyzMOudu/dSy/n+OHVq6e67zt37vM8zz9zuqq46XVV9fue3CyklBoPBYGherLkegMFgMBjmFiMIDAaDockxgsBgMBiaHCMIDAaDockxgsBgMBianNRcD2Ay+L4vPW9qUU62LZjqZ+cCM96ZxYx3ZjHjnVkmO9502t4L9Iy2fUEJAs+T9PWNTOmzXV0tU/7sXGDGO7OY8c4sZrwzy2TH29PTvmWs7cY0ZDAYDE3OgtIIDAaDYSYZGRliYGDfXA9jXHbtEoyXDNzRsZSWlrYJHc8IAoPBYAgYGupnyZIVZDLZuR7KmNi2hef5o26vViv09e2dsCAwpiGDwWAI8H2XdDoz18M4YNLpDL7vTnh/IwgMBoMhhhBirodwwEz2OxjTkMFgMNTw/p88xO1P9k7qM2ceuoT/vPi4Ce37rW99nUsuuZT29vYx9/viFz/HX//1ByY1jqlgBIHBYDDMEBs3buD+++9l69an6Ok5iHw+z6GHHs62bVu58cZf8YxnHM+f/3wPl1xyKV/84ue45JLXcfvtt7Jlyxbe+c73zNo4jSBYhAxVXD7zm4289LgVPPOQ7rkejsGw4Jjoyn48yuUS6XSGoaFBSqUSH/nIJwD485/v4bzzLmB4eCiM/pFSUi6XEcIinU6xadPj0zKGiWB8BIuQ9/zfA/z6sT2896oH53ooBkNT8/jjG8nnW/A8n0MOWcfVV/+YP//5HtauPYSf/vRqMpkMTz65iRtv/BWOU2Xz5k1ksxl838fzvFkbp9EIFiGP7hqa6yEYDAbgFa94FQDnn3/BqPv8/d9/HIDzzqvf5+STT52ZgdVgNAKDwWBocowgWITY1sIPfzMYDLOHEQSLEM9XzqeUEQgGg2ECGEGwiMnY5vYaDAuF66+/jo0bi9x00w112774xc+N+dl7772bW2+9ZcrnNs7iRYxOLtw5UOa9Vz3I285Yy4vXL5/bQRkMC4TuK59Pqrc44f3dJQX2v+6mxHvf+tbXaW/vYHBwgCef3MRznnMuq1at5pZbbqKtrY2TTz6Vbdu20t/fx2OPPcqRRx7FQw89yPOffz5f//pX6Ok5iPXrj2Pz5k1cf/11PPvZZ3P55d9i+fIVHHzwKnK5HA88cD8DA/2ccsrpU/6uRhAscsqOxxX3bGfL/hKf+GXRCAKDYYLUTupT5ayznoPjOLztba/nU5/6N6644nt0dy+hs7OTbdu28tBDD/DBD36En/70qujc+/eTz+e5+OJXA7Bu3WG8+MUv5be//Q3pdIbOzk52797NU09t5q/+6m+4++47qVarUx6jsR0sYoarHmf/1+1cee/2uR6KwdC03HzzjVx77dXhiv3MM89mZGSYarXKEUccybHHPoNf/vLnPPjgA+Fnuru7KZVKXHPNVRSLj7F8+XKuvvpHnHjiKQBUKuqzz3722VxzzVXcc89dBzRGMV5N6/mE43jSdCgbn9M+d+uo2+76wHOmOqQxaabrOxeY8c4serw7d25hxYpDpu24E60pNFnGK0MNJL5LT0/7PcCoSQlGI5gn7Buu8u83Pc6T+xbOj8dgWIxM5+L47W9/17QLgYkw2e9gBME84TO/2cj/3beDt19531wPxWBoWiwrheNM3dY+X3CcKpY1cRewcRbPE7b0lgAYrEy8mYRhcTNcdfnYLx7jr597GCd2tcz1cJqCtrZOent3AfPbZC7EeK0qBR0dSyZ8vDkTBIVCYQ1wObAcddW/USwWvzhX45lrUvb0JH+NpxL6UmItgsYbzcDf/vRh7t7az22betn4z6PXqjFMHy0tbWF7x9SOO5Ety/C6DpvjUdUz3T6YuTQNucAHisXieuAM4LJCobB+DsezKNBZxaNRdcd2MBnmD4/vNf6iuaT7motpv/F9cz2MWWHOBEGxWHy6WCzeG/w9CDwKrJqr8cw1JWd6Ss664wmCcSINDPOHnraF3zt3oeO3HzzXQ5gV5oWPoFAorANOAu4Yaz/bFnRN0VZq29aUPzsbDJQj30BXV8uUxztYHtvHkGvN0tWem/Rxx2O+X99aFsJ4U7ESIQthvHEWy3jTbUum93v0bYGOVTAJR24jpvv6zrkgKBQKbcDVwPuLxeLAWPt6npyyXWy+xzXHBUFf38iUx9tfcsbcvqd3mMwMaAXz/frWsiDGG/P3eJ4//8cbY0Fc3xiNxtsDlP00w9P4PXq+chKDZ/8z5ePfekDHmez17ekZO4R1TsNHC4VCGiUEflAsFn8yl2OZL7Sk7QP6/LimIXd+R0MYIoRx6s85Ygaih6xy77Qf80CZM0FQKBQE8C3g0WKx+Pm5Gsd840B/+42dxZK/sH8BGGfxQmKaAskMB8JMVF6Yh9Uc5lIjOBN4I/C8QqFwX/DvxXM4nkVBXCOw8WhnhA6G+Vj6B3QwTMU4ixcMCY2gPKbV1DDd6MlazkDf4Hmo6c2Zj6BYLN4GzL8rMscc6DMSFwRvtG/kk+nLOaeiapn3iD6jESwg9Fy0Vuwi/bl18JebR3UySinxpGlGNG34ga/Nn8YET+kn/59HmBITi4y4aSiLepg7GQYggxuGj2Y3XIO979HZH+Ai56GnB7j4W3dy6xP7pu2YmeA+ikr/qPv846+KXPiNOxgymenTQyAAxHRqBL46lvDmXwkLIwgWGa4frTYqpAGwUe+lcUONoOPGv6Ll7v+a/QEucr5wyya29pX59A0bRt1nqOJSnlDeSNByNLh/1hiC4BeP7GbfcHVaBVAzI2ZEIwiO5Y8d2TcXGEGwyIhrBE5g+csI9QBmcJIJZcLc/ulm77Ba7fWONP6xV12fV377Lt7wvXvxx3EaajOfTbCSLPeNe37TnnSaCFbv0ykIhNYIjCAwjIc4ALfJUMXl/u2RU1GHvrWj4o3TwqMS9xHMQ1vlQic9jo1+z3CF3hGHLftL4yb/aUGQ0oJgFI3AjQl34yKYHvRkLaYoCFI776mPDtLH8owgMMwg773qQf7jt0+Qo8I3058LJ5AuMQQkfQTAvAxjW+iM5+zfH9MU+sZJ/os0AnXPhNs4gciJaYHlQNA/PVDm8ju3TlvpkkWP75Hd+LPYazf5/yQQlQG6r34Z1sBT4Xsdv3onqd33qe3TaW6aJowgWEQ8vHMQgFViL+fZ94QmhchZ7CQ0gkbJMndu2c+Xbn1y3OJ1hsaMl9AXn5hrHbtW/xaWfWVN+Nqr1QjccuNzetE59f19/08e4ku/f5L/vGXTJEbfxPRtoeOG98RMQlP3EQhHLbxSvZGfKPvE9WQ3/0a9mIfO4jkvMWFIMp0hxtrJqDWCNC5vv+dC3GVfGPUzl131IABHL2/jvELP9A2mSRhPyXJik3bVS+6c2veoEs5SghChyccWWiMYRRDEAgR0MMCmoNPdHVv2T+4LNCuO6gci3BIy0xau2qeyehfhsZIanEzl1R9GIzBMhK37S/gHsCKPawAA3USCoM3ZS9fP3xjsOfo5tNPTML0kBUGNj0ZPPo66f3U+glEFgWz4N8z39irzB+FV1P/BtQ8naznxSVuU99P6x89EAqD2fgXBGcZZbBiXgbLLxd++i5/ct33Kx2gV5cT/bUKtUHJCPYCl9ZeqHcdYvhqf49QYb+L1Yqt3p0YQhA7KoBaN9Fw25y7laBHYmr3xBUGdSc/4gSZGYK7RgiDSCCbuY0lv/wMt934Z4ShBIFz1uwuDMnwTPmqYJB+79mG+cMsTPLF3OHzvwR0D9I0SlhhHryBbURNHS/B/WxA9hB3UuR/jITfTxxQZZ+IdyzQUagRBVMlyuQuAE6xNifcBNt3ybYoP3MZg2U04nT1pNIIpoa9tYNaJfASTmLTtbHCMQBDoYwWagdbo5mNCmfERzFM8X3LFPdv55SO7ueE9z+LBHQO87cr76GnLcN07nokQJFpOxltUZlATSovWDEJBEKwoax/2BscwzAzx1XttuY9wgvDV/wd5e8CGiv6ZBvdr71CFZz78DwCc/ser2D0UTSxujXAxt3SCaNNQzEksrfSYi6U6gottlVVSn6gO0POV1ex745/U68CJbDQCQ0PGSizaH6z27t2mYsj3DFU5/2t/5FO/TmauxieYSCNQD7cWCJ0iUHsDG2atIyxuVjgQH4VhdOLmoHofQTJ23ZZqgm8hOUlt61P3c6vfkxACAK7RCKaGl9QAhO8gU/mx7flSJh2/wb7W8B4A7GGl0dlDyswrqkPBsY2z2NCAiUy6tiV4l30dL7LuYKDsct3DuxLb45NKWmsEaI1ATSRLhUo20zbMWkfYmLZmw4QY76qN5djVpp/Uznvo+cpqLKlea0GA59B62z9hDWwFYD9tdcevvW9Gy5sg2kcQ1whSuTE1gtY//gtL//ek8LWe4K3SXvW6rCK2rOHd6rX+3ZmEMkMjak3Fo/Hh9JW8O3Vdw21OrOFMOigp0SrKDMlcqBFowVDrENMkBIGZQKZE7WWTUnLlvdt5cIcSwvHkL3cUjcDuUz6BVFDwLC8ijaDl/m9w6OPfAsBqIHaMAJ8ienL2YlpZKjemGcfe9xhWORaeG5j0rPJ+pJ1FVIOFV+D8F84Q0kqbqCFDYyby49XeACfm1ik5Hr0j6uFz/LhGoCaQFsoMk6PDCkxE2sSgoxlqViZjrVYNE6P2qj349CCf/+0TvO3K+4Dk5F+nEWjTQjC5RAI80ggApF55NhAEJnx0isiaOkC+i7RzY1cfDYIuUrsfoP2GyxBe4Owv70dm2rAqKsFTdyQT1eFAyzCCwNCAiQgC/QPPEtmE3/T9e7nom3eyf6SaMA2lAtNQG2WGZY62YEUZrixDZ3G9RtDNAOdYfzaCYJrY0Z8M+YxfV6dWFQzME9qU0BHkg+SpIIUVOhurqAlIUF8rymgEU6Q2tNN3gkl7dHu+DARB5skbyG28Nor6qg4i022IqhIE2kQk3BLYuUT013zBCIJ5wETMMPoHrlf7AJt7S1Rcnw27h5OmoUAQ5EWFEXII38ETafKBEImcxQ4//vN27gsc0a7nc6l9M9/JfNZMKFMkbpOXUiYCAaSUCWdxy8g20ttuD1+HeQTBhN8h1H1qERV8Oxea9PxA+5uIachY+CZI2CsgMg3JVL6hIMg98G3af/2XYKky72E5gPD+jeBn2iPTUFUHaZSQ6XxjjaA6jDW4Yzq/0aQwgmAeMDGNQP/461eBI443immowjA5dQ47R44KHlZoGqpWynz25id4x4/uD84hw+QzIwimRnyR78lkbaGS4yc0gos3fYSua18T+4CeSNTEoTPEWyjj2Tncinr9wLbAGWl8BNOGCJO+dAivA6lcwwifXPFqco9fF3WLCzOGo8xwmW0PNQLLUf/jlpS5qYFG0H7rR1l6+enT+I0mhxEE84DJmIYaZfzuLzmJ5KQwakhUGJEqycW1c+Fr6ZSQqRyeG5mZ7H1FrMHt4eTiew5W35NT/UpNS20I7lAlEgRV10+ag7SJLiDUCIIww3atEVDBs3MMDqoJReeJxDWCM60H+XL6i4m6Q4ZJEBabC+o6ee7opqHayqSB2hWagpxhZLo9FC6hRuCMhD4Cu3cjIjABwuiVZWcLIwjmARMxDelEoYYaQdXDcX1SuDyefQPtIppgRlCCwLPz5KlQJqNU1FQeW1b529SPeK19M0t++HyOuvnN4eTS8tDlLP3B2ewarEzHV2wa4hOxJ2XCd/O+nzzI9+/eFr7eX1Vi/cl9ybBCHWbYQQlH2qSFh2vnSPvqvoatK2OC4MXWnVxo32F8O1NFO4t1SLVUgqCRRhCu/IOSH+HKXyeSOSPITBTaK6qD+OlWdb8CQbDkynPp/MVbotPbSnO39z5C+w2XTe93mwBGEMwDxtMIPF/GNALJEWIba0SUR+B4Po7v00M/KeFzmIhsjcNSVTz07Bx5qpRkBguJTLVg+S7vTV3LZ9L/A0Cq2hcKmg6p/AbXPPD09H3RJiB+Lz1fJl4/umsosa+OANvaF1SrrNEIWihTChzDrpUn7auJR5v+GpuGkq+NWJggOjoo1plsVI1AZ4DrKqPaF1DSYaLKNKQR1SFItajTpPJRvsFw9BvWjufs5huV4zl+nlnACIJ5wHiCwI1NKALJb7If4hvpz4fbq65P1ZMsFyo6oU1EkSraR+DaLaSFRynQEGS6BTuwhw4EwsLyqqFGoJuhjNc8xZAkfi9zm66vzxWI4Qc/PymlcjJ7VbUKDa59XlTC++XaOVKemnh05FgqFjighcLAUD8PP3r/NH6jJqGmNWWYR9Cg+mioEQS+NquiBEGYSCY9ZDqmETiDyLQWBLloghcqosjq3wKp4HcZlKpO7XmInv8+bDq/4ZgYQTAPGM805Pp+nY/Aj926qidxPZ+1QtkcWylRliqiQQsCP1A99Qqz302FK9Bq0OQeZDih2MH/E2uybtBoQdBCmZU3v2fU0tGgrjbA52/ZxIXfuIOqU0UGK0cg0OACQWBlSXnJqrK6F7U+GsCJu67mnJtfEr1rwoYmhrbna83Ac4I8Aj/clnvkSlK77guFQ1hETvsGKgNIW0/oufDQojoUCQIrHUUZCYuOGy5j6ffPRAo7+JwSBGJElakQw7vp+crqmfjGCYwgmAeM599zPFkXNaRXimq70gi0c7GVMkOoB2pERlFDAOXgcxv7o3yDcGUpZfi37m420axng5p09fVaKpRpLeP0jbq/HwiCHf1ldg9V6RsaDicMXwpyRBqBY2VDH4EuIqh9BXFM+fAp0kgjsFJIKxW+1/7bD9J652ejxDG3pDKFtUZQHUKmW4HI1ANaEKj3sewo7BQR5hhoZEaZlLSjWfsddEXTmcIIgnnAeKYhX8o6jcCOOY2rno/j+eQCk0GbUIlkAENh+KgSDCWpHtCyzFCRykatJ3/Pl2FEio488sYwbRiSxO9jD4EgcAdH3V/WTNuq0JkSBEPkyItqqMFVRC7U1looU5U22ZggiDTF5DGN83iCyKQgQLpKCFipRL0hlVsQ+HLcMjLbFeZ9JJzEdrRQE8hQwCvhkqYenYsQnN/V/gd1bKs6+nM0HRhBMA+orRhZi+fLMGpICG26iR7O3hGH3z2+L7Qdt1EilVMP5LnrD1HHCFROHUVUIR06K7XzUUpJTqhj6ElGSOMjmCjxSVebb7LOwKj71911z1EJR8AIOVqohKahSiAQhmVWCXryodCGyEcQCRf1eqjijlnd1hCgE8riAsFKIUWqpjaQSEzWfrYjqt3lxDUCdd98/VoLAmGDHdMIapzRwk82yLFK+8JzzSRGEMwDJhI1pP0I+gefimkEv924l5s37g0n8RZRIdPSAcAphyxVOwV2yXJMEFQDQRAJFRlqFXoiszwTPjpR4r4erbFZ/vg+gh72c1v2fYEgUBPGsFQJgFpw6/s2TJ7WIJrIwg/vnZ7+tXanhYQvYbhi/DzjIj11P8LMYket3K1UpC2AEhjBb0m4JWS2A+GocGzd7xgi01D4WkcNWXZSIwgn+ODZ0WVGdMlq/b8RBIuf2paFtbgxjcAKBUGDjMeYqcC11UrEb1sJRBNFJXAiO6TCVWZK+LFjqAdRl6OQjhEEEyXeFEb7coTnsDl3KceILXX7a0HwPPs+Vou92F4pnDBGyJIRHuXgHmkT0YjM0iIqVGWaChm6GKKVaJIIFwOUOVU8BkB/2Wh14+L7qohc6CxWGgFWKlmcMRZFFAmC4TBKKFz5a6dx7fsi5iMQInI4a40k7G6mBcBI4v2ZwgiCOebrt2/m3T9+YMx9PF/iBQ4qvdK0GySW5WIF6aqpQBBku4BIk3BQ0Qm+FKG/IE426GucCype+mNEvRiSNGoOpM0KR4mtdfvre/Jv6W8CkPZGQtPCcI2zvyq0z0e975DiabmEm7J/y08z/xAeSy8GjrGe4qrsPwGSgfL8a4Qy75CeKh0db1ofOIvj5hvhe9Gk7bvIVAtCevgZbQIaRSPQJiIrjYybhvTvKzQJBRN/mKWs+x/P7O/QCII55n/+9NS4+3hShg6qdG2kT4wcVYYDm3LJCpxWQViajkLoCaJZAA6zdiY+L5Chb6AlqFQqHSMI4gyUHXYONL4mcROf1tys4L7Fczs02jdzrfdsADJ+KfIR6PsYaAJlS00ke2UnoATB4dbTdIlhjrS2xwSBum8dROUpdKlywxhIT2kEMV+BFJb6/cjYost3w0lbCiua8NPJCT/UCIIooNCJLCxS/ZvDw4XZyWGdIi0IokJ1wIwnlxlBsADwfZBeUhDYooFGIKr0BV2rRmzlI9CFsXQeQTdK5TzF2lD3eUHkJA5NQ8ZHkOCS79zDS4PS37XENYLIR1AJXtcLbm3eWymUQzDljvC7zWoiGCEZ7jtiqwnlWGszEGl2Gh2arrXCJUE3uhYq9JeMRjAuvqcmdW368T312xEW4EcCwi1FUUTCCs080co/8AUEq/7acFLhVXGXHh2coxqt9GsKDsYjkfTnZhIjCBYAni/BcynLdCgI4rzWvpnNuUvJUWVAqgdvyApWIqkcXstB9C4/U70fTDBPyIMTx/ClIC081ltbcKVFPmx0bzSCOPuG1Q/yib31NtukIFCTRdZTP+xsg5h/rdW1Bdc6J0s8NagEiA7/dQMBriO8+oP765DiCvd5AFRlJBS0j6AbXaDOQZpCE+MTmoa88DXCUv+kD9qWX94fNqQBK0wciwRBUhOPMorV/8Ir42eVVifcQOuORSKFPoEaJ7GY4QVZavxdZo5CofBt4EJgd7FYPG4uxzKfcaUE6TJCliUieEBiP+7DhKoHlKPKAOqBGyRYiaRy9L71XvYFrRItJEeVv4uHxRP2G8NjeFhYwcQ0HMSwq5MbjUATLxdhW/WpW14DjWBwsA9I+m86GKZDjISmIS10bSF5tvUwEIX5lmQWBLiBBvCYXMMR7OBUawN3+Gpl6WOFzmndzWyJUIIgKxzTk2AiaGdxWFnUB2EjhUBIP5yIrXIv0s6o10Lg57qBaOVvDavfoo4MCjPFg/Bt3HIYIBRO7sKK9aJI+gjCCrWLXCP4DnDBHI9h3qM1gnibyniDmj6pViFtosQ+qUxCAyIoehXYKvW8ZeNTJY1XY1pIi+h4T8ulsfaIWnWtqlR36dN20wcQ5b7p+noLhuFqdI0aRXrFK49q050VrOhCwQp8LPV9bsv+dajdtYpI2D4tl6hzBZrbYKAZ7Cup45XjGeXB8yAR4bHaKFGSGbpFXCMwjEugEYQrc+mpsg/CAikRfhWJwKr0hxqB8Cp1mkB1lfL3aJOs9vlozUB41TBXQbglpQ1IP9bUZjj5f6gRLGJncbFYvBXoncsxLAQyg1uw/EqNIHBZyT6ebT3ECqEu4Vqxm16pBMAJh60BoponerXaqIx1LcOoGPYhmcP2Kkgpw4lflPaRf+xHpLfdNm3fb6EQN/1UGwqCeo0g8rlEk72+XynhUZFpWoh+5J93Xw1EzuIdgWDYsEN9RteQ+pn3LKrB3z4iXBi0iRL76GApsd4FRhKMi/C9oBFNYMKTHlgx05BXbVg+InxPC4SWHrVBC4JY1dHoQzF/kZ1RGoGe8J0R1Za0mgwfnWmNYE5NQ5PFtgVdXS3j79jws9aUPztTTLQg2Nm/eRF70y/Bk1aYEJDG5UeZf2KttSfcb5kY4ORjjoSNN3Hk4YfCb6FridIQOgaD8LTYrPAD9/m8PnVT4lwP+utwsckIj17ZQQaHbGsWfeW6pKqw2JrxaQmu5y0b9rCs5HLcyo5JX4O5YirPQ9mK1k2pbLru87nBWKOfGkEQNw1p30AajxGydIuhMD9gq1QTiXYW67wPXVdI5xV0MhwmBEpE6Hhupcx+2RZqBFmq5Fsys/7sz8ff21gIfOx8K7bl0tXVgp0SpNpasW2b9vYMWDYi2wbOEFY6KiiX71SmoWx7FwCtXer/tk61IMu1dyTeT2fSEF/cCwF2hlSQeJjyS5DrxHaVRpCWagHRkpHkYtdzuq/vghIEnifp65taYkVXV8uUPztTTKYOzHJ3By42vhRYQpLGpUsM1+1XspQjaqBi41+2DYLvvLZVTShWbDJ7QB7G60kKgixO6JAcljlWiz3sv/ELeMecxxJgeOcmOoHS4ADlvhE27RvmHd+7h5Ql+OP/O3syX39OmcrzsD8WNtrbX6r7fH9/lNilNa8wLyNmGtJmoxTK79PNEMNkaQkaBz2/8lnOsVQp6YpvBfv6XOedwc+8Z/MXqV/yXPsBbvJPCo+pM4nbRYmKzPAMa5N6X7gMD1dm/dmfj7+3sVjqOTgiD+URBvpG6KhUqIy4tPgwOBA0Ckq1kAI80ur/jkMYdtLq9+BlaAMGy4JuYHDYYQlQqvi0AUMjDq3LjqO05gXk7/ufhClG2hn88hAW4JeHIN2ONaIqCXulQSygPDTESOx6Tvb69vS0j7l9rn0ETc1k+st6rpMwDWWFG0amaKchgJNNOq802rm5rjum1sa2X+Geq46LgxucZ5A8r7dvYt19/xqGr1kjQc31IIpiz5B6vxmKm8XvV6Pv28g0VJupHd+WwgsTxvT/LjZPyFXou6OLyA3Qwl857+MBeTgAT/grEz4CrWW0MxL6GUDdz8V/Z6YBz0Gm26IwTekhQ9OQRHhRifAwIcyv1puGtAko7GesfXGCvtf8ikrhVVE9I42VVtnJdhbhDKvG9zpKyeQRLH5G6y/bRX2lQYnADbsEgCPtcJJJhCbqBzLWKk/jdRxC/5JoFRmvfqmdxxnhhjHqQzJPi06E0lETwUpFh9PZoj56ZrESn+gbCfH4/dQTcyMfgRYEmUAjAFX7SaGupwx+mmnhcUb5S9zon5I4l0MqJgggHfQmyIsqf/DXh/v9Y+o7pifBRPCDOk86Si6MGrJUMqbnRGGjulm950YCIKNMQFoQSC0ILDv5Gmq6ngkVheQMq6ZE7kiYhAazFz46p4KgUChcCfxR/VnYVigU3j6X45ltGk0mp4rHuC/3rkQhOI2LFdqIHVLYQSXSuCBI2cEtjZXB1fS+8XY2H/u+uvffUP0wn3dfBSgbsw5V7KUjzFC9/ymlCdRqBON9n8VEXBA0qugZ7+9shZN9vWlIxDSCsANZjZVWH92VNjtZSm2nAR+LZR1a2ItEFNkgke34blkwGsFE8NxAIwiea+mp1byw6Lz2Ndj9T0ZO4kAQhMID8PXCq67EtKDvZT/CXREJcp0sBkHkUCoXNq9Rpa07Y/uOKE1khsO459RHUCwWXzeX559rGk2cOsywlTIDtIa234xwcUkxQo5WKuFkDY0blIxGxo5kv9YIbvOfAaiksg4xEgqbdWJnGFb6nds3cE4GLN05KfjBxFfBjudjW8mw1MVEbT/iWj51w8bwbxsfV1qhj6CRRpASPiU/6DdAcgLRBQHvlkc1HguCd519FNygHIdpN1plDgWtRzfKNZRkNswjENVBNaks4ns0ZYKoIKEzfHVCWRAymn76rjqNAN+NmYaCBE6rRqBbNs7qMxPv+W2rAAt7aLvaJ92qwlW16SmmEeCWlIAwpqHFSyM7c1dQAqKVMksYCE0MLcFKvRwUitMRI54U4WQDsGPps9n3lntGPWcmFd3y2izlfSj1tjfIRfixd060b2B6yG6+AYg0AidWcbPsLu4mNuP5COLofA1d/ykeNaTrEFWlHbYJLdcUAPyF90z+1nlXaCKqGws2VlppE/m0zZFLIg1QF6zbLzrJi0qoESz75jHkHv7eBL5pE+K7KuZfh4/6nprUA3u+VemPykdr05DvhKv3qCGNFuhWYt84fRddQe/rbg5fh9nH2q8XmI5kKoflDKsS14vZNNTsNFpV6km9RZS5N/duTrCeAJRgcGTMRxAIAhfVqcrRZQaEhd+6fNRzpmMawa+90/i0c2n4+oPOu/g75x1hLoIucAaQqWlQo+Oc44lV45XTXujE71dfyaG4eyix/Q2nRr1llSBIkcVhiHzCWazNRi6pUBPQxeU0/bRxlffc0ceCFU0yUmLF7o8+5n7RSQuVRGaxPbRjIl+1+dBmnngegbCjIInhnaFGkN4ZLLSEhcx2su8Nt0WlJawU+y/+KX77quDADabYdAtkomCOUKvIq0CPVG9RHT5YbMl0i3EWL2YarSq1vb89qDG/LGh52CIqiWxgN5j4PWwyOOGP/7S13WOeMxsTBL108E3vwvD1Lf6J/Mg7lwuPUz0MWlsjh3OrSDbGsIMKinGNoNJEGsH/3rGVN3zvXh7eGTn28+nAwWsLbOFTIU0GhyGZJycqHCueJEs11AhcbIakLi6XFAStGZuufJrXnryKRuyRXegZXvjVRKlkvSjoE51BslrsOfNrIlYMiiAqSHgOorxflXqwrFAQqMb00T3qfc0N7L/kVwD4neuQqaDaqJ3DXXlqVAXQGn2KdQ46kYHzvhyYisDPBzkkJ7wjsZ/MtGEN75qe7zkKRhDMIWMJguVCNbXuDHIFWinjYId2/fZWpf67WGRxwuqiqQY1cOKk7fGjfDKrVWTRQd1d4XvtjITt9waf+xlERQkoJ+YjWPSCoIGD+E+bo8R4vdkSAhuPqkyTFQ6DgUbwi+xHeW/qp6GPwMUKhbtX81M858hl3PCXZ3DS6k5q+e8TruWg13ydqKuVE5ZGVsdVx+y3usiLConHrKY1oiHAc4N+xC7LvvUM0rvvU85iPxIE2hFcOfzFeMvW43UfHn1eh43atc7i0afYvlf/nMpRL8fPq3Df1J4HgVgTmwBp58g8fQczWTTKCII5ZCxBsDQoI6ydjPkaB3FXSfUx6BAlcsKhw++b0DnbsuPHB7jBCkU3QwFVIVMX2PK6Dw8jH+IaQaOyC4uJeAcyTTZV73gVxE1DVYZlPvQRHCp2hn4fP/bz0/WiQqRECNHwB3rm8ceyZvlBkUYgvbCSJUQRSH2ikzzVRNSQkEYQNEIEpiF7JFp5S2GHzmOr2h9qBLrZUxyZaWPPZdvq35+AYz4sWBcEYmihUjruTQC4y0+guvacSMuYAYwgmEMa+wjUhLE0MAnp3sFp4SHsND1t6mF0lxSmdM62bIrPvfxYziv0jLqPXvm7IlKFW0UZGQgCP9cd8xHEBMEi1wjcBiuyuAImw/cEVuAszuJQJhMmhvWIPjIx5/45XerH/y/upbyk8i/h+/qyNvrtR1pf0LDeziZ62upAgj1+B3kqyTyCUXJXmh6vWrcSx7JD27xwy+HvYlL9gxs4i2sJu9I96yPqdeA/8HNKU/BaV9D/0u9P/JxTwAiCOWQsjaBDqPj9cw+JilWNeFa4AnUPOiHxuTs7Xjjh8z7n8KW881mHhK/POWJpcocgjM2P2TfbGYlK7ua6VHVEKRPho4vZNHT7pl5+9/jeuvcbCXMhVEmISuAsdrFDH4CNH4YEA6TaDgKgRI6H5brwfT15iwaSIBXcF2flaZTWX4q00omoEt3LYK+Xr+9fUZvValD4brIwHChncWzSly3qXoWO4XEYOeEvqK4Z3eEfon0PQUi2H5wnCled+XDfBVVraDGxc6DME3vqawVFzmIlCI7qskCFG6uic9oWXGPr/cnKD3LYJZ+f8PnXLW3hm685gYPas3S3pPnT5v186GePACp6Ye9b7sW/Ieql3CZKyJwSGNLOQpAOH9cCFqtpyPUl77/moVG3AWzYPcS3grajQhBpBMINEgGzdFDCR4R5HwJJ/7JTWbnzxrrjRv6G+nOmAj+PzHUzdO6/k930K3BUpUvhjoR9jVvbOrEGZdK0bJzFjfEcSOUSb0lhR9VIAT/fzb433RkuiMZj+KxPTuzcwTnKx70JZ+XpUXayFgSzkPdhNII54uJv38U/3xC1i8zg8P7UVWRxqEo71Ah0XXKAVDrD4PO+wMAL/rNOEKTSmURG4kQ4cXUnB3fmyKdtDlkSrYaEEMjWg1Q99oA2SqGq+h+/20op24M1shvHX/ymocHy6Al7OmT287c8Eb4nENj4YeVQh1RYPtpCko2t0rOpxnbf1V258Fi11AYE+FmV9zF05j8w/MwP8a03nwXAoSsOQiC5sbiHkaCXQv6xH43xTZsYz0FaycituogfYeG3HwzpGs3hAAmTxYSFt2x9+Lur7XY2kxhBMEc4NY7H1WIP70/9hLyoMEhLqBHEBYGDjbPmLFW4yk/GFWdTB3YrO3JRtIO2RgzaQaN0adMqyty7V2348UO97Bh0QfqJrl3xxi2LiWoDJ7FGm4bu2dofvmcJVT5CJ4u5pEJHfwYnzBVJ2xbLWmujTOD1p6zmLc9cq47fwC9RKwj0hFE+7g2MnPo+1i1rp/fS3+FaWSwkD+8c5Ed/3h77gCk6UYfvJMJDgboJWM7QhFw++tX0v+S70Ru6l4GuY2TNvOHGCII5JoXLv6S+GU78nQwzJPO0hxrBSLiajPemra31c6CCoDMXPWx6ohmSOb7nvoANcjXtlPjVNovzK/+Gj6Wcn76XmCT7ShMvdbGQaFRXSDNahrGFHxaUU4mA6pq2UcKV6l61ZWy1wozRmrF5/zmHkU+re93I71LbJlPGkpM0XvfhIKIWlj+8d3v8E6N+n6bFd+pDP2sn4Ak4fqeCzHZSXff8uvOEpSZm6LxxjCCYYw4XO7g09dswX6BLDDFEPiz2JtyRcEKp+rEJICYIemnnZc9YcUDjSNsWHz//KN73nEND7aDienzcfRuDtNBGiSppNkjV+czDwvU8vn93FDLXO7I4BcFYxfQaCQKVR+AzKJXT3SEV5X+IkVBTAKge9mL2vHND4rNxSk69llXrQJaj2qxFmLzWO+KE3eqQi9OEd0B4bl3BOCks9r7lXvov+IZ6YxYmZCAUQL4WBEYjWJzEV5i6yfhSVN5AF0oQtMd8BDripBLTCCpHvJTKuhcA0Lr2FJa01Ki1U+CiZ6zgjaetCV+XHd0Qvcwaa09o8wZVJnnzvmSJhf0jM5sGP1eMVVeoUW6BLyW28BkIqoCeum5ZWE22g5FYgTmp7HCxsMXahL9zjljKkpY0y9pGv7910S4ay0p0pAsdxcY0FOGWg17cXmiC0WGiCBvZehBeV5A4NkuCIPQRaNOQ8REsTuKTR2vQty6eSTwo83QQ9TAtB/1r46ah8jPezMBLvqP2maFIEF1EToc7VmNBZj6CXI2jc7FqBJM1DXlSYuOHVUDXLetgVbu6dlnhJq5jLbX2/45cml+86wz+6zUnjj7AUVb4ksg0pN7w8BH84O6tox+rybAHA43Wc0KfQFj8TU/A2mQ0CxMyEPkIdOKaiRpanMQnj5YaQdBtlZCZDlWaQOaVRiCCBC85ygPhz8wEPFJVAmCjVJnG8cxmH1E3AS1WQTC2aahBE3svEAREYYB2rF9AJqNNNNFxD+lW+zbq+5yyBCeu6eLUtV38xRlrJz5wEZmGQCKkKo39tds2TfwYixxdKkUgY7Z5XUAueB1oCHK2TEN2mj3vKEbjMBrB4iQ+ebQFmcM9og8AWzpUbGUqGKAFUR2iGmT4xovOxREzVD9Ghxxu9FVVzfha1Udw/I2v5DL7p+F7xd1DizKpbKxk3EYageNLbLzQR4CVStyj9tZ65+6/v2w9rz15FR89/8iG50nbFl979fG868x19RtH0QiEsEJBYCHxEYmyFoaaoAstCHRZaK0h1JSfnhUyrZFJyAiCxUkjjaCN6IE8bKUqIy2ynQivQjXQCJxRBMFMFRLT4aA3+ScD8LiMIlx0nfxjrKfCqpsAm3sXTsPyidKotES4rYGPwPOVRjBMsPK30kmtrYHz77ClrXzg3MMTYbwTxR54qvEGIdARQjY+EguJSJqLmpykIAgS9bRpSN8new4EQex8E6lXdKAYQTBLfPfOrXzsF4/iS5nIIdA+gryISgSsXaFSzJcuU/9rQeDOkSB4WK7jyPLlQVN1ha6WuVd2cEh3S9jwpm8RmocmGzUEKnw0FNzSm9Gqn+nd9zd8Xwo7phH4SKHCWLVmZw1sBWfxCe5J0aBuUJTIpSfi2Sv1kEBHh9VkPM8ERhDMEl/+/ZP8+rE9PL5nOGEa0s3hs7HGJWEUSa4LiKqAOqM4GcUM+QjiuQW15/ZjhiIh4PkFJbTi4aSLBX8MQVDcPcS3/1S/Ik/hR6WlpT+K+W6mo3dEQhA4vrpvAskDOwZY+r1n0fb7j8/wGOYn2Q3XsOzrR0U9imOEeRl1zuLZ1gjU+f1JVgyYCkYQzALxWHBfyoQ5QXeuysX6DmvVVD8AjqU0gtqa9dFBZ2a1+V+vfAYXHHMQLzrmoLptOi7exse2RFhq4U9b9i+6JvaNsns12/vLfO32zXXvW/gxe7yYMYc+gFNTgDAkllBmB4JJBsLh7Vfep3Zp0lDS9M57EO5IXWImRBpBaKMPfQQzVwa6IVojaVD2eroxgmAWiAuCiusn6vPo2vTxnrZ+PijuFjwA3jiCoK587jSxfkU7//zio+lpy9Zt84PsWDtYY8a/U9ldXKUmxutP3AgbPzLlCYGYiyQuIcI8Ai2YJCRyC+I5CJ4vF3270ZBYeelawqJyutaQFgCzfQ8DQdCo/8F0YwTBLBAvxuZ4Es9rIAhEFVeolYffqrKEowdy9LjzfW+5e8ZrldsNnhJtGrLwSdkCL2buWmzF58bKIxiNlPCSGgGx6JNpZ5SVqmVj43Oy2JDQCBImqVhZ6o9f/xgv+fodi7ZUSJxQADQSBPllwU7J350ONZ01hGDg/K/Nio/AlKGeBeL1eL78+ycTE0tKBFUhqeBYOVKeEzaf16UD7MBGmamtLU8kNGaS2rIHcdLCZU1Xjnws2mWxhZBOxdRl4auy4Yk3U8pENM3mGGfl6VgjjXraClLC5yfZT3Jq+Wt4qExjCxlFDvkuLX/6N1L7HuXG4tsAuHnjXi4+fuW0jnG+ofs3NNQIAo281iegmzHNJpUjXzor5zEawSwQXyE/vHOQR3dFD1RkGnJwLKWmhw1gdAu7lJpks8zNSq22yBnEBViVU6zH+cDJsfIXTSgIbDw+mPohosYmD0RhiYFGkNq/oeExpsrwWf9A75vvqt8Qm8jaxEhYLFAgw+AE4ZbIPnE92c2/Cfe9fVNv3aEWHYEm1LDbWCrPnnc/UecTmKmgjPmAEQSzwFgNW7QgyAqHTE7HL6sJQ6aUbf4Zq5VgOGddfSLSbGA36pIVjLuFCm/e8C56rn0thy9TvorF1qBmjCrUIQfRx2Wpn9HOCJeesoqurMWHztPtRIPrV2vim2FHrYyZjJYwmHAWa5+UcEbqJsPH98z+ynfWCcqyxMu8a6SdBrveLxbmFyxCjGloFhjLAZfCwxVpUtIhpUsPCIF74ZeorjoHgPZsmpET3sH6494wJ6lADfvmBoLgHFvFsIv+rWS7lFagi9UtFiaiESwTQY9pKixrzbCmM0P3ch32pzWCFOWjXkF17XPp+M37Z9z5GBfILaISCgJBFKWW3VzfHW3nYEUlxTVqj7bQ8V2soZ1hP49G5p5GwRf73vhH/LaD695fLBiNYBYYy1SSwsO1lACIN8aQJ7weYhEdw2d9Ar/r0Jkb5Bg0cpamqI8MeqXzcw4VTy+6qKGJCII2oVbVOi9ExKpZ+rqKZK6bwfO+RKXwqmCfmUsyg+Rz10oJX4pAEEhyYvRKsb5cfJFfmtyjP2Lp985QZadRGlEtXschde/5HWtmpfjbXGE0glmgthtZnFAQeIMN1VHF3MZ6N5oHGwmCtw9/nePTR/GU84JZGNXsMVbUUBqXV9i/Z4/sAqJMcXwPhEXv62/F61hLdd0Lon4AgLv06NHLR08TSUFQDsJHA0FAlVLnkeT7N8Y+oYJLYfFWqg7NYL4WBEnT0N633Itsrc+bWewYjWAWqDQwDZ0iilyf+TApPIaDOv+jhRfOddJPo8xaLQi2p5LVMDO4lBs0U1nIjJVH8CzrYf49/U06UBNKFkcJTumBsPG6DgMrhd+xBtnSE35u/6t+Tt/L/29Gxx3vWqdNQ5Gz2Ak1Uc351t0zOp75gNYAtOO3VhA0oxAAIwhmBaeBaehs+0HWW1tICY+RoN+Adg7PNxqtiHVBtR0ZZa6S3er/NF7oI9g5UOb+7bMcez0DjFVi4mCxD4BuoWzNraLM3971LKzKwNimhFRuxuPDn33okvDvNkqhRmDjkxNVHCub6If7LOuR8O9FqxE4gU9gFI2gWTGCYBZoFEVTleoHmMajuyOoQT/qxDC3v8psqn5C2y9VGz1fJ90EERVp3NC+/KGfPcJf/PB+NuxeuFEo1tDTZCp7R93ehfpuWhBop7FV2jMrnaXG4pmHRC0sW0Q5cBbDH3Lv49nWQ1RFjv2vvp7e19zAEysu5CE/8kHJRdbXOLvxZ3T9+MWhczjMI3CG5/w+zQeMIJgFGgkCO4w390JbsbTnpyA498hlde/pNoxhe7+gUFcKN9QIdL7Eo7sGZ2OYM0LnTy/hlfe9KfaOJBPL59AtRTu1QCD2XefRBNMW+Ah0tvPJYiPDqW68Zevxlq3HkXaYG7JW7KL9sR/O5XCnncwT15Pe80DoIxBuGWlnlUYwqm+ueTCCYBaIZxZfZN3OO+yfkxZKNU3j4ad0A5PGE8fMlSaYGIcubeG7rz8p8Z6eUELTQhAZkxYeJcdL+Alqm60vJFL9T9JW3RO+fpl1Oxtybw5fpwNfidYI9P/AvIoyebl9G14sr2C56GPDcB4pJd/8wxY29Tmkg8z1t9q/4qDbPzxXQ50RwggtXWPIGUamWxDOyLw1yc4mowqCQqEw4xFFhULhgkKhUCwUCo8XCoW/n+nzzTZSSr74u0388pEo/f/v0j/ko+krwqqjaVycnEppd3WT7Bh9F11B6fi3zc6Ax2D9ivbE67ABohZSgUawWuylWhnhxmI0eX73zq3IBWR03jtc5d0/vp+fPbizbtuR1vbEaz15ahOR/h9mp+n4RFkSCChdpqRLDLJxf5Xv3LmVb/xxC7uGvfC76AioxYUSgsKLEulkqgXhO4mw7WZlLI3gzpk8caFQsIGvAC8C1gOvKxQK62fynLPNY7uH+P7d23h01xCHi+0sYYBhqcw/rUFz+nYxgheUm64c/hL2/OXmxDGcNc9B6tonc0zDBKMajQCgrX8Dl98VNUh/an+JjXsWjlPuFw/v4p6t/fzzDaoUhF/bqzmGnjxDE9E80wh6L/0djy67AFBJZNqs1U4JR6b46m2bAaiSCr9LWBqjUfmFBUo42etoIXckShwzpqExBcFM6/OnA48Xi8VNxWKxCvwQeNkMn3NWiWfY3pD5EP+R/u/QpNIeJCC1U8LLBBmodmbMSqNzzZtOU72Ll7dn+Q/3Nby7+v5wvDImCFqHN9d9du9wlSf2LgxhsHc4mWwlY6YtWfOT0WG0HYxQkSm6YxrBfPAReN2H49rKB2XjhRpBTjiJZkMudvhddHKcVVpENYeCe6E1Aoj1Jp7hfI6FwFizTk+hUPib0TYWi8XPH+C5VwFbY6+3Ac8c6wO2Lejqmlrtfdu2pvzZqdLaF1U2tIXkoKBBPahwPlDx3ZkOteLvOGg5ZNUY52K84/GhFx3Di44/mL1DVd5zZYVtsoe3Zm4AwOp9PNzvgqGruX3FeWzujVaUf/2ThwD41ptO4TlH9jDXjHV9u2v6L8RNPL6s0QhEpBEM0EqXiIRdZ3dbeD9ncrzjsS/IJzjM2okbq4gaFwQOKTL6u6C0m47UMCyg39tY2Dl1T7VDHMAOouGsvIram0/jHY/pvr5jCQIbaGPmNYMJ43mSvr6p9Vjt6mqZ8menSm/N+exYpaBWEQmJEU+prX2lFJTUZ+ZivBPh0I4sO/ZFk92S4U0AiE03A3C1dzY7rTU4gbM4bYtEZvUNDz7N8T1zX7xrrOtbqSSrTMbNQbLONBRpBFvkcnpiwr5voArp6bmHB/I8JPpfiFjfiLggkCnyosKx4kneklLCfWT3Nqr5o2Z9vDNBW9UnD3iVUqjTWTvuBcC1W8jAvBrveEz2+vb0tI+5fSxB8HSxWPynCZ9p8mwH1sRerw7eWzRUa0pLqC5RaiJpJ2Z/XWDOqrir4Mi+WwFw3/JrrB+8kiedFeSdkdA5nLEtHC9aha3qmv9quC/htuz7eFv1g0D95B9H29XzosoWfzlHWzEldx74CIBYu/okSY3A5jL7V7w3dS0AXroNUWMaenqgzL1b+3nuEUtpy85fE2YjdFc24dZPnn6mY7aHM++YSx/BXcCRhULh0EKhkAFeC/xshs85q9R26oprBG0xQbC0dWE5q6zYo/Hroz4NgFx1Gnve8Sgj5OiSg9hSraozNe3NuvNzGwo7EXypop+ea6nKqnFBYAUrat13II2LI9WEv0t2Jw80D3wEwKi/ZEfGfQQpWkQlfN136EVYpSiRbudAmYu+eSef/FWRc7/8B+7btsAyxnUmcbU+gWwxl5eeKGMJgufP5ImLxaILvBf4NfAo8ONisfjwTJ5ztql6Pp9JfYPnWUoFVV2h1K+yTZSo2sEDOG+MbxMjnhawtedcBl7wxfB1CpfXp27i7/r/GVCmoTgLIWPVDdpurhD7gWSPX60BaKdrGo8SSUEerrTniSDY1X4cAA/7yaqacY2gWmMccFpWJgTBrU8ktYPfbNjDgkI3oqkOImSyFpbMjG02aQZGFQTFYnHGQwaKxeL1xWLxqGKxeHixWPz0TJ9vtqm6Pq9N3cK51n1AZE8GFT7qBdEcC63Oebx1pZ3OUCm8ElDyTEdDnezcwzvt6+rMYwshncANxmwH9yte9E8LgHOs+/hc+mukccO6S1rj22fpnrfzQ8JvWH4h68pX8JLqvybej0/+R4ikVdbLtCHKfVj9WwCouB7HiycoZlUyXetCMw01qC3ktagCc87BZ8zJmOYTJrN4BtGToF4xZoQTmhkkgnxVrbjc5SfV5Q/MZ+LzW9z0I4QgT2Re+Ej6Sk5b25X47EIQBLokiKXtyvi83f4F309/mpbg+11g38Ur7d8nigb+zj8eoE5DmGuyduOfuRvLj1grdie2SZEi/+gPWfr9M9VNc8sUrK1kReBIXwg3EqJxakEQazdZOVr1hage8jycjy6iUNkpYATBDKInFJ2gE28+nxU1TUnmcf5ALXGNIFUzybTEBAHAOw/ZzQ/eeDLPO0SFuo1V23++4PhaAET/v9y+nbPsh8OEMW1fz+CEE/9IoBlY6ZmtKjpZlrc3FkyDMgo/rPVvxO3o2Q3X8Df3nhvfugAMfIr237yP9t+8PzQNAQyf+tcAlI59o3rDnv9+q5nGCIIZpBJU4dQrrwxOwt68UIlHDaVrso1rO1+dduvrWc8TfHvXy4G5Lp83MVxPO4QDQSD9sOFMVrd4tJQgP9HaxBDKxPeh85Ut/qDuTuYTx61spzVjc+qa5Lj2MPo4/ZggsIZVqY3Ppr8BKGE/XfI89fTd9Hxl9fQcrAG5DdeQ3fjT0DQEMHLSe9jzzo347avpe8XVM3buhUTTCoLM5pto+dO/z+g5HCc5KWZwA4cx3C+PAEAu8PT22qign3nP5ifeWYn37P0q2ayDYTqDvIMx8V3S2/8wbWOcLDraywqjvCRLhKoqqgXBC9dF923dwcrHs2apcjramfkVIptL29z4nmfx5VcdT+9rbmDDS64DYK+MBME2qZL8npW5mnXlKxIaQW0P33Ot+6ZNoKf2PjRNRxqHeH/odAuk8yAEzsFj5rA2DU0rCFru/iKt9/zXjJ7DdaLCcqAqc+o0/iOWteIuPQZnxckzOoaZIGkaSmoEt8oT+RvnPcn9gxXlvdl38aq7Lxn3+Ontf6Drp5dMe3P3y+/cyj9c/1i44h8Nz4sigkA5i+0wXNRDCht7JLKptx32bLVfdQAYq5z43JG2LWxL4C1bT37V8bys8k8MEIVNDp30l2x/y4PhvZUiMlUKt5w41hD5adMIwmP709TVrtqgjIn0Qx+BOun8cOLPJ5pWEIQ2w+mYbHyP7h+eBzWNsB0niKXHpRLYlPUqM5Oy6HvFVfS/9PsHfv5ZJmEaqtEIGv3E7P7NADwoD1NvjDeLBFpSz1fXjr3fJPnS75/kl4/u5t4GMfBhg3rpQ9C0RJdcsEQkCDLCQea6SfUGBenSbfj5oBNY0FhIznDnsQMll7Z5Mnt04r18NkOmtTucI/2Yz0pUBni4PdLyVA7M9EgC4QVF4Mr7D/xgUtLzzQKiMpA8h/SnT9AsUppWEOjepXq1ekDHqvSR2vco9mAyBK8a0wg87SeIOYlltnNBVj6M9xeozRPQ/F/65eHf+UeuBOAoobJuRSw+vRHxBK7ME7+Y6jBHpbYH8e8e38dZX7yNGx55mp6vrmVJ9WkgMgNBFBqawVF17P2gnLHvIIPMVGdlYGZYAI7//3zFcZxzRFTVVpv49JWPV1y1Kn30p1RI7I3iWbSLEcbo3glA+tNL6Pj5m8beCcBTGoE1zjMxEXRoqCjtq9/mleveM0Q0ryCo9OPnl05LhUV9DFFNduIqBTVrUnjY+DjSjkpLLIDomdGIawTxBulAqHbvE111n2sNMleX/e9JpJ/63ajHF16F6qpnAZDaPwGfwgRwYuYgr2YW++zNj+P6ki//8k/qnHseACBPBV8KKjIVhk1mcHF7jos+7FWRua5g4Ho5Pf9Xn884uIP3nh21ptSanQhNQzFBMLKbilBaziE8zWfS/4PtVUg/9Tta//CpcL/M5ptoufuLoRkmu+VmHtyRXJ3XoltGZp665YC/kyj3qf+dEXq+spr0lt+G3yO9+/4DPv5ipnkFgVvCa10ZPjwHglUOBIEzlHj/0Z3q2BnhYuMxSD5RWmKhEtcI2msSi/SWP9inArB31fkA3Nb1isR+Xde9np6vrFahfbXH9yrIVF6F+Um3bvtUGK5Gk3PJSU7UfaVgkg8m+x6U6ahVlHGwE3kBh3baibLFAomz8jT6FqCJb2VHZMLSvSZCjSDmI7CGd1NB1cMqWirIoTB4O22//zgtf/5v7N6NpHbeQ+cv3kzrHZ8l/fRdAGz2l/O2K+8bu/y4Wxl92yQRzmDwvzpfqre4ILSz+UDzCgLfRbYsRVT6DvxYuiF2NRIET+wZZKSszAcq8chnULZgiYWrCWhGYpPq0tZkwTwtI56UB7OufEXYInBXVq0+r13/Jby2VeH+ueJV9X4at6zs7XauzlEJIEYmX94gPuZ4nwiITEXakd8VRAi1UsLFZiQmCGzpIFM15X+FhbP2nPCl376KhUAmps0NVYKEq+D+bemPJuj0rnvZba8A4PuZ1wBQsVuxB5Spb8mV59J9ddRKpO13H8U/+GSWiEGW0c/9Y2gFwqvgdq7DGt416j4TRZt749nDeFXKQea7YXSaVhDge/j5Hqxp0Ai0czH+AJ7x42O4wL4rsdvIPMs4nSrxBCVrlAiMrUEvhvZeZWYZTCl7dF92NdZI8kevncka4VWQdlY5Xd0yI1WP/73jKXYNVhDVIZb970nhNR+P6x/ZxWmfu5VXfju6F+WaYoDaVKQT/pYEDegPEn242BwslMYnsRBedcxGJv0XfIPh097Pnsu2TWh884VQEAQ6wbfu2JHYvlGuZl35CmhRvgKHNG7PMxoeq+T60LaCDXI1h4kdDJZjWp2UpHbeE74Uzghe56FYI7sbHGkUvAqpnffWvR0KgqDCaNsfPoVALvgQ7dmgiQWBg5+fJo1A90HVPoLg9SfTlyf2i5sY4p2SFhoHd+b4/htO5lfvrq/RUisWMmXlBCynVAezkXRXIrkH4P9+exu/ezxy8EWCII9wS/zgnm189bbNXPiNO1j2TRXtYjVwCDbiE78sAkkHsU7006Rx2Zy7lK4ga3iJGGSjv4qlYjBRmE0KS/kE0qMLgurhL4YF1PHqdSevIpuyeOUJQb2r4AbqJEit/WyoqjyD9tZWfu8dhy3dUc0u9sBWZLqF7XIZB4t9DFai+20NblXaQ+AjE9UBvO4jJxW0kX/o+3RffVHd+6GzuCZ6b74U/5vPNJ8gkDLsxernl2JNR9haUL/EHlLRJqNNUrplIDDhFe18pbC8rc4s1Ijdh6p6LtvzBd5c/TsckUzn37L8fN614yN87Wc3RG96VbAzyJQyDT38dNIJD0zJt1MQT/Es62EqMY1gsOzSEmQNr0Tdt9ViD8OB0E7FCgVKYYf+i8XC/zvnMG667Nkc3BmUxwgEgc6q1qGw2yrqenS3ZHBI8ccnduFUKwyc9+W6Y7aICr/aOMR2uYxVYm+obQDkH/wuAD1fVa1IrOog3pIjsIcmEb2nI4B8l2VfWxeaFrUgsGrCR7Gab5qbLE13hXKP/ICerx8JwkLmug5YI7AGd4QOr5Y/fw2A9NN3N9z32DXLASg94y2UTnznAZ13vlIbmvnk6f+M17ocz8rxO/8E/PgK287ip1RS0xFiezKWX1hKEHjlhvk/taG6oxEXVv+V/jJXZj6dEASfu+WJMHFMd41bK3ZTRQmsHHHNTSohNc/zBCaDEKI+8ouoVIhMKQGwvV9dm+58mippUtLhyT19jLQfWvdZgO0jNrtlFz2iL2Eaarnv64n90k/fhddxCFawiJoQQcKeqA4hfDfMQej4jaohVPub1pFDbuc6nFHMWc1O0wkC/cAJ6eG1rsQemLwt1+7bROttnwSnxNLLT+fBTU/xW+8EhjKqrG3HDX/Z+HNZNekNnfkPlI99/dS+wDynNr7cEja9b7kn9ELGi865y9YjgmtSJhtONkL6ygyTyoNTor9UHzlkD2yZ0Hh0p7T3PefQ0Nxx11N94fZbNu4NM7/bKLNXdpAX1bCoXE7Ecgn8KgiBtBZvkTJdglsLwNoS6R25FD6C/878J1kcrnl4H/91yH9zT+HvEvu1M4KLjY2fMA01ws91AxNP7NRaShikURNQUKflCzXN9V/0Q/peuah6X00bTScI4glcfuch2IOTFwSZJ2+g5f7/CX0CDz+5hSHylKtj2/3Dmi3NFNIWrOa1U/m/bn0y3OS1r8G1lJmlJDN8+gaVravsxyLUCAbK0WS8L72C0nFvxpqgRqDlzovXL2dJVxcAD+8cZEvQd/mwpS2kgiS/NjHCHqn20b4BXwp+7Z0afZ1AW1ms6Iq5RbmW/fl1eO3JgnBCRN31jrB28MMH9vL5YgevvP8EAPYFhewOtXbiY40rCKSw8LoOm1yGf7DCtypB4yA3GZJdZzYU2t+RNZVGR2HxPtEN+MkDT3PzI0+Fr/1Me10S2MRQk9qvH1CTWifDDMo8GRyWff3IUT8V2pYX8URSixYAtdYdP7+MwXM/y5J9KpqnQjpW+kEq01CmDavUy0A86sRz8TrWYA/tYCJo/UMI8GM1gHYE2sfJa7rCaKFWyuyTqnBcKAgQPCZrSl0E36m0/lJKx7x2QuNYKGiz2TbZw+Un/JDy+kv5tTgTgFeesBIQoQkNoCqjifWVlU/wAT4AwDur/w8PCws/YRry2tfQ/6Jv4WeVwBDSVwujySRY6t4CIyoQQWsEXsdaRk5+D1bMNDR0xt9Hv7dmWoBNkuaZkYB/vXEjW/fHIgrsbBjhMznUQ/vLBzYD0CpKDJMnhVu3OkmwwJrUTwdaAMST0H7ecjG9l/4WMq2JCSC05werbj/fg6gOMFSNJhIhPbyONZPQCHRPAbBS6vgpXDqD3slSRg7hNlFmEKW16VH5jaonBRNL+bg3MvS8/5jQOBYK1VgG9kjVw1l9Jl/t/jAAFxx9EMPVyLkOsI+o8fs9skCRdYxY7QzQho/AxmcolsMhU7nAFER074WtnNMTFQZBcIZVDoIy3ChbX6bbEnWLvO7Do6ghEz00Kk0jCPSEYOHjBTZeaWemGMapJgcrqDbZRpkymRrHYj1+ftkUzrWw0fN/3OH7vfa/QAaTwZ5lKgTVQjJQdtR90mYCywbPxYm1u5TSx29fQ6r3scmNA0FGqvuzRuyhc+tN4bZQEFAKNYGwFaOwIqGQC4rLBT8buQhXmNWYI11nYOu8i1za4rClLeSD5/yI8uWhU11TFnmue8HvAfCkhSV8yvFMbt+NNABt2w8fjokJAt1lzCrVm4ZkurUmN8hS/iYW5/2aLppGEOgQNhsfJxsU27IzqnjYZOv+BJVLD21TP5BWUaIi01ixB/lfndclPjJ8+t82pWoqQh9B9F7cofzYkX/JA/6hqlSDJwMzUGAastJh+Y6UJUhZAqTPUGYZwi1j924c9/zhqQS0Wyq661Opb7PuxreHe0SmoRJO4FCuBiUVhGWxQ6rnpbpaVeCUoalh8dmb4z2mS0EGtp7Isymb5xy+lK7l66im2nBp/Dw/70i14NE+grggEL4b/O5cer4RmVGlsCavEQRh2pGzWCLTrcmoIWHFTENGIxiNphEEOwfUJGAh2b72FQw873PhZIM/Oa1ABEXFMp4yM7VQoUzS7FOqee32PCNRyKtZ0JmqcQPL3qEq3/jDZvYOVZBYVEmH5bn3DgeCWQiwbIT06GCY1ozNuiUt6v4NeHgty8P6/2MRWh8Ayxlhu1hJt0jWhNIaQbsYCaOFovtp8WPvHE4tfy32pfQKc/EJgjgjDTQCIQTuy7/DdWc1rgqrBf+rTl4VCgIvPr9LF2mlwuzf6IPWhB3GYSP6wDQUlnaREj/ThlWJlRkXViQARPMtxCZK0wiCq+5R0UEWPpVUB5VjVM0UZR5yxvpoPVILAvUAtlCmWrM6iveDBbXiEdPcaGUhECYoxWxDT/aO8M0/PsXfXfcovpT4obhQgkBIiVLp1TXNUiWbsujMpxBIBioefuch5B65Ytzz6+66QqiEo63WKjpEVApESkgHUUNLGQxDTMva5CEEINhLJ6F+Eao5i1sQlALbvnYg63wDmWnHy4zdjvPwnrbQWQyRaRbfG8VXJphwCGlgzrWGVVmKqLSLRGa7ErvKuEbQREEak6Vprsy3/7AZUILAj39tKz15h3GwIsn4alXTKip1ttKtQeu/EGElGmg3G40qEj2wYwCJ6j+gzWr7hqtAkEeQacXNdJEXShDk0zYWkhs27CP99J3kH/3RuOeNNAKBcIbZbq8KM4j1fdcJZT2iL/QRpIJiebZt8/yjlvH/zjksdrDmsDmHGkHwfy4VK009SpMvLfCfeeiSQBCoa6b9PMJ3Gl+3KWgEusppVOxRInM1AipucjKdyUalKQRBPNvVxk9Egkg7G9ZED/fp3YAY2UvHL95Gy91fqj9g8MDmAkGQpxKaFDQ7UXblx1/2a/WGsELbZjOhJ4Z4Geg4Ukr8wKkIsGeoGkQNqc9Vcj3kqZBN2eRSFgLJrx7bw54lp01yHKoGzX6rGzuoAGsN70QStRLNCScUBE8SVBAVFp956XouPWV16KQMV5aLMCa9NRNN9qWqh5QyYRrSjFZsUHPMinZs2w4b+mihgu819pUJMa6v2BrYxpLvng6+i9e+Gqs6oMxMTmQa0uY6P9MeHNdiurqpLWaaQhD8/omo9o+FxIt/bTsDXpVtfSXe9aP7ueup/Sy58nl0Xv82sptvIPfoD+uOJ4KV/XJflUNupRw6GTWutLjllY/SufpY9Yb0kbmlNBvj1SP6/aZegvQxIOYjCIS1a+fIUyWXtnh6oBJqdP+39h/xa8wAjQj9j74D0sOJ+W6Wfu/Z7B2uksJjn1RhkC42JZnhPgp1xxo+/QMMPP8LhD+bRWga+ocXHhX+vb/kJMxC8cnfGkUl0O+mbIuPX3AMqUDo6ggkpRE0eibG1wiskd3YQzsQbjlR+TRqTSlDs5Ol84Mm44RuYppCEHzhlifCvy18fBl9bR1C+sXfbeLebf285/8eBECM0mwGCE1DL7duVccUMrQth8dFsKQlmiiE71I+5hL2vvW+aflOC4H/eNmxKtIHeP5RjUNnr31wZ+AjCARBjUbgWlnyokI2ZbG6K4eF8ins8fLKWTzO5KF9BLZbQqZbkTUr2V89ups0LntQ4awOKY6pfIf7CSbEmF3ZW7aeytGvDo+xGE1Dzzuqh+vecTq2Jdi6v0TviNKCcjX1iEY3DUV/5zNp9E/gqd4gxDOmETg9x9N3kfLzSCEQE/QRiEofXodK8vPaV2NpH4Fs4MA3GsGEaApB8N6zD+XUQ7o5fFmLimIQNRqB79BfTqbBh47dRh2UGkw+To2zWAmC2MrHr6oopZbmyCU4aVUHz431xD3+4A4uOm55w319LJbm1fXbN1xBh48CLN13N6+2f0c2ZfHesw/FEhKJYMegq6JAxvHvaKugXe1HZtpDt3S5/RAelusAZRoqC5X17Ugl0MMCeI0cjIs4fBRgRUeOVZ05JPDIzqBJT00nutFMQ/F3pbBIBxrBe69+kP0j1SBqKLhuqSzOmudEn6xZuYuRvYnMf90nOrfx2rDekN+6PLZYk/VmJ2EtiNahc01TCILzjz6IK//imRzSrTqE+TLuI8ggvEr9CsePVNm2mz5Axy/fQWbLzXRc90bwXdzWlQzIqBxxOpNUdyXJfr6Tjkxa4NQ2fxFC8PEXFnjz6Wvq9pUI1nRlo8/JSBD0tR7BdrmMbMpmRUeOtJD4WOwarEwqITA9tD3ROWyw5xRcbE4SG0kJj3LQk1drdlFf40YTXrBtEcelp231vT/880cBWL+8LbF9NI0guZMdCgKAobKj7m1QFjoRTi0EtSv3JT84m85rY/k4sXuto4NkfimiGvQhiD03A8//z+AczRmtN1maQhBobEsENubYUxxMJnqFo8Pdwggf3yG78Rqym35JeuvvyT71W4T08dLtdIgoo9GzkqWJzzmiZuW/gBvRTIa/OEOp7P958XENtz+6s762Uz6T4rmHKdOMlFobU/dj25JnUZIZOoJkMBEYe3YNVkL/zliEJSacYfxsR3hc387SQx/XZD9BniqVoPidUyMIZAONQPiLf2KpXfEftqx1zO0NETYpEV2rjgzBir1R+G29RmBVB0nvvi/aI7aY8roOo3LYiygfcVFCI5D6/saEfqN7aEjSVFfIEspZ7Ma+trTUZKJXOBfbvw82+EgEQvr4Xo1q6bu46fbEW+VUTdhaTTMMMcmktYXKu85cx10feE7SLBbjPWetY213srHL8au6aMmo66XKVMvwx1slxYfSP+aLW14a7K00gt0T1Aj01GJ5FVVbSkSCIIuaWDoZphoIglVCBRZ4Y4UcNkEYsF3zvVvS9pjbNfF8EYRFOiYI2tIqSZBGPpYGx/PTSS0k3sxJpnIMvOibeF2HJcJHa49tVQZM/sAEaKorlLFVpqPjJU1Dg8PD3LGlD4hCCYXvUg1CQq1QtQw+Jz0GSCaM1QqC2sfa7R69KmkzcezKDq5+22lJs5kVxZtLSDiLyzJphxfSpyufwZPgiQzCS9ai1+warAS1i4LPeVWknQlvjGdlyAY1czrEMK6t7qcuJ+GO5SOQY9fXXwzYNbaf2iih0RSChI/ASmPFhGYK7R/QGkGtsz2pEYRl2/WxvSrlI5ItKlU70+AZiEWb+S2qN0giocwwKosv7KEBqV33Yd3za3raLkHg01fxGK66tGZSYGe48ZHtgJqoR6SyVVvlXrJhnZyoFiWo8NG7drpcZMNWv4c11h7K6a7EOWXsJ7HQGpnPBq864WB+cM82XnBUD2CRrvZh0aF+yzHTUCVeqiOY1Vd05uktDVIVjZMBhyouF37jDiCa0LK77lYaRLCPb2XIE3SWo4yVPQjK8APv+UDcR9BgEmlC01DtVagVFI0PkiJFvHG9h6oAOpogSCIz7TCyOyo54jtR7ka8MKH0aL/xr5TGIAR73rkB0i3sfftDyGwnfuchUyw33zzMiagsFAqvLhQKDxcKBb9QKJw6/icODOFVENvuZHlHDhvJjRt6OedLf2DD7qEgoSyaTHrpoNLWwKEprKjpuu9RCVaquqaQnc4mP2CyGMfkPWet418uPIaPnn8kCMGRd32Ef0n9D4/vHeYn9+/gjqdUvZiERhB0Llverq51lXRD09Atj++NfUYi8Gl95PtBJnIQlprpDBPLWkSF9pY857hfpkRNG8pGPoJmMA3VfO1ajSCXbuwojz/20kpj45HB4eeZj4DnKL9AaL4Z20egm0i13PV5tYenzIG9r7kBZ/WZwcdsRHWQ3IZrgjBSAYEmIXNdIARe12EMn/WJCX/3ZmSudKaHgIuBW2fjZNJKge+yvD2bcBZf88DTYGfoiWmgNh77c2vpXXlO8iDCjlYh0gtLSujkNJ11qSOJjBgYm0zK4rxCD23ZVDgxHGttBqDsuPxxc5/6u0YQIKwwpt1toBE8vneYf/zVhvC1J6GdyKnfI1US4HCs124rFYSd4fuXXcQxNdExDXMFmiAcsc40VPNAr6vx82gSu1kp8rZPGyWOszYjfBdpxTWC2qihGgKzn73/8eB1FewM3rL1ifpBySJz5pc3FeZEEBSLxUeLxWJx1k5opRC+y4pQEKiv3ZJJIYXNhZs/TT5otmHjs7/sU7GTURLCd6JmKL5HJRAE+ljZoA5Lb5ChKo0omDi6dk+sUmk2WHGW/PjK0wescFLyRLquPEijqKQe0Ye0Uuy5bBtOcN+cWNG0Fsr4IkU2ZfHplxzDUT2t/MfL1qszBrbmBE2gEdSZhmpep2pVBk18PztNRxpecfwKdYzhnQmNoD4PoyaPwC1TLrwSb4lK7hNefVZyM1b0nQkWlI/AtgVdXS3j71hLuQ2kS2FNN/fhh6v4pR05svtV5MijubexrnyFciZLgZftqDtMdotqZpJJwZLONhiGvjUvwDv+WDo2qQdUly9OZTNTG2uAbVsH9PnZ5kDGa2e0UI1CeIWw6epqwUlFJreujhxYFhmd3JTO0ZYXyNh529tqTHTAp9PfRvguXV0tpIL4+LBtKMo0VM7k6OpqoaurhV+872wAnCPvR2Ta6Monv5cVOI9m8/7M9vOQqTH9tLVm687/5deeyBV3buWYle186/bNgDIhdXW1YNsW7V3tpITH37/wcNgA7dVtWKl0eJxMLhf+LSyLzo4ctEbnsPwq6ZZ20tkUma4WrIwEWknHx1ETWdTZ2Qr5yV+nZvq9NWLGBEGhUPgNsKLBpo8Wi8Vrp3JMz5P09Y2Mv2MN9rBLt+filqpBiQIlCCzfxykNo6eObMrC8iWOtBjyRq+R41Qq+EIJivb2bnoPu4T0JtUkRcehD7vpKY1V09XVckCfn20OZLztjiQHnGhtYhV7EEh8CX19IwxW1L3a0X0G6b5hlgFOUMCu6tsM9w9SjZ23t7++Veh2qXI6+vpGIAgFjhcibKVEvycajH8pVIBKzfuHvZrUq0/AncX7M9vPQ9VJaj3lUrXu/M9c1cEzX3Es3787CoaQvvqNdnW1MDDk0OVWGegbZCmQuu49uN1H0NdXogeouIKh4JhLJfT3jyCd6BxLnRIVP40sVxnpG6FlaAiEYCQ2DlGuEM/Y6R8oISuTbwm72H9vPT3tY26fMUFQLBZfMFPHnjSBjwBgTWcGvzeIJElZocNqRGYRqCYlEivMNG2E8Ks4Imh3Gaim2jSkuzb5pgnGxImZE86371aCINAOSr66jjKVC5yJMdOQlakzDQ1V6s029/mH8+ITlE/AEvURP91iiM01psAxSeVxDzph4vsvQDy/JrlrDNv7yasjM1uts9iq9Cez6q1UzFkc1zrqjy/cstLcfBdr6GlVsK42t6DONGRMslOhKQJspWWHgoCYaUhKGWYg5qhiC1/1NMaqi19P4FZCW7MWJNpkuk+2Uyh/J2rEYRgfO2nOsWIZok6wgrcsVZRMCiusF+RZ9ZnF8Ub3mhxVpK0EuyV1vkI0YSxlgGqqre5zzYxbKwjGmCnyMTNSIo8g045wy4nQTRlfINUuluK/GS08rBQtf/4aS797GuhckPhHagWBcRZPibkKH31FoVDYBjwL+EWhUPj1jJ7QSoeCQPh+OAm4vmT4jL/jytOuZYQsLVSw8ZFYpCr7Rz2c8CphT1v9YFtCcFblP/kb5y+pkKn7IRlGJ6wdD5xibUAg8fRk76n7tnLXb8OoIT3bPLKnSrWSNAU10ghyVMMiZVojiJcZsYSkYo+tOjcbdYJgjJV2vE9BguCa271F/Ezgc4tHYSWkS1SBFkB4ZXXPYmHbwq8XBPUSygiCqTAn9otisXgNcM2sndCyIZhQ0hZ4QRlq11f1ywdzBzNMjjZRxhY+nrA5Yv8fRj+eF5mG/EC9tSzBNhlFmNSq1oYxSEVmuAvtO7jCfV5UMyZuVggyjrVpqN+xuPa+p7goZqUZqjTQCEQ1PIeuJRUvPAgYjaCGjD12ZnGcfKxzmWiwIs9svRWZ64bqQFIQxDQCWVt0zi2rexY7Xu7RHzNUmw9Qk+dhovWmRnOYhkQqLAuwrMUOV4Pxyboq06Rxgw5mFj9c+0+jHk+4pbDBSagR1OxjNIKJI2tMQyJmGrqXo2M7quqS2jRUIc3O/f0JM1xDQYATaQS6lEXNfFG1jSCIc1BN9NVYicSjagRA5bAL8Ft68HPdddvqfASx+yjccvBciNh7pbpnpc5HYExDU6IpBAF2lHhkC58XHrMSiCZriTIV2MIPBcG21FrWlRs3R7cq/YwE9ev1Cqd2xWQEwcSpVfdPsDaFwnqP18bX3ZcAYFX6lCAILvUIWfJUeWJvFD3RqCVmnkooCHTrROkn71dlnGbszca6pTUhs2NMsJlY3ahsTQMbaWcR1SFkOki0LMWyvhNlqJPHVI7iXP2GWtOQcRZPC00hCGQqD05gS5Y+uYyavL9622bVLINAECCDhDM7bK0HUf0hUNVKRXk/A1YXQGjXrK3GaExDkyBQ791OFdmz3toSCoJKvK+BWyZurS7JLC2iHDZPgcYaQVbEncXqeD+4e2tiH2cyUUNNwBtOXc3pa7vC12NpBHEhkarZUdo5hDMcJo/VRRAl9w7/El4F7FxdR7n6DmS1EmT0cRpGpykEAXZGqZ2eA9Ln4O7oR/+/d2wFqXILbCGDDmY2vcNRNMpDHB7+7bcsRUif+3eqVagdpMHXPo9GI5g8ftvK8G8pBZffuZV9w9VwFS98V7U0DC62dvDvG4nu1VADjSBHFYIEMl1A8NGdwzV7mRkkTj5t8+VXRX2BS87ECu3VPfepQCOwdBhwfFEVCQLhVlj63dNjB9LO4sCM2xqkJNWahgzTQnMIAlDmIemC9DlkSWQP7g01AlUKOYUXdsDSCGTYa9jP9wBRjaGUpzSNWo3ACILJ4y2JSnX7CL70+ycB+JL7iuBNJ+EsHiFHq0iWoR4e10egJrSn5dK6/QxJ4o7ffcPVMfaMKNUkokk7o0x6wfWPZ3THNQLdXEaHmtaZhgKTUF3UUP2oJzROQ5LmEQRWCuF7COmxpDVaVQxXvchHgB8WpdOC4Ar3efxQnodsWcbwKX+F1620Ay0I7EAQ1PoIjGlo8gyd+cnw7/jV66cNr22VCiOMRYmMyKyy/wc7SylHcRZX6wRBbY9p42Mcm5bMxGr65GtKU8h0K+ldfyb7xPU4B51I6YR3RBtj91KHiFoje1j29aPIbL4xIQhCk5A9Rn4PmBs5RZpGEIjqEPb+jUHPVJvvv/FkALb3qxWlj4WNMg1VpRU2s/+I+xdcL58NwMgZf0dug4p6dbF50F/H/iWnAPU2VKMRTIHYj1yH+IaESYEWfSVlZx4hR2tQLNDxfL5z51a8Bpc9J6p1UUMAP3afyx5ZX1PKEPE/rz2BN566mpesXz7mfv9+0XpWd+X4pxcfnXhfpiMzbN+rf07l6FdF2xpUdhXl/Qh3hMyWm5Lho9q0VFeoru4I42w3NKJpBAFA91UvDZpjWKzuUhPDjv4yvlQagSWUs9jxg9V+MLs3ShLOUeWl1X9h/5ITgfqoCtdb/M1Lpo2Yuj9w1GsBqNTM6FJYgWnI4p6tquzwiMySFxUkkl8+upuv3ra54eGVjyCpEQB8yH0XV3rPA8z0MRonrOrkfc89LBEZ1Ihzj1zGNW8/nSNqehvLIFlw8Nx/r/9QgzIs9uAOtSkMFQ1+g/oZMaahGaGpBAEQJCXZqjsZKipl874RJQjwSeGHWa06pyY+JY2c+C4A9gUryXDBUiMInEZLU0NDKke8lOHT/h8ApUPUxFyb8IWwQ9PQmYcuAWCYHC1Bl7Hdg8maQ3HiJSZEjVR3pakJNZPIjPLHeV2H1W9sULei44a/BHQeQaauZHXD/hBxjGloSjTdr0D4Xt3D8uvHdnOpjhoSPl5QZ8iqyXa84p5tnOEdwjOBnShno05uqjUNVYxGMGH81uWMnP4B9XdKxa/7dYHlNvgOUgjec/Y61i1tobLboeWJMlKOHdUSNw2laorOucFayMwfM4M1ohoBee2Nuv6NPv2I6pCKENK9KrRJyDIawUzQNBqBf8T5qkCV9MNCVc9YqVb1LRmVQWDjc6x4kpwMIoGC2d2Xkm19Jb5wyyauundz4rjhgqVGElRdIwimgo4qqSsVYNkqBl1YLGnJ8IZTV5PLt5EXKpqlNxZCWmueiDuL7TpBYBqbzCSVwy8E6rPHgWSHshqE9JRGoLvB2UYjmEmaRhB4538Gv+3g0DQEsH6FUlvTthX6CF5g/5lXVH4KROYex5P87bUPA/Bz7wzeWv1g3fFrSrNQNRrBlJDpeo3gsrPWKeHtu8RXfL6wg7LhUXjjf5/fxSXHx0NDJS1UkIGmYdd0wdJlw83tmhn8thWUD78QmV9Sv3G8Sd3OkH/gW8l9jbN4RmgaQUC2HVEdCJ3FEE30vpR4gUYQJ77I12UMSuT4rX9S3eFrfQQVoxFMjSDKRGsEp6/t4i3PXBtFDcVCDqWwsfFwPZ8/blbVYi+49cWct/e74T4fOqMLO5ODjDruUDqZP6AbCbm+uV8zgrAYvOC/64rDQX0J6cGz/znxOrXrzzirz1L7ah+BCR+dEZrHR5DtUE2uK/2hk0o/M75UE48uP6CpbeDdCH2M2horxjQ0NfzANDSMMuVE8esiKFUQPbI+NkvEEN+5YwvxNc2K1DD/euExHLq0haPYgrUlCn28dc17ef/T54WvtWmoNhHKMAvUaAR+Sw9+ug1LJ5d5DiPHv53cYz+OQovH0yIMU6J5NILYSkJSoxH4Ek9adeadsQptabSzuK7Y1oGMtZlJaxOOEqQ6QSm9+z7af//xRGapXlGeZT2YOITwXV5Q6OHwZa10/uItpPZvDLedc/TB7CUqMDcgA02hQR8DwwxTWzDOSic6zknLjjKKQ2fxeKYhw1RoHkEQR/cQCDUCGVYfTew2AS1T75K2m/NSTjcy08YnnDfze1/VuanNVI1PBH5d5Um9Icoutod2JDYd2dPGu888JHzdjxIEgw0ykg0zTI2z2G9fpdpRxuz8oeCfqGnIMCWaSs/ys53KPCS0aUj7CHRmsRIEP/fOACZnGqqtNWSYGgL4rvfC8HVdyYK4aSiYHIZlTX/pmCAYOfFdpHqLic0rO6L9tWlosGwEwWwTv5d73rM19mPKgFcBREwjmKiz2DAVmmoZ23fxT9UfobNYvfSD6qMCuMU7gau85wTbJz65T0RoGCZPS0bdq97X36reqDEN3eEfTarGyS/8Kp0/u5TOn70emWnH6Tk+sT1eKnlvkBjomx7Ts0+iH0Hy9zNy0rspnfiuKOxU79vA6QwTCCs1jElTXb2o7ol66JIagcostoh6Gk9kctc1heL7ru3O8zfnHD7aRwyTQGsEugxx7Q++KlNkhJNwymQ3/Sr821l5at0kEzfjHV44iZfvvYKPnX3odA/dMB6j5BH4rcsZfvbHgCCxbALITAei3DttQ2s2mkoQ+O0HUzn8JWGsup4OpJR42KSFchjWZbWOwUhQ/35pa7RSvfptp03PgJuQ2p63OW0aCu6ZcOPlwQVV0mRQReg+/IIj4Da1TVpphO80LBQV1wiee8QyznvJMdP5FQwTZLTM4urBz4r2Gbe2kMJv6cEygmDKNJUgABi44Ovh37uH1KTSX3appm3SuGEZahjfWfzCo3t45iGqF+va7jwff+FRrGg3jTOmk/Zs8hHNbL898bpKiiPEDm7ilGR7RSulitQBtUlG6drwMMPcMFpmcdwhHEYLjZ0B7nWuo/8l35mecTUhTeUjqOXInqhBTZV0IAiixulj9RQ4+qA2PvWSYxJVGS86bgWnH1LfpNswcWqn6PZcUhAkatYIyFPlw+kryVOmNa27YLXETEiNNAIrfgjDXDGKXT9RajrUEMe/U35HfT0jw8RoakFw8fFRa0RHpsjgosrMNeiXW0PKrCpnhFr/fFwj2HPZNnrf9MeGn1sndoXJZ8IdUcUFQZmG6nwE0WsT7DU3OCtPx+tc13hjjTlo71vvw2tfPfYBzY08IJpaEGRSFj1t6qGrkiKDgyV8/KApyliCIG2ihGaF1jE6Ywmi/gJtlBJdtIQ7UrNnRNxHYO7i3NB38U/CXgV11ISIypZlqjSMYcZoakEA8JbT1wKqdWEaF4EMjQnlMQSBbRLIZoTaifmwuN1/DE6wnqjzJ0DUAjFOKn7vzEpy3tEoaUyE/p7RMPfxQGj62SwX2PgrgY9AIENn8Vgawd1P9c3G8JqagztzdVFEcYSINIKPpX9AJmXhta1K7NNy75frJnujEcxzGvUcaCDQExiBfkA0vSDQNYIcUrzKvY5WKvjmsswdsR90bpz2iFA/kevQ4LH2iucRmOlj/tFYIxhbEIzfy9gwFk0XPlqLjlN3ZIqlcj9Lrf1zPKLmJj4x19UZqttXJJrRW8O78FuW4XlV7IEtsR1H1wgM85BG0URjCILq6rOpHHXxDA5o8WMEQVqtDuNJZEYjmB/oezMWaRFNEMIZBikbd8OKf8ZEDc1rdDe5BGMIgv6XXTmDo2kOmn7G0+aH+MpyIlVn3vpME7M8E8Qn5if3jYy+Y7Dvt90L+Kp7Ef2pZeAph2K9aWEsjcBIgvlGI0EgpCkKOJMYQRCYH+KTv9YIlo+RJfyiY5aPus0wdeLTcu/IeJEicL1/Bv/uvpahVDfCKwOyQYXKUcpWYDSC+Ub/i78d9jlO4Jvw0ZnECIJAI/Bil0KbiT56/pGjfs7MHzODEIJ/fFEBgMvfUN8SdDQ8K4vd9yRQ3yhd1sz28SZCpujo/KJ66PkwSdOQ4cCZEx9BoVD4LPBSoAo8Aby1WCz2zcVYIodk0lzwxlNX86x1S0hZIqwwGsesJGeOF69fzovXT07jcq0sHTe+Fz/dhrv8xDH3jZcXr7hmpbkQcFecjP/4z+d6GIuWudIIbgSOKxaLxwMbgA/P0TjCbNStsid8ry2X5rUnq3j0/7z4uERlUc1Y8e2G2SF+B/wg9txyhqJG56nxk9HKjuktvRAor7+Ufe98bK6HsWiZE0FQLBZvKBaLWtf7EzBOIZGZQ5cwuM6PSt9+9ZITOCjwDzzzkG6ueXt9WWkTgTjPiFenDGrVuEuPVq/HENploxEYDPPCR/A24JdzdXIRq2545bL3B+8lL0s+bfOm09bwptPmTF4ZGhDXyhJRX7q94QRq2RuNwGCYQR9BoVD4DbCiwaaPFovFa4N9Pgq4wA8mckzbFnR1Taz2TP1nrXE/e3f2NF4HtHe0QM2+H7/oWAAuv2sbAC2t2SmPZSJMZLzzibkYby4XRQfFcwMyuTwAqazS6vL5LNnRxpZaGNfZPA8zS7OPd8YEQbFYfMFY2wuFwluAC4HnF4vFCcVueJ6kr2/s2PLR6OpqGfezm0fUhR0crOClx953X98ISyaQ8DRVJjLe+cRcjLdcjsJLZSy8sOIJ8oDj22SBUsmhNNrYXH9BXGfzPMwsi328PT2jVHoNmBPTUKFQuAD4EHBRsVicN1d/Tzn4YwLJK45n4g7nmrjlP/Eg6/BRbRpq4CP4+htO5vxCD68+6eCZGp7BsGCYqxITXwaywI2FQgHgT8Vi8d1zNJaQfcNVhs76GF7XEaPu87JnrKC4a4ijDmobdR/D7JPwEeiooTEKkT2vcBAnLzf30GCAORIExWJx9Jl2DujKp+krOVhCUDppbHn0sfOPmqVRGcYjvtC3iWlxusREKAhMiJfBMBbzIWpozvnyK5/BMcvb+NzLj53roRimSNovhX9LqyZqyOR8GAxj0vTVRwEKy9u4/A0nz/UwDAfAUPvhLOl/WL3QGsEEwkcNBoPRCAyLhEdP/CdGTr4MiOURGNOQwTAhjCAwLFjiCWXZTAZn5enqhW51qDUDYxoyGMbEmIYMi4Js2sanEwCZUgll4zWoMRgMCqMRGBYs8XV+LmWFK3+ZVckzMtPeYE+DwVCLEQSGRUEuZeEuDxz+UtUP0poB0hSWMxjGwggCw4LF8aKCcV0tmUgj0E7iQAAIZ3jWx2YwLCSMj8CwYNk1WAn/1n2Iey+9Ba/zUACEW0n8bzAYGmMEgWHBsnOwfoL3uqOkddXDGPCMIDAYxsKYhgwLlrI7di8BP7dE/WGZ9Y7BMBbmF2JYsPzNOYfxiV8W+avnHFq3bd+b78RvXYG7/CTc7nlV2spgmHcYQWBYsBzZ08YVbzql4Ta/TZWXdg86fjaHZDAsSIxpyGAwGJocIwgMBoOhyTGCwGAwGJocIwgMBoOhyTGCwGAwGJocIwgMBoOhyTGCwGAwGJocIwgMBoOhyRFSyrkew2TYA2yZ60EYDAbDAuMQoGe0jQtNEBgMBoNhmjGmIYPBYGhyjCAwGAyGJscIAoPBYGhyjCAwGAyGJscIAoPBYGhyjCAwGAyGJqcpGtMUCoULgC8CNvA/xWLxM3M8JAAKhcJmYBDwALdYLJ5aKBSWAD8C1gGbgUuKxeL+QqEgUN/hxcAI8JZisXjvDI/v28CFwO5isXhc8N6kx1coFN4MfCw47KeKxeJ3Z3G8nwTegcpBAfhIsVi8Ptj2YeDtqOv/vmKx+Ovg/Vl5XgqFwhrgcmA5IIFvFIvFL87XazzGeD/JPLzGhUIhB9wKZFFz3VXFYvEThULhUOCHwFLgHuCNxWKxWigUssH3OwXYB7ymWCxuHut7zNJ4vwM8F+gPdn1LsVi8bzqfh0WvERQKBRv4CvAiYD3wukKhsH5uR5Xg3GKxeGKxWDw1eP33wE3FYvFI4KbgNajxHxn8eyfwtVkY23eAC2rem9T4gkntE8AzgdOBTxQKhe5ZHC/AF4JrfGJsgloPvBY4NvjMVwuFgj3Lz4sLfKBYLK4HzgAuC841X6/xaOOF+XmNK8DzisXiCcCJwAWFQuEM4N+C8R4B7EdN8AT/7w/e/0Kw36jfYxbHC/DB2PW9L3hv2p6HRS8IUBfi8WKxuKlYLFZRK4GXzfGYxuJlgJbe3wVeHnv/8mKxKIvF4p+ArkKhsHImB1IsFm8Feg9wfC8EbiwWi73FYnE/cCONJ+uZGu9ovAz4YbFYrBSLxSeBx1HPyqw9L8Vi8Wm9gisWi4PAo8Aq5uk1HmO8ozGn1zi4TkPBy3TwTwLPA64K3q+9vvq6XwU8P1h1j/Y9Zmu8ozFtz0MzCIJVwNbY622M/fDOJhK4oVAo3FMoFN4ZvLe8WCw+Hfy9E6WGw/z5HpMd33wY93sLhcIDhULh27GV0bwab6FQWAecBNzBArjGNeOFeXqNAw3kPmA3akJ8AugrFotug3OH4wq296PMR3M23mKxqK/vp4Pr+4XAhJUYb824Jj3eZhAE85mzisXiySgV77JCofCc+MZisSgZe0Uwp8z38QV8DTgcpWo/DXxuTkfTgEKh0AZcDby/WCwOxLfNx2vcYLzz9hoXi0WvWCyeCKxGreKPntsRjU3teAuFwnHAh1HjPg1YAvzddJ+3GQTBdmBN7PXq4L05p1gsbg/+3w1cg3pQd2mTT/D/7mD3+fI9Jju+OR13sVjcFfy4fOCbRCr9vBhvoVBIoybVHxSLxZ8Eb8/ba9xovPP9Ggdj7AN+CzwLZULRgTLxc4fjCrZ3opzGczneCwKTnCwWixXgf5mB69sMguAu4MhCoXBooVDIoJw+P5vjMVEoFFoLhUK7/hs4H3gINbY3B7u9Gbg2+PtnwJsKhYIIHEj9MfPBbDLZ8f0aOL9QKHQHJoPzg/dmhRo/yitQ11iP97WFQiEbRJEcCdzJLD4vgf35W8CjxWLx87FN8/Iajzbe+XqNC4VCT6FQ6Ar+zgPnofwavwVeFexWe331dX8VcHOgkY32PWZjvI/FFgUC5c+IX99peR4WffhosVh0C4XCe1EXwga+XSwWH57jYYGy+15TKBRA3YcrisXirwqFwl3AjwuFwttRJbcvCfa/HhUm9jgqVOytMz3AQqFwJXAOsKxQKGxDRSJ8ZjLjKxaLvYVC4Z9RP36AfyoWixN16E7HeM8pFAonoswrm4F3BeN6uFAo/Bh4BBUNc1mxWPSC48zW83Im8EbgwcAuDPAR5u81Hm28r5un13gl8N0gwscCflwsFn9eKBQeAX5YKBQ+BfwZJdwI/v9eoVB4HBV08NrxvscsjffmQqHQAwjgPuDdwf7T9jyYMtQGg8HQ5DSDachgMBgMY2AEgcFgMDQ5RhAYDAZDk2MEgcFgMDQ5RhAYDAZDk2MEgcFwgBQKha5CofCeuR6HwTBVjCAwGA6cLsAIAsOCxQgCg+HA+QxweKFQuK9QKHx2rgdjMEyWRZ9ZbDDMAn8PHBcUCzMYFhxGIzAYDIYmxwgCg8FgaHKMIDAYDpxBoH2uB2EwTBVTdM5gmAYKhcIVwPHAL4vF4gfnejwGw2QwgsBgMBiaHGMaMhgMhibHCAKDwWBocowgMBgMhibHCAKDwWBocowgMBgMhibHCAKDwWBocowgMBgMhibn/wNnd7VIeq/wcQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = 4\n",
    "yhat = model.forecast(steps)\n",
    "\n",
    "plt.plot(val_data[:len(yhat), 0], lw=2, label=\"actual\")\n",
    "plt.plot(yhat[:, 0], lw=1, label=\"predicted\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and globally:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f0f933206a0>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = int(round(len(val_data) / model.hparams.prediction_length))\n",
    "yhat = model.forecast(steps)\n",
    "\n",
    "x = np.arange(0, len(train_data))\n",
    "plt.plot(x, train_data[:, 0], lw=2, label=\"train\")\n",
    "x = np.arange(len(train_data), len(train_data) + len(val_data))\n",
    "plt.plot(x, val_data[:, 0], lw=2, label=\"val\")\n",
    "x = np.arange(len(train_data), len(train_data) + len(yhat))\n",
    "plt.plot(x, yhat[:, 0], lw=1, label=\"predicted\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}